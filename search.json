[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pica-rs",
    "section": "",
    "text": "Start\n\nDas Projekt pica-rs ermöglicht eine effiziente Verarbeitung von bibliografischen Metadaten, die in PICA+, dem internen Format des OCLC-Katalogsystems, kodiert sind. Das Programm pica stellt unterschiedliche Kommandos zur Verfügung, um Daten auszuwählen, statistisch aufzubereiten oder für die Weiterverarbeitung in Data Science-Frameworks wie Polars (Python) oder der Sprache R nutzbar zu machen. Die Anwendung ist in der Programmiersprache Rust geschrieben und lässt sich unter den Betriebsystemen Linux, macOS und Windows verwenden. Die Kommandos lassen sich über die Standard-Datenströme (Kombination von verschiedenen Programmen mittels Unix-Pipelines) miteinander verketten, wodurch sich leicht Metadaten-Workflows erstellen und automatisieren lassen.\nDie Entwicklung von pica-rs wurde vom Referat Automatische Erschließungsverfahren; Netzpublikationen (AEN) der Deutsche Nationalbibliothek (DNB) initiert und wird dort für die Erstellung von Datenanalysen sowie für die Automatisierung von Workflows (Datenmanagement) im Rahmen der automatischen Inhaltserschließung genutzt. Weiterhin wird es zur Unterstützung der Forschungsarbeiten im KI-Projekt sowie für diverse andere Datenanalysen innerhalb der DNB eingesetzt.",
    "crumbs": [
      "Start"
    ]
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "Installation",
    "section": "",
    "text": "Installation unter Linux\nDas Kommandozeilen-Tool pica lässt sich unter den Betriebssystemen Linux, macOS und Windows nutzen. Folgend wird die Installation sowie Einrichtung und Konfiguration des Tools beschrieben. Die Zeichenkette X.Y.Z ist ein Platzhalter für eine konkrete pica-rs Version und muss in dem Befehl entsprechend ersetzt werden.\nFür das Betriebssystem Linux stehen vorgefertigte Releases stehen auf der Plattform GitHub zum Download bereit.\nDas tar-Archiv enthält neben dem Tool pica auch weitere Dateien wie bspw. Shell-Skripte zur Befehlszeilenergänzung:\nEine systemweite Installation von pica in das Verzeichnis /usr/local/bin kann mit dem install erfolgen. Hierfür sind ggf. root-Rechte nötig:",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#installation-unter-linux",
    "href": "install.html#installation-unter-linux",
    "title": "Installation",
    "section": "",
    "text": "$ tar -tf\npica-X.Y.Z-x86_64-unknown-linux-gnu.tar.gz\npica-X.Y.Z-x86_64-unknown-linux-gnu/\npica-X.Y.Z-x86_64-unknown-linux-gnu/pica\npica-X.Y.Z-x86_64-unknown-linux-gnu/README.md\npica-X.Y.Z-x86_64-unknown-linux-gnu/completion/pica.zsh\npica-X.Y.Z-x86_64-unknown-linux-gnu/LICENSE\npica-X.Y.Z-x86_64-unknown-linux-gnu/completion/pica.fish\npica-X.Y.Z-x86_64-unknown-linux-gnu/completion/_pica\n\n$ tar xfz pica-X.Y.Z-x86_64-unknown-linux-gnu.tar.gz\n$ sudo install -m755 pica-X.Y.Z-x86_64-unknown-linux-gnu/pica \\\n      /usr/local/bin/pica",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#installation-unter-macos",
    "href": "install.html#installation-unter-macos",
    "title": "Installation",
    "section": "Installation unter macOS",
    "text": "Installation unter macOS\nUnter macOS wird nur die Zielarchitektur x86_64-apple-darwin (macOS 10.7+, Lion+) unterstützt. Diese lässt sich analog wie unter Linux installieren:\n$ tar xfz pica-X.Y.Z-x86_64-apple-darwin.tar.gz\n$ install -m755  pica-X.Y.Z-x86_64-apple-darwin/pica /usr/local/bin/pica",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#installation-unter-windows",
    "href": "install.html#installation-unter-windows",
    "title": "Installation",
    "section": "Installation unter Windows",
    "text": "Installation unter Windows\nUnter Windows kann das Programm direkt dem zip-Archiv x86_64-pc-windows-msvc entnommen werden. Nach einem Wechsel in das Verzeichnis, in dem sich die pica.exe befindet, kann das Programm direkt genutzt werden. Soll pica aus jedem beliebigen Verzeichnis heraus aufrufbar sein, dann muss der Installationspfad in der PATH-Umgebungsvariable mit aufgelistet werden.",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#aus-dem-quellcode-installieren",
    "href": "install.html#aus-dem-quellcode-installieren",
    "title": "Installation",
    "section": "Aus dem Quellcode installieren",
    "text": "Aus dem Quellcode installieren\nDas Projekt lässt sich auch direkt aus den Quellen kompilieren. Hierfür wird eine aktuelle Rust-Version (&gt;= 1.85) mit dem Paketmanager cargo benötigt.\nDer aktuelle Entwicklungsstand lässt sich wie folgt installieren:\n$ git clone https://github.com/deutsche-nationalbibliothek/pica-rs.git\n$ cd pica-rs\n$ cargo build --release\nDas fertige pica-Programm liegt im Verzeichnis target/release/ und kann bspw. in das Verzeichnis /usr/local/bin installiert werden:\n$ install -m755 target/release/pica /usr/local/bin/pica\nWenn der Quellcode nicht benötigt wird, kann das Projekt auch direkt über den Paketmanager cargo installiert werden:\n$ # Installation der aktuellen Entwicklungsversion\n$ cargo install --git https://github.com/deutsche-nationalbibliothek/pica-rs \\\n     --branch main pica-cli\n\n$ # Installation der Version X.Y.Z\n$ cargo install --git https://github.com/deutsche-nationalbibliothek/pica-rs \\\n      --tag vX.Y.Z pica-cli\nDas fertige Programm befindet sich dann unter ~/.cargo/bin/pica.\n\nFeatures\nWird das Programm anhand der Quellen gebaut, können optionale Features aktiviert werden.Die folgenden Funktionen können mit der cargo-Option --features aktiviert werden:\n\nunstable, um die neuesten Funktionalitäten, die noch in der Entwicklung sind und für eine der nächsten Versionen vorgesehen sind, zu aktivieren\nund compat, um eine höhere Kompatibilität mit der Abfragesprache PICA Path zu erhalten.",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/beginner.html",
    "href": "tutorials/beginner.html",
    "title": "Anfänger-Tutorial",
    "section": "",
    "text": "Was ist pica?\npica ist ein Set von Kommandozeilen-Tools zur Arbeit mit PICA+-Bibliothekskatalog-Daten. Große Datenabzüge bis hin zu Gesamtabzügen können schnell gefiltert werden und es können die Daten einzelner Felder und Unterfelder in CSV-Dateien exportiert werden, Häufigkeitsverteilungen des Inhalts einzelner Unterfelder erfasst werden und vieles mehr.",
    "crumbs": [
      "Tutorials",
      "Anfänger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#wie-funktioniert-pica",
    "href": "tutorials/beginner.html#wie-funktioniert-pica",
    "title": "Anfänger-Tutorial",
    "section": "Wie funktioniert pica?",
    "text": "Wie funktioniert pica?\nDas Tool kann mit extrem großen Dateien umgehen, weil es diese sequentiell ausliest und prozessiert. Die Dateien werden nicht geöffnet und in den Arbeitsspeicher geladen, sondern ›häppchenweise‹ ausgewertet. Es ist deswegen kein Rechner mit besonders viel Arbeitsspeicher notwendig. Es empfiehlt sich aber, die Ausgangsdaten auf möglichst schnellen lokalen Laufwerken abzulegen. Netzlaufwerke sind weniger geeignet und verlangsamen das Tool unnötig.\npica läuft unter Windows, Linux und Mac OS.",
    "crumbs": [
      "Tutorials",
      "Anfänger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#installation",
    "href": "tutorials/beginner.html#installation",
    "title": "Anfänger-Tutorial",
    "section": "Installation",
    "text": "Installation\nEs ist möglich, die Quelldateien herunterzuladen und direkt auf dem eigenen Rechner von Rust zu einem lauffähigen Programm kompilieren zu lassen.\nFür die gängigen Windows-, Apple- oder Linux-Systeme, stehen aber fertige Programmpakete unter https://github.com/deutsche-nationalbibliothek/pica-rs/releases zur Verfügung. Unter Installation sind für die Betriebssysteme Windows, Linux und macOS die Schritte zur Einrichtung des Tools beschrieben.",
    "crumbs": [
      "Tutorials",
      "Anfänger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#kommandozeile",
    "href": "tutorials/beginner.html#kommandozeile",
    "title": "Anfänger-Tutorial",
    "section": "Kommandozeile",
    "text": "Kommandozeile\npica ist auch deswegen sehr schnell, weil es kein grafisches Interface hat. Man sollte deshalb einige Basics der Kommandozeilen (auch Terminal oder Shell genannt) des jeweiligen Betriebssystems kennen. Alle Befehle werden hier in der Fassung für gängige Linux- und macOS-Terminals gezeigt, abweichende Befehle der Windows Power Shell werden meistens in Klammern erwähnt.",
    "crumbs": [
      "Tutorials",
      "Anfänger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#pipes",
    "href": "tutorials/beginner.html#pipes",
    "title": "Anfänger-Tutorial",
    "section": "Pipes",
    "text": "Pipes\nUm das Tool optimal nutzen zu können, sollten Sie verstehen, was Pipes sind. Im Terminal wird die Ausgabe ausgeführter Programme oder Befehle üblicherweise in die sogenannte Standardausgabe (stdout) geschrieben. Normalerweise ist das die Bildschirmausgabe des Terminals selbst. Wenn sie z. B. den Inhalt des aktuellen Ordners mit ls (Windows: dir) auslesen, wird eine Liste aller Dateien und Ordner direkt im Terminal ausgegeben.\nSie könnten diese Ausgabe aber auch umleiten: z.B. in eine Datei oder auf einen angeschlossenen Drucker.\nPiping nennt man ein Verfahren, bei dem die Ausgabe eines Befehls direkt als Eingabe für einen weiteren Befehl verwendet wird. Wie Rohre (pipes) werden die Befehle aneinandergesteckt und die Daten fließen von einem Programm zum nächsten.\nDazu werden die Befehle mit einem senkrechten Strich verbunden: | Unter Linux und Windows ist dieser Strich normalerweise über die Tastenkombination AltGr-&lt;AltGr-&lt; zu erreichen, unter MacOS über Alt-7Alt-7.\nMan könnte also z. B. die Ausgabe von ls bzw. dir an einen Befehl weiterleiten, der die Anzahl der ausgegeben Zeilen zählt. Dieser Befehl heißt wc -l (von word count -lines). Das korrekte Piping geht so:\n$ ls | wc -l\nDie Ausgabe von Word Count lässt sich wieder weiterleiten, z.B. in eine Datei:\n$ ls | wc -l &gt; ordnerinhalt.txt\nDer &gt;-Operator leitet den Inhalt in eine Datei weiter und ist eine Art Sonderfall des Pipings, der nur für das Schreiben in Dateien gilt.\nMan könnte die Ausgabe mit einer weiteren Pipe auch an noch einen weiteren Befehl übergeben.\nMit Pipes lassen sich die einzelnen pica-rs-Tools (select, filter, frequency usw.) miteinander verknüpfen. Die Ausgabe des einen Tools kann entweder zum nächsten Tool, in eine Datei oder einfach auf den Bildschirm geleitet werden. Alle Tools schreiben immer in die Standardausgabe. Will man die Ausgabe anders erhalten, muss man das dem Befehl mitteilen.",
    "crumbs": [
      "Tutorials",
      "Anfänger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#los-gehts",
    "href": "tutorials/beginner.html#los-gehts",
    "title": "Anfänger-Tutorial",
    "section": "Los geht’s",
    "text": "Los geht’s\nNavigieren Sie im Terminal zu dem Ordner, in dem Ihre Daten liegen. Wir gehen davon aus, dass Sie im Hauptverzeichnis Ihres aktuellen Benutzers (unter Linux und Mac OS über das Kürzel ~ zu erreichen) im Verzeichnis pica-test arbeiten. Das Testdatenpaket heißt testdaten.dat.\n$ cd ~/pica-test\nÜberprüfen Sie, ob das Testdatenpaket vorhanden ist.\n$ ls # (unter Windows: dir)\ntotal 1872\ndrwxr-xr-x   3 testuser  staff    96B  9 Nov 14:24 .\ndrwxr-xr-x+ 76 testuser  staff   2,4K  9 Nov 14:25 ..\n-rw-r--r--@  1 testuser  staff   935K 14 Sep 18:30 testdaten.dat",
    "crumbs": [
      "Tutorials",
      "Anfänger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#print",
    "href": "tutorials/beginner.html#print",
    "title": "Anfänger-Tutorial",
    "section": "print",
    "text": "print\nWir beginnen mit print. Dieses Kommado formatiert die unleserlichen PICA+-Daten zu gut lesbaren Datensätzen. Mit dem Befehl lassen sich die teilweise unübersichtlichen Daten überblicken. Wir wollen nur einen Datensatz aus den Testdaten auf dem Bildschirm ausgeben.\n$ pica print -l 1 testdaten.dat\nDie Option -l steht für Limit und begrenzt die Ausgabe auf eine bestimmte Anzahl von Datensätzen am Beginn der Datei. Die folgende Ziffer gibt die Anzahl der auszugebenden Datensätze an.\nWir können die Ausgabe auch mit der Option -o in eine Datei schreiben:\n$ pica print -l 1 testdaten.dat -o testdatensatz.txt\nWenn Sie nur einen Dateinamen angeben, wird die Datei im aktuellen Verzeichnis abgelegt. Wollen Sie in ein anderes Verzeichnis schreiben, müssen Sie den kompletten Pfad dorthin angeben.\nIm Folgenden gehen wir davon aus, dass Sie grundlegend mit der Struktur von Pica-Daten vertraut sind, also z. B. Feldern, Unterfeldern, Satzarten und Codes.",
    "crumbs": [
      "Tutorials",
      "Anfänger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#filter",
    "href": "tutorials/beginner.html#filter",
    "title": "Anfänger-Tutorial",
    "section": "filter",
    "text": "filter\nMit filter können Teilmengen aus einem Daten-Dump nach einem bestimmten Selektionskriterium gebildet werden. filter gibt grundsätzlich den ganzen Datensatz aus, wenn die angegebenen Filterkriterien erfüllt sind.\nWir wissen, dass in unseren Testdaten jeweils 100 Datensätze der unterschiedlichen Satzarten enthalten sind. Wir wollen alle Oa-Sätze herausfiltern und den ersten davon mit print ausgeben.\n$ pica filter -s \"002@.0 == 'Oa'\" testdaten.dat | pica print -l 1\nDas Ergebnis könnte man auch wieder in eine Datei schreiben:\n$ pica filter -s \"002@.0 == 'Oa'\" testdaten.dat -o oa-test.dat\n\n\n\n\n\n\nVorsicht\n\n\n\nDateien werden ohne Rückfrage überschrieben und werden nicht im Papierkorb gesichert. Gewöhnen Sie sich am besten an, in ein eigenes Ausgabeverzeichnis zu schreiben oder fügen Sie das aktuelle Datum an den Ausgabedateinamen an, damit Sie nicht aus Versehen eine ältere Datei überschreiben.\n\n\n\nFilter-Ausdrücke\nDer Filterausdruck ist das mächtigste Werkzeug von pica. Mehrere Ausdrücke können zu komplexen Suchfiltern kombiniert werden.\nJeder Filterausdruck besteht immer aus einem Feld wie 002@, einem Unterfeldfilter wie .0, einem Operator, der angibt, wie der Inhalt des Feldes gefiltert werden soll, wie z. B. == und einem Wert, mit dem das Feld verglichen werden soll.\n\n\nFelder\nFelder können in der einfachsten Form direkt benannt werden: 002@ oder auch nummerierte Okkurrenzen haben wie /01. Okkurrenzen lassen sich nach ihrem Wert filtern oder alle Okkurrenzen können mit /* durchsucht werden. Bereiche von Okkurrenzen können ebenfalls eingegrenzt werden: 047A/01-03\n\n\nUnterfelder\nUnterfelder werden mit einem Punkt und ohne Dollarzeichen angehängt: 002@.0 meint Unterfeld $0 von Feld 002@.\nUm z. B. Unterfeld 9 aller Okkurrenzen von Feld 041A zu filtern, müsste der Feldausdruck lauten: 041A/*.9.\n\n\nOperatoren\nWerte können mittels der folgenden Operatoren verglichen werden:\n\nGleichheit ==\n\nDer ==-Operator prüft, ob es ein Unterfeld gibt, dass einem Wert entspricht. pica filter \"021A.a == 'abc'\" liest sich wie folgt: Es existiert ein Feld 021A mit einem Unterfeld a das gleich abc ist. Es könnten noch weitere Unterfelder a existieren, die nicht abc sind.\n\nUngleichheit !=\n\nDas Gegenstück zu ==. Prüft, ob ein Unterfeld existiert, das nicht einem Wert entspricht.\n\nBeginnt mit Präfix =^ (!^)\n\nDer Ausdruck wird dann wahr, wenn der Wert des Unterfelds mit dem angegebenen Präfix beginnt (=^) bzw. nicht beginnt (!^).\n\nEndet mit Suffix =$ (!$)\n\nDer Ausdruck wird dann wahr, wenn der Wert des Unterfelds mit dem angegebenen Suffix endet (=$) bzw. nicht beginnt (!$).\n\nRegulären Ausdruck =~ (!~)\n\nPrüft ob ein Feld einem regulären Ausdruck entspricht. Die Auswertung dieses Operators benötigt die meiste Rechenkapazität. Er sollte deshalb nur dann verwendet werden, wenn er wirklich absolut notwendig ist. Es ist z. B. schneller, nach einer Kombination von =^ und =$ zu suchen als nach einem regulären Ausdruck.\n\nEnthalten in in (not in)\n\nDer Ausdruck wird dann wahr, wenn der Wert des Unterfelds in der angegebenen Liste enthalten ist (in) bzw. nicht enthalten ist (not in).\n\nTeilstring =?\n\nDer Ausdruck wird dann wahr, wenn der angegebene Wert ein Teilstring des Unterfelds ist (=?).\n\nExistenz ?\n\nDer Ausdruck wird dann wahr, wenn das gesuchte Feld/Unterfeld existiert.\n\nÄhnlichkeit =*\n\nDer Ausdruck wird dann wahr, wenn der angegebene Wert ähnlich dem des Unterfelds ist. Die gewünschte Unschärfe kann über die Option --strsim-threshold parametrisiert werden.\n\n\nDie Operatoren können in runden Klammern gruppiert und mit den booleschen Operatoren UND &&, ODER || sowie XOR (^) verbunden werden.\n\n\nMehrere Felder adressieren\nEs kommt öfters vor, dass sich ein Wert vom gleichen Typ in unterschiedlichen Feldern befindet. Z. B. befindet sich im Feld 028A.9 die “Person, Familie - 1. geistiger Schöpfer” und im Feld 028C.9 “Person, Familie - weitere geistige Schöpfer”. Um Datensätze zu filtern, die entweder einen 1. geistigen Schöpfer oder einen weiteren geistigen Schöpfer haben, könnte man schreiben:\n$ pica filter \"028A.9? || 028C.9?\" testdaten.dat\nDer Ausdruck lässt sich vereinfachen zu:\n$ pica filter \"028[AC].9?\" testdaten.dat\nAn jeder Position in einem Feld kann eine Liste der gültigen Werte angegeben werden. Es wird dann jede mögliche Kombination ausprobiert, um einen Match zu finden. Bsp. 0[12][34]A führt zu der Liste 013A, 014A, 023A und 024A.\n\n\nMehrere Unterfelder adressieren\nSo ähnlich können auch mehrere Unterfelder adressiert werden. Beispiel: Im Feld 045E befindet sich die Sachgruppe der Deutschen Nationabibliografie. Im Unterfeld $e die Hauptsachgruppe (HSG) und im Feld $f die Nebensachgruppen (NSG). Ist man an allen Datensätzen interessiert, die zur HSG 100 oder zur NSG 100 gehören, könnte man folgenden Filter schreiben:\n$ pica filter \"045E.e == '100' || 045E.f == '100'\" testdaten.dat\nDer Ausdruck lässt sich verkürzen zu:\n$ pica filter \"045E.[ef] == '100'\" testdaten.dat\nBeide Verfahren sind kombinierbar: 0[12]3[AB].[xyz] ist ein gültiger Ausdruck.",
    "crumbs": [
      "Tutorials",
      "Anfänger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#select",
    "href": "tutorials/beginner.html#select",
    "title": "Anfänger-Tutorial",
    "section": "Select",
    "text": "Select\nMit select können die Werte einzelner Unterfelder in eine CSV-Datei exportiert werden. Dabei können mehrere Unterfelder kombiniert werden. Man kann aus riesigen Datenbeständen exakt die Daten extrahieren, die man für weitere Datenanalysen benötigt.\nDer Selektionsausdruck enthält eine durch Kommas getrennte Liste von Unterfeldern, die ausgelesen werden sollen, z. B.:\n$ pica select \"002@.0, 003@.0\" testdaten.dat -o test-select.csv\nDas Ergebnis ist eine CSV-Datei mit zwei Spalten, in diesem Beispiel einer Spalte für die Satzart und einer Spalte für die IDN.\nWenn Felder mehrere Unterfelder haben, können diese in einer Liste in geschweiften Klammern an die Feldbezeichnung angehängt werden.\n$ pica select \"002@.0, 003@.0, 021A{a,h}\" testdaten.dat -o test-select.csv\nIn die Selektionsausdrücke können auch Filterausdrücke eingebaut werden. Dazu muss die erste Position der Liste in den geschweiften Klammern ergänzt werden.\n$ pica select \"003@.0, 028A{ (9,d,a) | 4 == 'aut' }\" testdaten.dat -o test-select.csv\nIn diesem Beispiel werden die Angaben zur Person aus Feld 028A nur übernommen, wenn Unterfeld 4 den Wert aut enthält, die Person also Autor*in ist und nicht etwa Herausgeber*in.\nFür diese Filterausdrücke gelten dieselben Regeln wie für Filterausdrücke im filter-Tool, die oben beschrieben wurden.\nWenn Felder wiederholbar sind (z. B. bei Schlagworten), wird pro Wiederholung eine neue Zeile in die CSV ausgegeben. Die ausgegebene CSV-Datei kann also mehr Zeilen enthalten, als Datensätze in den Ausgangsdaten waren. Es empfiehlt sich deshalb einen eindeutigen Identifikator mitzuselektieren, damit die wiederholten Felddaten von neuen Datensätzen unterschieden werden können.\nEs können auch Spaltennamen für die CSV-Ausgabe angegeben werden mit der Option -H. Wichtig: die Anzahl Spaltennamen muss der Anzahl der selektierten Unterfelder entsprechen.\n$ pica select -H \"idn, autor-idn, autor-vorname, autor-nachname\" \\\n    \"003@.0, 028A{ 9,d,a | 4 == 'aut' }\" testdaten.dat -o test-select.csv\n\n\n\n\n\n\nHinweis\n\n\n\nDie doppelte Filtermöglichkeit einmal mit dem filter-Kommando und einmal im select-Kommando verwirrt auf den ersten Blick etwas. filter prüft eine oder mehrere Felder oder Unterfelder auf Bedingungen und gibt den gesamten Datensatz aus, wenn die Bedingung wahr ist. select prüft ebenfalls auf Bedingungen und selektiert dann die benötigten Teildaten. Man könnte auch sagen: filter arbeitet auf Datensatzebene und select auf Feldebene.",
    "crumbs": [
      "Tutorials",
      "Anfänger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#arbeit-mit-großen-datenabzügen",
    "href": "tutorials/beginner.html#arbeit-mit-großen-datenabzügen",
    "title": "Anfänger-Tutorial",
    "section": "Arbeit mit großen Datenabzügen",
    "text": "Arbeit mit großen Datenabzügen\npica parst immer den kompletten Datenbestand, auch wenn man nur wenige Ergebnisse erwartet. Deshalb ist es manchmal sinnvoll, die Ausgangsdatei in kleinere Dateien zu teilen, die dann viel schneller verarbeitet werden können.\nIn unseren Testdaten haben wir Titeldaten und Normdaten zusammen. Es könnte z.B. sinnvoll sein, die Normdaten zu extrahieren, wenn man keine Titeldaten braucht oder nur eine bestimmte Satzart zu extrahieren, wenn man nur innerhalb dieser Satzart suchen will.",
    "crumbs": [
      "Tutorials",
      "Anfänger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/rmarkdown.html",
    "href": "tutorials/rmarkdown.html",
    "title": "Datenanalyse mit RMarkdown",
    "section": "",
    "text": "Datenabfrage\nBeispielhaft werden in diesem Tutorial demonstriert, wie Datenanalysen mit RMarkdown mithilfe von pica durchgeführt wird. Datenabfrage mit pica und Datenanalyse mit R können so in einem gemeinsamen Dokument kombiniert werden.\nZunächst erzeugen wir mit folgender Pica-Abfrage aus der Datei TEST.dat.gz eine CSV-Datei mit allen Tupeln aus der IDN des Titels und IDN der GND-Entität. Um die GND-Entitäten abzurufen, die mit einem Titeldatensatz verknüpft sind, müssen die Felder 041A/* mit dem select-Kommando ausgelesen werden. Sind mehrere GND-Entitäten in einem Titel-Datensatz vohanden, expandiert das select-Kommando diese automatisch, d.h., in der Ausgabe title_idn_gnd_idn.csv kommen auch IDN-Nummern von Titeln ggf. mehrfach vor (je eine Zeile pro GND-Entität).\nDas Einlesen der Daten in R erfolgt über die üblichen Funktionen, z.B. aus dem tidyverse-Unterpaket readr.\nDas Einlesen der Daten kann auch ohne Zwischenspeichern des Outputs von pica select erfolgen. Die Ausgabe von pica select wird dann direkt als Stream an read_csvübergeben, was bei größeren Datenabfragen deutlich performanter ist:",
    "crumbs": [
      "Tutorials",
      "Datenanalyse mit RMarkdown"
    ]
  },
  {
    "objectID": "tutorials/rmarkdown.html#datenabfrage",
    "href": "tutorials/rmarkdown.html#datenabfrage",
    "title": "Datenanalyse mit RMarkdown",
    "section": "",
    "text": "$ pica select -s --no-empty-columns -H \"idn_titel, idn_sw\" \"003@.0,041A/*.9\" \\\n  DUMP.dat.gz -o title_idn_gnd_idn.csv\n\ntitle_idn_gnd_idn &lt;- read_csv(\"title_idn_gnd_idn.csv\",\n    col_types = list(\n        idn_titel = col_factor(),\n        idn_sw = col_factor()))\n\ntitle_idn_gnd_idn &lt;- pipe(\n  'pica select -s --no-empty-columns -H \\\"idn_titel, idn_sw\\\" \\\"003@.0,041A/*.9\\\" DUMP.dat.gz') |&gt;\n  read_csv(col_types = 'cc')",
    "crumbs": [
      "Tutorials",
      "Datenanalyse mit RMarkdown"
    ]
  },
  {
    "objectID": "tutorials/rmarkdown.html#datenanalyse",
    "href": "tutorials/rmarkdown.html#datenanalyse",
    "title": "Datenanalyse mit RMarkdown",
    "section": "Datenanalyse",
    "text": "Datenanalyse\nWir wollen die abgefragten Daten im Folgenden beispielhaft analysieren. Zunächst erzeugen wir eine zusammenfassende Zählstatistik:\ncount_summary &lt;- title_idn_gnd_idn |&gt;\n    summarise(\n        n_title = n_distinct(idn_titel),\n        n_terms = n_distinct(idn_sw)\n    )\nIn dem so erzeugten Datensatz gibt es 245 verschiedene GND-Entitäten, die an mindestens einen von 103 verschiedenen Titel-Datensätzen vergeben wurden.\nAls nächstes beantworten wir die Frage, wie viele GND-Entitäten pro Titel im Durchschnitt verwendet werden:\nsw_per_title &lt;- title_idn_gnd_idn |&gt;\n    group_by(idn_titel) |&gt;\n    summarise(n_sw = n())\n\ncard_d &lt;- sw_per_title |&gt;\n    summarise(\n        avg_count = mean(n_sw)\n    ) |&gt;\n    pull(avg_count)\nIm Mittel liegen 2.77 GND-Entitäten pro Titel vor.\nWir wollen nun die IDN-Nummern der am häufigsten verwendeten GND-Entitäten ausgeben:\ngnd_term_freq &lt;- title_idn_gnd_idn |&gt;\n  group_by(idn_sw) |&gt;\n  summarise(freq = n()) |&gt;\n  arrange(desc(freq))\n\nhead(gnd_term_freq, n = 10) |&gt;\n  knitr::kable()\n\nHäufigkeitsverteilung GND-Entitäten pro Titel\n\n\nidn_sw\nfreq\n\n\n\n\n040118827\n10\n\n\n040305503\n5\n\n\n040538818\n4\n\n\n041245113\n3\n\n\n041248538\n3\n\n\n041879635\n3\n\n\n040509265\n2\n\n\n040013073\n2\n\n\n118636405\n2\n\n\n040526925\n2",
    "crumbs": [
      "Tutorials",
      "Datenanalyse mit RMarkdown"
    ]
  },
  {
    "objectID": "tutorials/rmarkdown.html#datenvisualisierung",
    "href": "tutorials/rmarkdown.html#datenvisualisierung",
    "title": "Datenanalyse mit RMarkdown",
    "section": "Datenvisualisierung",
    "text": "Datenvisualisierung\nÜber alle Schlagwörter betrachtet sieht die Verteilung der GND-Entitäten-Verwendung wie folgt aus:\ng &lt;- ggplot(gnd_term_freq, aes(x = freq)) +\n  geom_histogram(aes(y = after_stat(density)), binwidth = 1) +\n  scale_y_continuous(labels = scales::percent, limits = c(0,1)) +\n  scale_x_continuous(breaks = c(0,2,4,6,8,10)) +\n  xlab(\"Häufigkeit GND-Entitäten-Verwendung\") +\n  ylab(\"Anteil an allen GND-Entitäten\") +\n  ggtitle(\"Verteilung GND-Entitäten-Verwendung\")\n\ng\n\n\n\n\n\nDas Histogramm zeigt, dass die meisten GND-Entitäten nur einmal verwendet werden. Einzelne Terme werden aber auch bis zu 10-mal verwendet.\nEine andere Visualisierungsform des gleichen Sachverhalts ist folgende:\ngnd_term_freq &lt;- gnd_term_freq |&gt;\n  mutate(index = 1:n())\n\nggplot(gnd_term_freq, aes(x = index, y = freq)) +\n    geom_point() +\n    scale_y_log10() +\n    ylab(\"Häufigkeit je GND-Entität\") +\n    xlab(\"Index GND-Entitäten (sortiert nach Häufigkeit)\") +\n    ggtitle(\"Long-Tail-Darstellung\")",
    "crumbs": [
      "Tutorials",
      "Datenanalyse mit RMarkdown"
    ]
  },
  {
    "objectID": "commands/completions.html",
    "href": "commands/completions.html",
    "title": "completions",
    "section": "",
    "text": "Bash\nDas completions-Kommando erzeugt Dateien, die Anweisungen enthalten, welche Argumente und Optionen des Toolkits für eine Shell zur Befehlszeilenergänzung verfügbar sind.\nNachfolgend werden exemplarisch die Befehle gezeigt, die für die Einbindung in die jeweilige Shell nötig sind. Die Schritte sind vom System sowie der Nutzereinstellung abhängig und müssen ggf. angepasst werden.\nEs werden folgende Shells unterstützt:\nAlternativ kann auch immer die aktuelle Version, passend zur installierten pica-Version, eingebunden werden. Dafür muss folgende Zeile in die .bashrc eingetragen werden:",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#bash",
    "href": "commands/completions.html#bash",
    "title": "completions",
    "section": "",
    "text": "$ mkdir -p ~/.local/share/bash-completion/completions\n$ pica completions bash \\\n    -o  ~/.local/share/bash-completion/completions/pica\n\n$ source &lt;(pica completions bash)\n\nBash (macOS/Homebrew)\n$ mkdir -p $(brew --prefix)/etc/bash_completion.d\n$ pica completions bash \\\n    -o $(brew --prefix)/etc/bash_completion.d/pica.bash-completion",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#elvish",
    "href": "commands/completions.html#elvish",
    "title": "completions",
    "section": "Elvish",
    "text": "Elvish\n$ mkdir -p ~/.local/share/elvish/lib/completions\n$ pica completions elvish -o ~/.local/share/elvish/lib/completions/pica.elv\n$ echo \"use completions/pica\" &gt;&gt; ~/.elvish/rc.elv",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#fish",
    "href": "commands/completions.html#fish",
    "title": "completions",
    "section": "Fish",
    "text": "Fish\n$ mkdir -p ~/.config/fish/completions\n$ pica completions fish -o ~/.config/fish/completions/pica.fish",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#powershell",
    "href": "commands/completions.html#powershell",
    "title": "completions",
    "section": "Powershell",
    "text": "Powershell\n$ pica completions powershell \\\n     &gt;&gt; ${env:USERPROFILE}\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#zsh",
    "href": "commands/completions.html#zsh",
    "title": "completions",
    "section": "ZSH",
    "text": "ZSH\nDer Pfad ~/.zfunc muss in der Variable $fpath gesetzt sein, bevor die Funktion compinit aufgerufen wird.\n$ pica completions zsh -o ~/.zfunc/_pica",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/concat.html",
    "href": "commands/concat.html",
    "title": "concat",
    "section": "",
    "text": "Optionen\nDas concat-Kommando (Alias cat) liest Datensätze direkt von der Standardeingabe (stdin) oder aus Dateien ein und fügt diese zusammen. Die Ausgabe kann entweder in eine Datei oder in die Standardausgabe (stdout) geschrieben werden.\nDer wichtigste Anwendungsfall des Kommandos besteht in Kombination mit den Kommandos partition oder split, da mittels concat das Ergebnis dieser Kommandos (teil-)rückgängig gemacht werden kann. Häufig macht es Sinn, eine große Datei in viele kleinere Dateien anhand eines Kriteriums zu teilen (bspw. nach der Sprache), um anschließend einzelne Dateien wieder zusammenzufügen.\nDas folgende Beispiel fügt die Datensätze aus den Dateien ger.dat und eng.dat zu einer Datei ger_eng.dat zusammen:",
    "crumbs": [
      "Kommandos",
      "concat"
    ]
  },
  {
    "objectID": "commands/concat.html#optionen",
    "href": "commands/concat.html#optionen",
    "title": "concat",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-u, --unique\n\nEs werden keine Duplikate in die Ausgabe geschrieben. Die Strategie zur Erkennung von Duplikaten wird mittels der Option --unique-strategy festgelegt.\n\n--unique-strategy &lt;STRATEGY&gt;\n\nFestlegen, wie Duplikate erkannt werden sollen. Standardmäßig ist der Wert idn ausgewählt, der Duplikate anhand der PPN aus dem Feld 003@.0 erkannt. Alternativ kann die Strategie hash gewählt werden. Der Vergleich erfolgt dann über die SHA-256 Prüfsumme der Datensätze.\n\n--append\n\nWenn die Ausgabedatei bereits existiert, wird die Ausgabe an die Datei angehangen. Ist das Flag nicht gesetzt, wird eine bestehende Datei standardmäßig überschrieben.\n\n--tee &lt;filename&gt;\n\nAbzweigen der Ausgabe in eine zusätzliche Datei.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben. Endet der Dateiname mit dem Suffix .gz, wird die Ausgabe automatisch im Gzip-Format komprimiert.",
    "crumbs": [
      "Kommandos",
      "concat"
    ]
  },
  {
    "objectID": "commands/concat.html#beispiele",
    "href": "commands/concat.html#beispiele",
    "title": "concat",
    "section": "Beispiele",
    "text": "Beispiele\n\nÜberspringen ungültiger Datensätze\nDer eingangs verwendete Befehl geht davon aus, dass die zwei Partitionen ausschließlich gültige Datensätze enthalten. Gültig in diesem Zusammenhang bedeutet, dass es sich um valide Datensätze im Format PICA+ handelt und nicht ob ein Datensatz einem bestimmten Regelwerk entspricht.\nDas Ausschließen von ungültigen Datensätzen wird mit der Option -s oder --skip-invalid erreicht:\n$ pica concat --skip-invalid DUMP.dat.gz -o dump_valid.dat\n$ pica concat -s DUMP.dat.gz --output dump_valid.dat.gz\nAlternativ lässt sich das Überspringen ungültiger Datensätze mittels des config-Kommandos einstellen.\n\n\nLesen von der Standardeingabe\nDas Kommando kann auch direkt von der Standardeingabe (stdin) lesen. Das ist bspw. dann hilfreich, wenn die Ausgabe aus einem vorhergehenden Pipeline-Schritt mit dem Inhalt einer oder mehrerer Dateien konkateniert werden soll.\nDas folgende Beispiel liest im ersten Pipeline-Schritt die Datei dump1.dat ein, entfernt ungültige Datensätze und gibt die Ausgabe nach stdout aus. Der zweite Pipeline-Schritt liest diese Datensätze ein (-) und konkateniert diese mit den Datensätzen aus der Datei dump2.dat. Das Ergebnis wird in die Datei out.dat geschrieben.\n$ pica concat -s dump1.dat | pica cat - dump2.dat -o out.dat\nDer Dateiname - steht für die Standardeingabe (stdin). Wären die zwei Argumente vertauscht (pica cat dump2.dat -), dann würden erst die gültigen Datensätze aus der Datei dump1.dat und anschließend die Datensätze aus dem ersten Pipeline-Schritt geschrieben.\n\n\nHinzufügen von Datensätzen\nWenn eine Ausgabedatei bereits existiert, wird diese standardmäßig neu angelegt und überschrieben. Soll das Verhalten dahingehend geändert werden, dass an die bestehenden Dateien angehangen wird, kann dies mit der --append-Option erreicht werden. Diese Option ändert das Verhalten von --output und --tee. Die Option hat auf das Verhalten beim Schreiben in die Standardausgabe keine Auswirkung.\nIm folgenden Beispiel erzeugt der erste Befehl eine neue Datei gnd.dat. Sollte die Datei bereits existieren, wird der Datei-Inhalt überschrieben. Die darauffolgenden Kommandos hängen jeweils an das Ende der Datei gnd.dat an.\n$ pica concat Tp*.dat -o gnd.dat\n$ pica concat --append Ts*.dat -o gnd.dat\n$ pica concat --append Tu*.dat -o gnd.dat\n...\n\n\nAbzweigen der Ausgabe\nGelegenlich kann es nützlich sein, die Ausgabe des concat-Kommandos in eine Datei zu schreiben und gleichzeitig die Ausgabe an einen weiteren Pipeline-Schritt weiterzureichen. Dies hat den Vorteil, dass zwei CPU-Kerne gleichzeitig genutzt werden können. Mit der --tee-Option lässt sich dieses Verhalten erzielen. Der Name der Option leitet sich von dem T-Stück (engl. tee connector) ab, mit dem ein Klempner eine Abzeigung in eine Leitung einbaut.\nIm folgenden Beispiel werden alle Tp*.dat zusammengefügt und in eine Datei Tp.dat geschrieben. Gleichzeitig werden alle Datensätze mit dem filter-Kommando nach der Satzart Tp2 im Feld 002@.0 gefiltert.\n$ pica concat partitions/Tp*.dat --tee gnd_person.dat | \\\n    pica filter \"002@.0 =^ 'Tp2'\" -o Tp2.dat\n\n\nEntfernen von Duplikaten\nDuplikate können durch die Angabe des Flags --unique (-u) entfernt werden. Standardmäßig erfolgt die Erkennung von Duplikaten per PPN aus dem Feld 003@.0 . Alternativ kann durch die Angabe der Option --unique-strategy die Variante hash ausgewählt werden, bei der nur solche Datensätze als gleich gewertet werden, bei denen alle Bytes gleich sind.\n$ pica concat --unique --unique-strategy hash ger.dat eng.dat -o out.dat\n$ pica concat --unique --unique-strategy idn ger.dat eng.dat -o out.dat",
    "crumbs": [
      "Kommandos",
      "concat"
    ]
  },
  {
    "objectID": "commands/config.html",
    "href": "commands/config.html",
    "title": "config",
    "section": "",
    "text": "Optionen\nMithilfe des config-Kommandos lassen sich bestimmte Optionen setzen und das Laufzeitverhalten von pica beeinflussen. Falls noch keine Konfigurationsdatei existiert, wird diese automatisch angelegt und je nach Betriebssystem in den dafür vorgesehenen Pfaden gespeichert:",
    "crumbs": [
      "Kommandos",
      "config"
    ]
  },
  {
    "objectID": "commands/config.html#optionen",
    "href": "commands/config.html#optionen",
    "title": "config",
    "section": "",
    "text": "Überspringen ungültiger Datensätze\nKann eine Zeile in der Eingabe nicht als Datensatz (normalisiertes PICA+) dekodiert werden, brechen die meisten Kommandos die Verarbeitung mit einer Fehlermeldung ab. Dieses Verhalten kann mit der Option --skip-invalid geändert werden, sodass diese ungültigen Datensätze übersprungen werden. Dieses Verhalten kann auch in der Konfigurationsdatei hinterlegt werden:\n$ pica config skip-invalid true\nNachdem die Variable gesetzt wurde, kann die Angabe der --skip-invalid-Option entfallen. Die Einstellung lässt sich mit --unset rückgängig machen:\n$ pica config --unset skip-invalid\n\n\nÄndern der Unicode Normalform\nLiegen die PICA-Daten in einer anderen Unicode Normalform vor, lassen sich Filterausdrücke mit der Option normalization an die Normalform der Daten angleichen:\n$ pica config normalization nfd\nEs werden die vier Normalformen nfd, nfc, nfkc und nfkd unterstützt. Nur wenn eine Normalform ausgewählt ist, werden Filterausdrücke immer entsprechend transliteriert. Die Einstellung lässt sich mit --unset rückgängig machen:\n$ pica config --unset normalization",
    "crumbs": [
      "Kommandos",
      "config"
    ]
  },
  {
    "objectID": "commands/convert.html",
    "href": "commands/convert.html",
    "title": "convert",
    "section": "",
    "text": "Optionen\nDas PICA-Format kann in verschiedene Datenformate serialisiert werden. Das convert-Kommando ermöglicht es, Datensätze von einem Format in ein anderes Format zu konvertieren. Es bietet insbesondere die Möglichkeit, Datensätze, die nicht in normalisiertem PICA+ vorliegen, nach PICA+ zu konvertieren, um sie durch andere Kommandos verarbeiten zu können.\nFolgende Formate werden unterstützt:\nDie Angabe der Datenformate erfolgt über die Optionen --from/-f und --to/-t:",
    "crumbs": [
      "Kommandos",
      "convert"
    ]
  },
  {
    "objectID": "commands/convert.html#optionen",
    "href": "commands/convert.html#optionen",
    "title": "convert",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-f &lt;format&gt;, --from &lt;format&gt;\n\nAuswahl des Datenformats der Eingabe.\n\n-t &lt;format&gt;, --to &lt;format&gt;\n\nAuswahl des Datenformats der Ausgabe.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o &lt;filename&gt;, --output &lt;filename&gt;\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "convert"
    ]
  },
  {
    "objectID": "commands/count.html",
    "href": "commands/count.html",
    "title": "count",
    "section": "",
    "text": "Optionen\nSoll die Anzahl der Datensätze und deren Felder sowie Unterfelder ermittelt werden, kann dies mit dem count-Kommando erfolgen. Ungültige Datensätze können mit dem Flag --skip-invalid (bzw. -s) übersprungen werden. Im folgenden Beispiel wird die Datei DUMP.dat.gz eingelesen und die Anzahl der in der Datei enthaltenen Datensätzes (records), Felder (_fields) und Unterfelder (subfields) ausgegeben:",
    "crumbs": [
      "Kommandos",
      "count"
    ]
  },
  {
    "objectID": "commands/count.html#optionen",
    "href": "commands/count.html#optionen",
    "title": "count",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGroß- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim Ähnlichkeitsvergleich von Zeichenketten mittels =*.\n\n--append\n\nWenn die Ausgabedatei bereits existiert, wird die Ausgabe an die Datei angehangen. Ist das Flag nicht gesetzt, wird eine bestehende Datei standardmäßig überschrieben.\n\n--records\n\nGibt nur die Anzahl der vorhandenen Datensätze aus. Dieses Flag ist nicht mit den Optionen --fields, --subfields, --csv, --tsv und --no-header kombinierbar.\n\n--fields\n\nGibt nur die Anzahl der vorhandenen Felder aus. Dieses Flag ist nicht mit den Optionen --records, --subfields, --csv, --tsv und --no-header kombinierbar.\n\n--subfields\n\nGibt nur die Anzahl der vorhandenen Unterfelder aus. Dieses Flag ist nicht mit den Optionen --records, --fields, --csv, --tsv und --no-header kombinierbar.\n\n--csv\n\nDie Ausgabe erfolgt im CSV-Format.\n\n--tsv\n\nDie Ausgabe erfolgt im TSV-Format.\n\n--no-header\n\nEs wird keine Kopfzeile in die Ausgabe geschrieben.\n\n--where &lt;expr&gt;\n\nAngabe eines Filters, um Datensätze aus der Eingabe auszuwählen.\n\n--and &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen &&-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen ||-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "count"
    ]
  },
  {
    "objectID": "commands/count.html#beispiele",
    "href": "commands/count.html#beispiele",
    "title": "count",
    "section": "Beispiele",
    "text": "Beispiele\n\nAusgabe im CSV/TSV-Format\nDie Ausgabe des Kommandos kann auch im Format CSV bzw. TSV erfolgen, was die Weiterverarbeitung in anderen Programmen erleichtert. Die Ausgabe der Kopfzeile lässt sich mit dem Flag --no-header ausschalten.\n$ pica count -s --csv DUMP.dat.gz\nrecords,fields,subfields\n12,1035,3973\n\n$ pica count -s --tsv DUMP.dat.gz\nrecords fields  subfields\n12  1035    3973\n\n$ pica count -s --csv --no-header DUMP.dat.gz\n12,1035,3973\n\n\nAusgabe in eine Datei\nDie Ausgabe des Kommandos wird standardmäßig auf der Konsole ausgegeben. Diese kann mit der Option --output (bzw. -o) in eine Datei umgeleitet werden. Soll diese Datei eine neue Zeile erhalten und nicht bei jedem Aufruf überschrieben werden, kann dies mit dem Flag --append erzielt werden.\n$ pica count -s --csv -o count.csv DUMP.dat.gz\n$ cat count.csv\nrecords,fields,subfields\n12,1035,3973\n\n$ pica count -s --csv --append -o count.csv DUMP.dat.gz\n$ cat count.csv\nrecords,fields,subfields\n12,1035,3973\n12,1035,3973\n\n\nAusgabe von Einzelwerten\nSoll entweder die Anzahl von Datensätzen, Feldern oder Unterfeldern ausgegeben werden, kann dies mit den Flags --records, --fields bzw. --subfields erfolgen. Diese Flags schließen sich gegenseitig aus und können nicht mit den Flags --csv, --tsv und --no-header kombiniert werden.\n$ pica count -s --records DUMP.dat.gz\n12\n\n$ pica count -s --fields DUMP.dat.gz\n1035\n\n$ pica count -s --subfields DUMP.dat.gz\n3973\n\n\nAnwendungsbeispiel\nSoll die Veränderung (Anzahl Datensätze, Felder, Unterfelder) eines PICA-Abzugs über die Zeit verfolgt werden, könnte dies wie folgt erreicht werden:\n$ echo \"date,records,fields,subfields\" &gt; count.csv # Kopfzeile\n$ pica count -s dump_20220222.dat.gz --append -o count.csv # Initialer Aufruf\n$ pica count -s dump_20220223.dat.gz --append -o count.csv # Aufruf nach x Tagen\n\n$ cat count.csv\n$ records,fields,subfields\n7,247,549\n9,347,1022\nSoll auch das aktuelle Datum vor die Zeile geschrieben werden, könnten bspw. folgende Unix-Kommandos genutzt werden:\n# Schreiben der Kopfzeile\n$ echo \"date,records,fields,subfields\" &gt; count.csv\n\n# Aufruf am 22.02.2022\n$ pica count -s --no-header --csv dump_20220222.dat.gz | \\\n    xargs -d\"\\n\" -I {} date +\"%Y-%m-%d,{}\" &gt;&gt; count.csv\n\n# Aufruf am 23.02.2022\n$ pica count -s --no-header --csv dump_20220223.dat.gz | \\\n    xargs -d\"\\n\" -I {} date +\"%Y-%m-%d,{}\" &gt;&gt; count.csv\n\n$ cat count.csv\n$ date,records,fields,subfields\n2022-02-22,7,247,549\n2022-02-23,9,347,1022",
    "crumbs": [
      "Kommandos",
      "count"
    ]
  },
  {
    "objectID": "commands/explode.html",
    "href": "commands/explode.html",
    "title": "explode",
    "section": "",
    "text": "Optionen\nDie Verarbeitung und Analyse von Datensätzen auf Lokal- bzw. Exemplarebene ist mitunter nur unzureichend möglich, da Filterausdrücke die Grenzen von untergeordneten Ebenen nicht respektiert. Abhilfe kann das explode-Kommando schaffen, das einen Datensatz in einzelne Lokal- bzw. Exemplardatensätze aufteilen kann. Dabei werden alle Felder der übergeordneten Ebenen mit in die Datensätze übernommen.\nDas Aufteilen der Datensätze erfolgt durch die Angabe der Ebene (level) an der der Datensatz geteilt werden soll. Es können folgende Werte ausgewählt werden:\nSoll ein Datensatz in alle Lokaldatensätze aufgeteilt werden, muss die Ebene local ausgewählt werden. Die neu erstellten Datensätze enthalten alle Titeldaten (Felder der Ebene main), den Identifikator des Lokaldatensatzes (Feld 101@.a) sowie alle Exemplare, die diesem Lokaldatensatz zugeordnet werden.\nWird darüber hinaus für jedes Exemplar ein eigenständiger Datensatz benötigt, muss die Ebene copy angegeben werden. Jeder erzeugte Datensatz enthält die Titeldaten (Felder der Ebene main), den Identifikator des Lokaldatensatzes (Feld 101@.a) und nur die Felder, die zum jeweiligen Exemplar gehören.\nSchließlich kann ein Datensatz auch auf Ebene der Titeldaten (main) aufgeteilt werden. Diese Auswahl verändert nichts am Datensatz und gibt den vollständigen Datensatz mit allen Feldern aus.\nAls Beispiel soll folgender (reduzierter) Datensatz dienen:\nDieser Datensatz lässt sich in zwei Datensätze auf Ebene der Lokaldaten aufteilen:\nSoll jedes Exemplar ein eigenständiger Datensatz werden, wird dies durch Angabe von copy erzielt:\n-k &lt;expr&gt;, --keep &lt;expr&gt; Es werden nur die Felder eines Datensatzes beibehalten, die in der Liste aufgeführt werden.",
    "crumbs": [
      "Kommandos",
      "explode"
    ]
  },
  {
    "objectID": "commands/explode.html#optionen",
    "href": "commands/explode.html#optionen",
    "title": "explode",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGroß- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim Ähnlichkeitsvergleich von Zeichenketten mittels =*.\n\n\n\n\n-d, --discard\n\nEs werden die Felder eines Datensatzes verworfen, die in der Liste aufgeführt werden.\n\n-l &lt;n&gt;, --limit &lt;n&gt;\n\nEingrenzung der Ausgabe auf die ersten &lt;n&gt; (gültigen) Datensätze.\n\n--where &lt;filter&gt;\n\nAngabe eines Filters, der auf die erzeugten Datensätze angewandt wird.\n\n--and &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen &&-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen ||-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o &lt;filename&gt;, --output &lt;filename&gt;\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben. Endet der Dateiname mit dem Suffix .gz, wird die Ausgabe automatisch im Gzip-Format komprimiert.",
    "crumbs": [
      "Kommandos",
      "explode"
    ]
  },
  {
    "objectID": "commands/explode.html#beispiele",
    "href": "commands/explode.html#beispiele",
    "title": "explode",
    "section": "Beispiele",
    "text": "Beispiele\n\nEingrenzen der Datensätze\nIst nur eine Teilmenge der erzeugten Datensätze von Interesse, lässt sich die Ergebnismenge durch Hinzufügen eines Filterausdrucks eingrenzen.\nWerden bspw. nur die Exemplare mit dem Identifikator 101@.a == \"1\" benötigt, kann die Eingrenzung durch Angabe der --where-Option eingegrenzt werden:\n$ pica explode -s copy --where '101@.a == \"1\"' COPY.dat.gz -o copy.dat\n$ pica print copy.dat\n003@ $0 123456789\n002@ $0 Abvz\n101@ $a 1\n203@/01 $0 0123456789\n\n003@ $0 123456789\n002@ $0 Abvz\n101@ $a 1\n203@/02 $0 1234567890",
    "crumbs": [
      "Kommandos",
      "explode"
    ]
  },
  {
    "objectID": "commands/filter.html",
    "href": "commands/filter.html",
    "title": "filter",
    "section": "",
    "text": "Optionen\nDas filter-Kommando bildet das Herzstück des pica-Tools. Es ermöglicht es, aus einer (mitunter sehr großen) Datenmenge, bspw. dem Gesamtabzug des Katalogsystems, eine kleinere Menge effizient zu extrahieren, um sie anschließend weiterzuverarbeiten. Dies erfolgt über die Angabe eines Filterausdrucks, der darüber entscheidet, ob ein Datensatz in die Zielmenge eingeht oder nicht.\nIm folgenden Beispiel werden alle Datensätze aus der Datei DUMP.dat.gz extrahiert, die ein Feld 003@ enthalten, das ein Unterfeld 0 besitzt, welches mit dem Wert 118540238 belegt ist.",
    "crumbs": [
      "Kommandos",
      "filter"
    ]
  },
  {
    "objectID": "commands/filter.html#optionen",
    "href": "commands/filter.html#optionen",
    "title": "filter",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-v, --invert-match\n\nGibt alle Datensätze aus, die nicht dem Filterausdruck entsprechen.\n\n-i, --ignore-case\n\nGroß- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim Ähnlichkeitsvergleich von Zeichenketten mittels =*.\n\n-k, --keep\n\nEs werden nur die Felder eines Datensatzes beibehalten, die in der Liste aufgeführt werden.\n\n-d, --discard\n\nEs werden die Felder eines Datensatzes verworfen, die in der Liste aufgeführt werden.\n\n-F &lt;filename&gt;, --file &lt;filename&gt;\n\nEs wird der Filterausdruck aus der Datei &lt;filename&gt; eingelesen. Es darf keine weitere Angabe eines Filterausdrucks als Kommandozeilenargument erfolgen!\n\n-A &lt;filename&gt;, --allow-list &lt;filename&gt;\n\nEs werden alle Datensätze ignoriert, die nicht explizit in der Positivliste1 auftauchen. Werden mehrere Positivlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-D &lt;filename&gt;, --deny-list &lt;filename&gt;\n\nEs werden alle Datensätze ignoriert, die in der Negativliste auftauchen. Werden mehrere Negativlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-l &lt;n&gt;, --limit &lt;n&gt;\n\nEingrenzung der Ausgabe auf die ersten &lt;n&gt; (gültigen) Datensätze.\n\n--and &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen &&-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen ||-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n--append\n\nWenn die Ausgabedatei bereits existiert, wird die Ausgabe an die Datei angehangen. Ist das Flag nicht gesetzt, wird eine bestehende Datei überschrieben.\n\n--tee &lt;filename&gt;\n\nAbzweigen der Ausgabe in eine zusätzliche Datei.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o &lt;filename&gt;, --output &lt;filename&gt;\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben. Endet der Dateiname mit dem Suffix .gz, wird die Ausgabe automatisch im gzip-Format komprimiert.",
    "crumbs": [
      "Kommandos",
      "filter"
    ]
  },
  {
    "objectID": "commands/filter.html#beispiele",
    "href": "commands/filter.html#beispiele",
    "title": "filter",
    "section": "Beispiele",
    "text": "Beispiele\n\nInvertierte Treffermenge (invert match)\nMitunter ist es einfacher, einen Ausdruck zu formulieren, der alle Datensätze umfasst, die nicht in der Treffermenge gewünscht sind. Durch die Option -v/--invert-match werden dann nur die Datensätze ausgegeben, die nicht dem Filterkriterum entsprechen.\nBeispielweise enthält der Abzug DUMP.dat.gz verschiedene Normdatensätze. Werden nur die Datensätze benötigt, die nicht vom Satztyp Werk sind, ist es einfacher, zuerst nach den Werken zu suchen und dann durch das Flag -v alle Datensätze zu erhalten, die nicht dem Filterkriterium entsprechen.\n$ pica filter -s -v '002@.0 =^ \"Tu\"' DUMP.dat.gz -o not-Tu.dat.gz\n$ pica frequency '002@.0' not-Tu.dat.gz\nTsz,2\nTg1,1\nTp1,1\nTpz,1\nTs1,1\n\n\nGroß- und Kleinschreibung ingorieren\nStandardmäßig wird bei Vergleichen von Zeichenketten die Groß- und Kleinschreibung beachtet. Dies lässt sich mit dem Flag -i/--ignore-case deaktivieren:\n$ pica filter -s -i '028A.a == \"GOETHE\"' DUMP.dat.gz -o goethe.dat\n$ pica print goethe.dat\n...\n028A $d Johann Wolfgang $c von $a Goethe\n...\n\n\nFelder eines Datensatzes reduzieren\nTeilweise ist die Anzahl der Felder pro Datensatz sehr groß, was zu erheblichen Laufzeiteinbußen nachfolgender Datenanalysen führen kann. Mittels der Optionen -k/--keep bzw. -d/--discard lassen sich Datensätze auf eine Teilmenge der Felder reduzieren.\nWerden für eine Datenanalyse nur die IDN/PPN (003@), die Satzart (002@) und die Entitätenkodierung (004B) benötigt, können die Datensätze wie folgt auf die Felder reduziert werden:\n$ pica filter -s --keep '00[23]@,004B' \"003@?\" DUMP.dat.gz -o out.dat\n$ pica print out.dat\n002@ $0 Tpz\n003@ $0 118540238\n004B $a piz\n\n002@ $0 Tp1\n003@ $0 118607626\n004B $a piz\n...\nSollen bestimmte Felder entfernt werden, kann dies mit der Option -d/--discard erfolgen. Der folgende Aufruf entfernt das Feld 004B, sofern vorhanden, aus allen Datensätzen:\n$ pica filter -s --keep '00[23]@,004B' \"003@?\" DUMP.dat.gz -o out.dat\n$ pica filter --discard '004B' \"003@?\" out.dat -o out2.dat\n$ pica print out2.dat\n002@ $0 Tpz\n003@ $0 118540238\n\n002@ $0 Tp1\n003@ $0 118607626\n...",
    "crumbs": [
      "Kommandos",
      "filter"
    ]
  },
  {
    "objectID": "commands/filter.html#footnotes",
    "href": "commands/filter.html#footnotes",
    "title": "filter",
    "section": "",
    "text": "Eine Positiv- oder Negativliste muss entweder als CSV-Datei vorliegen oder als eine Arrow-Datei, die eine ppn- oder idn-Spalte enthält. Alle Dateien werden automatisch als CSV-Datei interpretiert, es sei denn, die Datei endet mit .ipc oder .arrow, dann erfolgt die Interpretation im Arrow-Format. CSV- bzw. TSV-Dateien mit der Ending .csv.gz bzw. .tsv.gz werden automatisch entpackt.↩︎",
    "crumbs": [
      "Kommandos",
      "filter"
    ]
  },
  {
    "objectID": "commands/frequency.html",
    "href": "commands/frequency.html",
    "title": "frequency",
    "section": "",
    "text": "Optionen\nDas Kommando frequency (Alias freq) wird dazu genutzt, um die Häufigkeiten der Werte ein oder mehrerer Unterfelder zu bestimmen. Ist das zu untersuchende Feld bzw. Unterfeld wiederholbar, gehen alle Wertausprägungen eines Datensatzes in die Häufigkeitsverteilung ein. Die Ausgabe erfolgt standardmäßig im CSV-Format. Im folgenden Beispiel wird die Häufigkeitsverteilung des Unterfelds 002@.0 (Satzart) ermittelt:\n-H &lt;header&gt;, --header &lt;header&gt; Kopfzeile, die den Ergebnissen vorangestellt wird; Spaltennamen werde mit einem Komma separiert.\n-t, --tsv Ausgabe erfolgt im TSV-Format.",
    "crumbs": [
      "Kommandos",
      "frequency"
    ]
  },
  {
    "objectID": "commands/frequency.html#optionen",
    "href": "commands/frequency.html#optionen",
    "title": "frequency",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGroß- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim Ähnlichkeitsvergleich von Zeichenketten mittels =*.\n\n--unique, -u\n\nDoppelte Werte eines Datensatzes werden ignorieren.\n\n--reverse\n\nErgebnisse werden in aufsteigender Reihenfolge ausgegeben.\n\n-A &lt;file&gt;, --allow-list &lt;file&gt;\n\nEs werden alle Datensätze ignoriert, die nicht explizit in der Positivliste1 auftauchen. Werden mehrere Positivlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-D &lt;file&gt;, --deny-list &lt;file&gt;\n\nEs werden alle Datensätze ignoriert, die in der Negativliste auftauchen. Werden mehrere Negativlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-l &lt;n&gt;, --limit &lt;n&gt;\n\nEingrenzung der Ausgabe auf die häufigsten &lt;n&gt; Unterfeldwerte.\n\n-t &lt;n&gt;, --threshold &lt;n&gt;\n\nZeilen mit einer Häufigkeit \\(&lt;\\) &lt;n&gt; ignorieren.\n\n--where &lt;filter&gt;\n\nAngabe eines Filters, der auf die erzeugten Datensätze angewandt wird.\n\n--and &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen &&-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen ||-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n\n\n\n\n--translit &lt;normalization&gt;\n\nAusgabe wird in die angegebene Normalform transliteriert. Mögliche Werte: nfd, nfkd, nfc und nfkc.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "frequency"
    ]
  },
  {
    "objectID": "commands/frequency.html#beispiele",
    "href": "commands/frequency.html#beispiele",
    "title": "frequency",
    "section": "Beispiele",
    "text": "Beispiele\n\nHinzufügen einer Kopfzeile\nFür die Dokumentation sowie die Verwendung in anderen Programmiersprachen ist es häufig sinnvoll, eine Kopfzeile hinzuzufügen. Dies erfolgt mit der Option --header bzw. -H. Die Namen der Spalten werden kommasepariert angegeben. Eine Angabe von mehr als zwei Spalten ist nicht erlaubt.\n$ pica frequency -s --header \"satzart,anzahl\" \"002@.0\" DUMP.dat.gz\nsatzart,anzahl\nTu1,6\nTsz,2\nTg1,1\nTp1,1\nTpz,1\nTs1,1\n\n\nAuswertung mehrerer Felder bzw. Unterfelder\nDurch die Angabe von mehreren Pfadausdrücken lässt sich eine Häufigkeitsverteilung über mehrere Untefelder ermitteln. Das folgende Beispiel berechnet die Häufigkeit der Kombination aus Satzart (002@.0) und dem Entitätencode(s) (004B.a):\n$ pica frequency -s -H \"bbg,ent,count\" \"002@.0, 004B.a\" DUMP.dat.gz\nbbg,ent,count\nTu1,wit,6\nTsz,saz,2\nTg1,gik,1\nTp1,piz,1\nTpz,piz,1\nTs1,saz,1\nEbenfalls können auch mehrere Unterfelder ausgewertet werden. Eine Auswertung der Häufigkeiten von verknüpften Sachbegriffen (Level 1) und dem GND-Code für Beziehungen im Feld 041R (Sachbegriff - Beziehung) wird wie folgt ermittelt:\n$ pica frequency -s '041R{(7,4) | 7 == \"Ts1\"}' DUMP.dat.gz\nTs1,beru,12\nTs1,obal,5\nTs1,vbal,3\nTs1,obge,1\nTs1,stud,1\n\n\nEingrenzung auf bestimmte Felder\nOftmals sollen nicht alle Felder in die Berechnung der Häufigkeiten miteinbezogen werden. Dies ist bspw. dann der Fall, wenn sich Felder anhand eines Unterfelds unterschieden lassen, wie etwa durch die Angabe der Metadatenherkunft. Durch Verwenden eines Pfad-Ausdrucks in {}-Notation können nur die Felder ausgewählt werden, die einem bestimmten Kriterium entsprechen.\nDas folgende Beispiel bezieht nur die Felder 041R in die Auswertung mit ein, bei denen ein Unterfeld 4 existiert, das entweder berc oder beru ist; Felder die diesem Kriterium nicht entsprechen, werden ignoriert.\n$ pica frequency -s \"041R{ 9 | 4 in ['berc', 'beru'] }\" DUMP.dat.gz\n040533093,2\n040250989,1\n040252434,1\n040290506,1\n...\n\n\nEingrenzen der Treffermenge\nSoll die Ergebnismenge auf die häufigsten n Unterfeldwerte eingeschränkt werden, wird dies mit der Option --limit bzw. -l erreicht. Das nachfolgende Beispeil ermittelt die 3 häufigsten Werte im Feld 041R.4.\n$ pica frequency -s --limit 3 \"041R.4\" DUMP.dat.gz\nberu,12\nobal,5\nvbal,4\n\n\nEingrenzen der Treffermenge (Schwellenwert)\nDie Treffermenge kann auch mittels der Angabe eines Schwellenwerts eingeschänkt werden. Sollen nur die Werte angezeigt werden, die ab einem Schwellenwert vorkommen, dann kann dies mit der Option --threshold/-t erzielt werden:\n$ pica frequency -s --threshold 4 \"041R.4\" DUMP.dat.gz\nberu,12\nobal,5\nvbal,4\n\n\nÄnderung der Sortierreihenfolge (Limit)\nStandardmäßig wird die Häufigkeitsverteilung absteigend ausgegeben, d.h., der häufigste Wert steht in der Ausgabe oben. Soll das Verhalten so geändert werden, dass die Ausgabe aufsteigend sortiert wird, kann dies mit der Option --reverse bzw. -r erfolgen. Das folgende Kommando sucht nach den vier Satzarten, die am wenigsten vorkommen:\n$ pica frequency -s -l 4 --reverse \"002@.0\" DUMP.dat.gz\nTg1,1\nTp1,1\nTpz,1\nTs1,1\n\n\nAusgabe im TSV-Format\nDie Ausgabe lässt sich mittels der Option --tsv (bzw. -t) in das TSV-Format ändern.\n$ pica frequency -s -l3 --tsv tests/data/dump.dat.gz\nTu1 6\nTsz 2\n...",
    "crumbs": [
      "Kommandos",
      "frequency"
    ]
  },
  {
    "objectID": "commands/frequency.html#footnotes",
    "href": "commands/frequency.html#footnotes",
    "title": "frequency",
    "section": "",
    "text": "Alle Werte mit gleicher Häufigkeit werden immer in lexikographisch aufsteigender Reihenfolge sortiert. Dies erfolgt unabhängig vom Parameter --reverse.↩︎",
    "crumbs": [
      "Kommandos",
      "frequency"
    ]
  },
  {
    "objectID": "commands/hash.html",
    "href": "commands/hash.html",
    "title": "hash",
    "section": "",
    "text": "Optionen\nMithilfe des Kommandos hash lässt sich eine Tabelle erzeugen, die in der ersten Spalte die PPN (Feld 003@.0) und in der zweiten Spalte den SHA-256-Hashwert des Datensatzes enthält.\nMitunter kommt es vor, dass eine regelmäßige und aufwändige Berechnung für Datensätze ausgeführt werden muss und es nicht praktikabel ist, die Berechnung über alle Datensätze durchzuführen. Mittels des hash-Kommandos können auf unterschiedlichen Abzügen die Hashwerte der Datensätze bestimmt werden. Durch Vergleich dieser Tabelle ist es möglich, die Datensätze zu identifizieren, die sich geändert haben, gelöscht oder neu hinzugefügt wurden. Das folgende Beispiel demonstriert die Erzeugung dieser Hash-Tabellen:",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/hash.html#optionen",
    "href": "commands/hash.html#optionen",
    "title": "hash",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-H &lt;header&gt;, --header &lt;header&gt;\n\nKopfzeile, die den Ergebnissen vorangestellt wird.\n\n-t, --tsv\n\nAusgabe erfolgt im TSV-Format.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o &lt;filename&gt;, --output &lt;filename&gt;\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll.",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/hash.html#beispiele",
    "href": "commands/hash.html#beispiele",
    "title": "hash",
    "section": "Beispiele",
    "text": "Beispiele\n\nAusgabe im TSV-Format\nDie Ausgabe lässt sich mittels der Option --tsv (bzw. -t) in das TSV-Format ändern.\n$ pica hash -s --tsv DUMP.dat.gz\nppn hash\n118540238   762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3\n118607626   8d75e2cdfec20aa025d36018a40b150363d79571788cd92e7ff568595ba4f9ee\n040993396   0361c33e1f7a80e21eecde747b2721b7884e003ac4deb8c271684ec0dc4d059a\n...\n\n\nHinzufügen einer Kopfzeile\nEine individuelle Kopfzeile lässt sich mit der Option --header (bzw. -H) ausgeben:\n$ pica hash -s --header 'idn,sha256' DUMP.dat.gz\nidn,sha256\n118540238,762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3\n118607626,8d75e2cdfec20aa025d36018a40b150363d79571788cd92e7ff568595ba4f9ee\n040993396,0361c33e1f7a80e21eecde747b2721b7884e003ac4deb8c271684ec0dc4d059a\n...",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/hash.html#anmerkung",
    "href": "commands/hash.html#anmerkung",
    "title": "hash",
    "section": "Anmerkung",
    "text": "Anmerkung\nZur Berechnung des SHA-256-Hashwerts wird der abschließende Zeilenumbruch mit einbezogen, um einen gleichen Hashwert zu erzeugen, der entsteht, wenn der Hashwert über die gesamte Zeile aus der Eingabe ermittelt wird. Im folgenden Beispiel wird zuerst der erste gültige Datensatz in die Datei 1.dat geschrieben. Anschließend wird der Hashwert einmal mit dem hash-Kommando und einmal mit dem Unix-Programm sha256sum gebildet. Beide Ergebnisse sind gleich.\n$ pica filter -s -l1 \"003@?\" DUMP.dat.gz -o 1.dat\n$ pica hash 1.dat\nppn,hash\n118540238,762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3\n\n$ sha256sum 1.dat\n762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/invalid.html",
    "href": "commands/invalid.html",
    "title": "invalid",
    "section": "",
    "text": "Beispiel\nBei der Verarbeitung von PICA-Daten kann es vorkommen, dass Zeilen in der Eingabe nicht als normalisiertes PICA+ dekodiert werden können. Diese ungültigen Zeilen lassen sich bei vielen Kommandos mit der Option --skip-invalid / -s überspringen, wobei die Anzahl der übersprungenen Zeilen nicht angezeigt wird. Es ist zu empfehlen, die Anzahl invalider Datensätze zu kontrollieren und einer Prüfung zu unterziehen, um diese ggf. zu korrigieren. Das invalid-Kommando findet diese Zeilen in der Eingabe und gibt diese wieder auf der Standardausgabe (stdout) aus. Durch Angabe der Option --output / -o kann die Ausgabe in eine Datei geschrieben werden.\nDer folgende Befehl findet alle ungültigen Datensätze aus der Datei DUMP.dat.gz und schreibt diese Zeile in die Datei invalid.dat:",
    "crumbs": [
      "Kommandos",
      "invalid"
    ]
  },
  {
    "objectID": "commands/invalid.html#beispiel",
    "href": "commands/invalid.html#beispiel",
    "title": "invalid",
    "section": "",
    "text": "$ pica invalid DUMP.dat.gz -o invalid.dat",
    "crumbs": [
      "Kommandos",
      "invalid"
    ]
  },
  {
    "objectID": "commands/partition.html",
    "href": "commands/partition.html",
    "title": "partition",
    "section": "",
    "text": "Optionen\nMittels des partition-Kommandos lassen sich Datensätze anhand eines Unterfelds in Partitionen einteilen.\nLassen sich Datensätze anhand von den Wertausprägungen in einem Unterfeld gruppieren, ist es mitunter hilfreich die Gesamtmenge der Datensätze in Partitionen aufzuteilen. Ist das Unterfeld, nach dem partitioniert werden soll, wiederholbar, sind die erzeugten Partitionen i.d.R. nicht disjunkt. Ein Datensatz der das Unterfeld nicht besitzt, geht in keine Partition ein.\nIm folgenden Beispiel wird pro Entitätencode im Feld 004B.a eine Partition erstellt, die alle GND-Entitäten enthält, die diesem Entitätencode zugeordnet sind.",
    "crumbs": [
      "Kommandos",
      "partition"
    ]
  },
  {
    "objectID": "commands/partition.html#optionen",
    "href": "commands/partition.html#optionen",
    "title": "partition",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGroß- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim Ähnlichkeitsvergleich von Zeichenketten mittels =*.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n-t &lt;string&gt;, --template &lt;string&gt;\n\nTemplate für die Dateinamen. Der Platzhalter {} wird durch den Namen der Partition ersetzt.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt.\n\n-o &lt;path&gt;, --outdir &lt;path&gt;\n\nAngabe, in welches Verzeichnis die Partitionen geschrieben werden sollen. Standardmäßig wird das aktuelle Verzeichnis verwendet.",
    "crumbs": [
      "Kommandos",
      "partition"
    ]
  },
  {
    "objectID": "commands/partition.html#beispiele",
    "href": "commands/partition.html#beispiele",
    "title": "partition",
    "section": "Beispiele",
    "text": "Beispiele\n\nEingrenzen der Partitionen\nSollen nicht alle Partitionen erstellt werden, kann die Anzahl der möglichen Partition durch die Angabe eines Filterausdrucks eingegrenzt werden:\n$ pica partition -s \"004B{a | a in ['piz', 'saz']}\" DUMP.dat.gz -o out\n$ tree out/\nout\n├── piz.dat\n└── saz.dat\n\n\nBenutzerdefinierte Dateinamen\nStandardmäßig werden die erstellten Partitionen nach den Werten im Unterfeld benannt. Der Dateiname kann individuell mit der -t/--template-Option angepasst werden. Jedes Vorkommen der Zeichenfolge {} im Template wird durch den Wert des Unterfelds ersetzt. Endet die Datei auf der Dateiendung .gz, wird die Ausgabe automatisch im Gzip-Format komprimiert.\n$ pica partition -s \"004B.a\" --template \"code_{}.dat.gz\" DUMP.dat.gz -o out\n$ tree out/\nout\n├── code_gik.dat.gz\n├── code_piz.dat.gz\n├── code_saz.dat.gz\n└── code_wit.dat.gz\n\n\nKomprimierte Ausgabe\nMittels der Option --gzip bzw. -g erfolgt eine Komprimierung der Ausgabe:\n$ pica partition -s \"004B.a\" --gzip DUMP.dat.gz -o out\n$ tree out/\nout\n├── gik.dat.gz\n├── piz.dat.gz\n├── saz.dat.gz\n└── wit.dat.gz",
    "crumbs": [
      "Kommandos",
      "partition"
    ]
  },
  {
    "objectID": "commands/print.html",
    "href": "commands/print.html",
    "title": "print",
    "section": "",
    "text": "Optionen\nMithilfe des print-Kommandos können Datensätze in einer menschenlesbaren Form auf dem Terminal ausgegeben oder in eine Datei geschrieben werden. Das Format ist an die Darstellung in der WinIBW angelehnt: Felder werden zeilenweise ausgegeben; zuerst wird das Feld (bspw. 003@), dann - sofern vorhanden - die Okkurrenz (bspw. /01), und schließlich die Liste von Unterfeldern ausgegeben. Dem Unterfeld-Code wird ein Dollarzeichen vorangestellt. Die Unterfeldwerte werden genau so ausgegeben, wie sie im Datensatz vorhanden sind; es findet kein [Escaping] von Sonderzeichen statt. Einzelne Datensätze werden durch eine Leerzeile voneinander getrennt.\nDer folgende Befehl gibt den ersten Datensatz aus:",
    "crumbs": [
      "Kommandos",
      "print"
    ]
  },
  {
    "objectID": "commands/print.html#optionen",
    "href": "commands/print.html#optionen",
    "title": "print",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-l &lt;number&gt;, --limit &lt;number&gt;\n\nEingrenzung der Ausgabe auf die ersten n Datensätze.\n\n--translit &lt;nf&gt;\n\nAusgabe wird in die angegebene Normalform transliteriert. Mögliche Werte: nfd, nfkd, nfc und nfkc.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "print"
    ]
  },
  {
    "objectID": "commands/print.html#beispiele",
    "href": "commands/print.html#beispiele",
    "title": "print",
    "section": "Beispiele",
    "text": "Beispiele\n\nTransliteration der Ausgabe\nStandardmäßig werden die Unterfeldwerte so ausgegeben, wie sie im Datensatz vorkommen. Mit der Option --translit werden die Werte in die angegebene Unicode-Normalform transliteriert.\n$ pica print -s -l1 --translit nfc DUMP.dat.gz\n...\n028@ $d Yohan Wolfgang $a Gete\n028@ $d Yôhân Wôlfgang $c fôn $a Gete\n028@ $d Yôhan Wolfgang $a Gête\n028@ $d Yohann Volfqanq $a Gete\n028@ $d Yogann Volʹfgang $a Gete\n...\n028@ $T 01 $U Cyrl $L uzb $d Йоҳанн Волфганг $a Гёте\n028@ $T 01 $U Hans $P 歌德 $5 DE-576\n028@ $T 01 $U Hant $P 約翰・沃爾夫岡・馮・歌德 $5 DE-576\n028@ $T 01 $U Hans $P 约翰・沃尔夫冈・冯・歌德 $5 DE-576\n028@ $T 01 $U Jpan $d ヨハン・ヴォルフガング・フォン $a ゲーテ $5 DE-576\n028@ $T 01 $U Hebr $d יוהן וולפגנג פון $a גתה\n028@ $T 01 $U Hans $P 歌德\n028@ $T 01 $U Jpan $d ヨハン・ヴォルフガング・フォン $a ゲーテ\n028@ $T 01 $U Geor $d იოჰან ვოლფგანგ ფონ $a გოეთე\n028A $d Johann Wolfgang $c von $a Goethe\n...",
    "crumbs": [
      "Kommandos",
      "print"
    ]
  },
  {
    "objectID": "commands/sample.html",
    "href": "commands/sample.html",
    "title": "sample",
    "section": "",
    "text": "Optionen\nDas sample-Kommando zieht nach dem Zufallsprinzip gleichmäßig Datensätze aus der Eingabe.\nIm folgenden Beispiel werden zufällig 200 Datensätze aus der Eingabe ausgewählt und in die Datei samples.dat geschrieben:",
    "crumbs": [
      "Kommandos",
      "sample"
    ]
  },
  {
    "objectID": "commands/sample.html#optionen",
    "href": "commands/sample.html#optionen",
    "title": "sample",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n--seed &lt;number&gt;\n\nInitialisiert den Zufallszahlengenerator mit einem seed-Wert, um eine deterministische Auswahl zu erhalten.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt.\n\n-i, --ignore-case\n\nGroß- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim Ähnlichkeitsvergleich von Zeichenketten mittels =*.\n\n--where &lt;filter&gt;\n\nAngabe eines Filters, der auf die eingelesenen Datensätze angewandt wird.\n\n--and &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen &&-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen ||-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n-A &lt;file&gt;, --allow-list &lt;file&gt;\n\nEs werden alle Datensätze ignoriert, die nicht explizit in der Positivliste1 auftauchen. Werden mehrere Positivlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-D &lt;file&gt;, --deny-list &lt;file&gt;\n\nEs werden alle Datensätze ignoriert, die in der Negativliste auftauchen. Werden mehrere Negativlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-o &lt;path&gt;, --outdir &lt;path&gt;\n\nAngabe, in welches Verzeichnis die Partitionen geschrieben werden sollen. Standardmäßig wird das aktuelle Verzeichnis verwendet.",
    "crumbs": [
      "Kommandos",
      "sample"
    ]
  },
  {
    "objectID": "commands/sample.html#beispiele",
    "href": "commands/sample.html#beispiele",
    "title": "sample",
    "section": "Beispiele",
    "text": "Beispiele\n\nZufällige PPN-Liste\nIn Kombination mit dem select-Kommando kann eine zufällige PPN-Liste erzeugt werden:\n$ pica sample 3 DUMP.dat.gz | pica select -H 'ppn' '003@.0' -o samples.csv",
    "crumbs": [
      "Kommandos",
      "sample"
    ]
  },
  {
    "objectID": "commands/sample.html#footnotes",
    "href": "commands/sample.html#footnotes",
    "title": "sample",
    "section": "",
    "text": "Eine Positiv- oder Negativliste muss entweder als CSV-Datei vorliegen oder als eine [Arrow]-Datei, die eine ppn- oder idn-Spalte enthält. Alle Dateien werden automatisch als CSV-Datei interpretiert, es sei denn, die Datei endet mit .ipc oder .arrow, dann erfolgt die Interpretation im [Arrow]-Format. CSV- bzw. TSV-Dateien mit der Endung .csv.gz bzw. .tsv.gz werden automatisch entpackt. Ist sowohl eine ppn- als auch eine idn-Spalte vorhanden, wird die ppn-Spalte genutzt.↩︎",
    "crumbs": [
      "Kommandos",
      "sample"
    ]
  },
  {
    "objectID": "commands/select.html",
    "href": "commands/select.html",
    "title": "select",
    "section": "",
    "text": "Optionen\nMit dem select-Kommando werden Werte eines unter mehrerer Unterfelder tabelliert. Dies ermöglicht weiterführende Datenanalysen in Excel, Python oder R.\nIm folgenden Beispiel wird die PPN eines Datensatzes (Feld 003@.0) und die dazugehörige Satzart Feld (002@.0) in einer Tabelle im CSV-Format erzeugt:\nIst ein Feld oder Unterfeld mehrfach vorhanden, werden pro Datensatz alle Zeilen mit diesen Werten kombiniert, indem das Kartesische Produkt gebildet wird. Dadurch ist es möglich für jede wiederholte Wertausprägung eine Zeile zu erzeugen. In Kombination mit einem nicht-wiederholten Feld (bspw. der PPN im Feld 003@.0 lassen sich Tabellen, im Sinne des Entity-Relationship-Modell, erzeugen. Im Folgenden wird eine Tabelle erstellt, die in der ersten Spalte die PPN und in der zweiten Spalte, die dazugehörigen Teilbestandskennzeichen aus dem Feld 008A.a enthält. Jede Kombination von einer PPN mit einem Teilbestandskennzeichen erzeugt eine neue Zeile:\n-t, --tsv Ausgabe erfolgt im TSV-Format.",
    "crumbs": [
      "Kommandos",
      "select"
    ]
  },
  {
    "objectID": "commands/select.html#optionen",
    "href": "commands/select.html#optionen",
    "title": "select",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n--squash\n\nWenn das Flag gesetzt ist, werden wiederholte Unterfelder als eine Zeichenkette zusammengefasst. Die einzelnen Werte werden durch einen Separator (siehe --separator) getrennt.\n\n--merge\n\nIn jeder Spalte werden alle Zeilen zu einer Zeile zusammengefasst. Die einzelnen Werte werden durch einen Separator (siehe --separator) getrennt. Ist die Option gesetzt wird für jeden Datensatz maximal eine Zeile erzeugt.\n\n--separator &lt;value&gt;\n\nFestlegen des Separators, der für --squash und --merge genutzt wird. Standardmäßig wird der Separator | verwendet.\n\n--no-empty-columns\n\nIst die Option gesetzt, werden nur Zeilen geschrieben, in denen jede Spalte einen nicht-leeren Wert enthält.\n\n--unique, -u\n\nMehrfach vorkommende Zeilen werden ignorieren.\n\n-i, --ignore-case\n\nGroß- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim Ähnlichkeitsvergleich von Zeichenketten mittels =*.\n\n-A &lt;file&gt;, --allow-list &lt;file&gt;\n\nEs werden alle Datensätze ignoriert, die nicht explizit in der Positivliste[^1] auftauchen. Werden mehrere Positivlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-D &lt;file&gt;, --deny-list &lt;file&gt;\n\nEs werden alle Datensätze ignoriert, die in der Negativliste auftauchen. Werden mehrere Negativlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n--where &lt;filter&gt;\n\nAngabe eines Filters, der auf die erzeugten Datensätze angewandt wird.\n\n--and &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen &&-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen ||-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n-H &lt;header&gt;, --header &lt;header&gt;\n\nKopfzeile, die den Ergebnissen vorangestellt wird; Spaltennamen werde mit einem Komma separiert.\n\n\n\n\n--translit &lt;normalization&gt;\n\nAusgabe wird in die angegebene Normalform transliteriert. Mögliche Werte: nfd, nfkd, nfc und nfkc.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n--limit &lt;n&gt;\n\nLimitiert die Aufbereitung auf die ersten &lt;n&gt; Datensätze.\n\n--append\n\nIst die Option gesetzt, wird die Ausgabe an das Ende der Ausgabedatei geschrieben, anstatt diese zu überschreiben.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "select"
    ]
  },
  {
    "objectID": "commands/slice.html",
    "href": "commands/slice.html",
    "title": "slice",
    "section": "",
    "text": "Optionen\nMittels des slice-Kommandos kann ein zusammenhängender Teilbereich aus der Eingabe ausgeschnitten werden. Dabei wird der Teilbereich als halb-offenes Intervall angegeben, wobei die Positionen von 0 an gezählt werden. Beim Auftreten eines ungültigen Datensatzes wird die Position weitergezählt. Enthält bspw. die Eingabe 1.000 Zeilen, mit 990 Datensätzen und 10 ungültigen Zeilen, dann sind die Positionen von 0 bis 999 durchnummeriert.\nDas folgende Beispiel extrahiert alle (gültigen) Datensätze aus den Positionen 2 bis 4:",
    "crumbs": [
      "Kommandos",
      "slice"
    ]
  },
  {
    "objectID": "commands/slice.html#optionen",
    "href": "commands/slice.html#optionen",
    "title": "slice",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n--start &lt;number&gt;\n\nStartposition des Teilbereichs (Voreinstellung: 0).\n\n--end &lt;number&gt;\n\nEndposition des Teilbereichs; diese Position ist nicht mehr Teil der Ausgabe. Ist keine Endposition angegeben, wird der Teilbereich bis zum Ende der Eingabe fortgeführt. Diese Option ist nicht kombinierbar mit --length.\n\n--length &lt;number&gt;\n\nFestlegen der maximalen Anzahl an Datensätzen, die in der Ausgabe enthalten sind. Diese Option kann nicht mit --end kombiniert werden.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n--append\n\nWenn die Ausgabedatei bereits existiert, wird die Ausgabe an die Datei angehangen. Ist das Flag nicht gesetzt, wird eine bestehende Datei standardmäßig überschrieben.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "slice"
    ]
  },
  {
    "objectID": "commands/slice.html#beispiele",
    "href": "commands/slice.html#beispiele",
    "title": "slice",
    "section": "Beispiele",
    "text": "Beispiele\n\nAusschneiden eines Teilbereichs fester Größe\nWenn die Eingabe ausreichend Datensätze enthält, kann beginnend bei einer festen Position (--start) ein Teilbereich mit einer festen Länge (--length) ausgeschnitten werden.\nIm folgenden Beispiel wird beginnend beim dritten Datensatz (Position 2) ein Teilbereich mit einer Länge von zwei ausgeschnitten:\n$ pica slice -s --start 2 --length 2 DUMP.dat.gz -o slice.dat\n$ pica count --records slice.dat\n2",
    "crumbs": [
      "Kommandos",
      "slice"
    ]
  },
  {
    "objectID": "commands/split.html",
    "href": "commands/split.html",
    "title": "split",
    "section": "",
    "text": "Optionen\nMithilfe des split-Kommandos können alle Datensätze aus der Eingabe in mehrere Dateien aufgeteilt werden, wobei jede Datei eine maximale Anzahl an Datensätzen nicht überschreitet. Ein Anwendungsfall für das Splitting könnte eine automatisierte Stapelverarbeitung oder die parallele Verarbeitung der entstandenen Dateien sein.\nDer folgende Aufruf des split-Kommandos teilt die zwölf Datensätze der Eingabe (DUMP.dat.gz) in drei Dateien mit maximal fünf Datensätzen pro Datei. Die ersten beiden Dateien (0.dat und 1.dat) enthalten die maximale Anzahl an Datensätzen, die letzte Datei (2.dat) die restlichen zwei Datensätze.",
    "crumbs": [
      "Kommandos",
      "split"
    ]
  },
  {
    "objectID": "commands/split.html#optionen",
    "href": "commands/split.html#optionen",
    "title": "split",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n--template\n\nTemplate für die Dateinamen. Der Platzhalter {} wird durch eine fortlaufende Nummer ersetzt.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt.\n\n-o &lt;path&gt;, --outdir &lt;path&gt;\n\nAngabe, in welches Verzeichnis die Ausgabe geschrieben werden soll. Standardmäßig wird das aktuelle Verzeichnis verwendet.",
    "crumbs": [
      "Kommandos",
      "split"
    ]
  },
  {
    "objectID": "commands/split.html#beispiele",
    "href": "commands/split.html#beispiele",
    "title": "split",
    "section": "Beispiele",
    "text": "Beispiele\n\nBenutzerdefinierte Dateinamen\nStandardmäßig werden die erstellten Dateien beginnend bei 0 durchnummeriert. Der Dateiname kann individuell mit der --template-Option angepasst werden. Jedes Vorkommen der Zeichenfolge {} im Template wird durch die aktuelle Nummer ersetzt. Endet die Datei auf der Dateiendung .gz, wird die Ausgabe automatisch im Gzip-Format komprimiert.\n$ pica split -s --template \"CHUNK_{}.dat.gz\" 10 DUMP.dat.gz\n$ tree\n.\n├── CHUNK_0.dat.gz\n├── CHUNK_1.dat.gz\n└── DUMP.dat.gz\n\n$ pica count --records CHUNK_0.dat.gz\n10\n\n$ pica count --records CHUNK_1.dat.gz\n2\n\n\nKomprimierte Ausgabe\nMittels der Option --gzip bzw. -g erfolgt eine Komprimierung der Ausgabe:\n$ pica split -s --gzip 10 DUMP.dat.gz\n$ tree\n.\n├── 0.dat.gz\n├── 1.dat.gz\n└── DUMP.dat.gz\n\n$ pica count --records 0.dat.gz\n10\n\n$ pica count --records 1.dat.gz\n2",
    "crumbs": [
      "Kommandos",
      "split"
    ]
  }
]