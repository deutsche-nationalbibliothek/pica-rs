[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pica-rs",
    "section": "",
    "text": "Start\n\nDas Projekt pica-rs erm√∂glicht eine effiziente Verarbeitung von bibliografischen Metadaten, die in PICA+, dem internen Format des OCLC-Katalogsystems, kodiert sind. Das Programm pica stellt unterschiedliche Kommandos zur Verf√ºgung, um Daten auszuw√§hlen, statistisch aufzubereiten oder f√ºr die Weiterverarbeitung in Data Science-Frameworks wie Polars (Python) oder der Sprache R nutzbar zu machen. Die Anwendung ist in der Programmiersprache Rust geschrieben und l√§sst sich unter den Betriebsystemen Linux, macOS und Windows verwenden. Die Kommandos lassen sich √ºber die Standard-Datenstr√∂me (Kombination von verschiedenen Programmen mittels Unix-Pipelines) miteinander verketten, wodurch sich leicht Metadaten-Workflows erstellen und automatisieren lassen.\nDie Entwicklung von pica-rs wurde vom Referat Automatische Erschlie√üungsverfahren; Netzpublikationen (AEN) der Deutsche Nationalbibliothek (DNB) initiert und wird dort f√ºr die Erstellung von Datenanalysen sowie f√ºr die Automatisierung von Workflows (Datenmanagement) im Rahmen der automatischen Inhaltserschlie√üung genutzt. Weiterhin wird es zur Unterst√ºtzung der Forschungsarbeiten im KI-Projekt sowie f√ºr diverse andere Datenanalysen innerhalb der DNB eingesetzt.",
    "crumbs": [
      "Start"
    ]
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "Installation",
    "section": "",
    "text": "Installation unter Linux\nDas Kommandozeilen-Tool pica l√§sst sich unter den Betriebssystemen Linux, macOS und Windows nutzen. Folgend wird die Installation sowie Einrichtung und Konfiguration des Tools beschrieben. Die Zeichenkette X.Y.Z ist ein Platzhalter f√ºr eine konkrete pica-rs Version und muss in dem Befehl entsprechend ersetzt werden.\nAbh√§ngig von der genutzten Linux-Distribution, gibt es unterschiedliche M√∂glichkeiten der Installation. Vorgefertigte Releases stehen auf der Plattform GitHub zum Download bereit.",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#installation-unter-linux",
    "href": "install.html#installation-unter-linux",
    "title": "Installation",
    "section": "",
    "text": "Debian und Ubuntu\nUnter Debian GNU/Linux und Ubuntu Linux k√∂nnen fertige DEB-Pakete genutzt werden. Diese k√∂nnen mit dem dpkg-Programm installiert werden:\n$ dpkg -i pica_X.Y.Z-glibc2.35-1_amd64.deb\n\n\nRed Hat, SUSE und CentOS\nF√ºr die Distributionen Red Hat Linux, SUSE Linux und CentOS Linux stehen fertige RPM-Pakete zum Download bereit, die sich mit dem rpm-Programm installieren lassen:\n$ rpm -i pica-X.Y.Z-glibc2.35-1.x86_64.rpm\n\n\nBinary Releases\nSoll pica nicht √ºber einen Paketmanager installiert werden, stehen f√ºr die Zielarchitektur x86_64-unknown-linux-gnu mit den glibc-Versionen 2.28, 2.31 und 2.35 fertige Binary Releases zur Verf√ºgung. Die glibc-Version des Systems l√§sst sich mit dem Aufruf ldd --version ermitteln.\nDas tar-Archiv enth√§lt neben dem Tool pica auch weitere Dateien wie bspw. Shell-Skripte zur Befehlszeilenerg√§nzung:\n$ tar -tf\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35.tar.gz\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/pica\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/README.md\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/pica.zsh\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/LICENSE\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/pica.fish\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/pica.bash\nEine systemweite Installation von pica in das Verzeichnis /usr/local/bin kann mit dem install erfolgen. Hierf√ºr sind ggf. root-Rechte n√∂tig:\n$ tar xfz pica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35.tar.gz\n$ sudo install -m755 pica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/pica \\\n      /usr/local/bin/pica",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#installation-unter-macos",
    "href": "install.html#installation-unter-macos",
    "title": "Installation",
    "section": "Installation unter macOS",
    "text": "Installation unter macOS\nUnter macOS wird nur die Zielarchitektur x86_64-apple-darwin (macOS 10.7+, Lion+) unterst√ºtzt. Diese l√§sst sich analog wie unter Linux installieren:\n$ tar xfz pica-X.Y.Z-x86_64-apple-darwin.tar.gz\n$ install -m755  pica-X.Y.Z-x86_64-apple-darwin/pica /usr/local/bin/pica",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#installation-unter-windows",
    "href": "install.html#installation-unter-windows",
    "title": "Installation",
    "section": "Installation unter Windows",
    "text": "Installation unter Windows\nUnter Windows (x86_64-pc-windows-gnu oder x86_64-pc-windows-msvc) kann das Programm direkt dem zip-Archiv entnommen werden. Nach einem Wechsel in das Verzeichnis, in dem sich die pica.exe befindet, kann das Programm direkt genutzt werden. Soll pica aus jedem beliebigen Verzeichnis heraus aufrufbar sein, dann muss der Installationspfad in der PATH-Umgebungsvariable mit aufgelistet werden.",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#aus-dem-quellcode-installieren",
    "href": "install.html#aus-dem-quellcode-installieren",
    "title": "Installation",
    "section": "Aus dem Quellcode installieren",
    "text": "Aus dem Quellcode installieren\nDas Projekt l√§sst sich auch direkt aus den Quellen kompilieren. Hierf√ºr wird eine aktuelle Rust-Version (&gt;= 1.74.1) mit dem Paketmanager cargo ben√∂tigt.\nDer aktuelle Entwicklungsstand l√§sst sich wie folgt installieren:\n$ git clone https://github.com/deutsche-nationalbibliothek/pica-rs.git\n$ cd pica-rs\n$ cargo build --release\nDas fertige pica-Programm liegt im Verzeichnis target/release/ und kann bspw. in das Verzeichnis /usr/local/bin installiert werden:\n$ install -m755 target/release/pica /usr/local/bin/pica\nWenn der Quellcode nicht ben√∂tigt wird, kann das Projekt auch direkt √ºber den Paketmanager cargo installiert werden:\n$ # Installation der aktuellen Entwicklungsversion\n$ cargo install --git https://github.com/deutsche-nationalbibliothek/pica-rs \\\n     --branch main pica-cli\n\n$ # Installation der Version X.Y.Z\n$ cargo install --git https://github.com/deutsche-nationalbibliothek/pica-rs \\\n      --tag vX.Y.Z pica-cli\nDas fertige Programm befindet sich dann unter ~/.cargo/bin/pica.\n\nFeatures\nWird das Programm anhand der Quellen gebaut, k√∂nnen optionale Features aktiviert werden.Die folgenden Funktionen k√∂nnen mit der cargo-Option --features aktiviert werden:\n\nunstable, um die neuesten Funktionalit√§ten, die noch in der Entwicklung sind und f√ºr eine der n√§chsten Versionen vorgesehen sind, zu aktivieren\nund compat, um eine h√∂here Kompatibilit√§t mit der Abfragesprache PICA Path zu erhalten.",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "tutorials/beginner.html",
    "href": "tutorials/beginner.html",
    "title": "Anf√§nger-Tutorial",
    "section": "",
    "text": "Was ist pica-rs?\npica ist ein Set von Kommandozeilen-Tools zur Arbeit mit PICA+-Bibliothekskatalog-Daten. Wenn Sie nicht wissen, was PICA-Daten sind, brauchen Sie diese Tools nicht. üòâ Gro√üe Datenabz√ºge bis hin zu Gesamtabz√ºgen k√∂nnen schnell gefiltert werden und es k√∂nnen die Daten einzelner Felder und Unterfelder in CSV-Dateien exportiert werden, H√§ufigkeitsverteilungen des Inhalts einzelner Unterfeldern erfasst werden und vieles mehr.",
    "crumbs": [
      "Tutorials",
      "Anf√§nger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#wie-funktioniert-pica",
    "href": "tutorials/beginner.html#wie-funktioniert-pica",
    "title": "Anf√§nger-Tutorial",
    "section": "Wie funktioniert pica?",
    "text": "Wie funktioniert pica?\nDas Tool kann mit extrem gro√üen Dateien umgehen, weil es diese sequentiell ausliest und prozessiert. Die Dateien werden nicht ge√∂ffnet und in den Arbeitsspeicher geladen, sondern ‚Ä∫h√§ppchenweise‚Äπ ausgewertet. Es ist deswegen kein Rechner mit besonders viel Arbeitsspeicher notwendig. Es empfiehlt sich aber, die Ausgangsdaten auf m√∂glichst schnellen lokalen Laufwerken abzulegen. Netzlaufwerke sind weniger geeignet und verlangsamen das Tool unn√∂tig.\npica l√§uft unter Windows, Linux und Mac OS.",
    "crumbs": [
      "Tutorials",
      "Anf√§nger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#installation",
    "href": "tutorials/beginner.html#installation",
    "title": "Anf√§nger-Tutorial",
    "section": "Installation",
    "text": "Installation\nEs ist m√∂glich, die Quelldateien herunterzuladen und direkt auf dem eigenen Rechner von Rust zu einem lauff√§higen Programm kompilieren zu lassen.\nF√ºr die g√§ngigen Windows-, Apple- oder Linux-Systeme, stehen aber fertige Programmpakete unter https://github.com/deutsche-nationalbibliothek/pica-rs/releases zur Verf√ºgung.\n\nWindows\nLegen Sie die Datei pica.exe im Verzeichnis C:\\Users\\&lt;IHR USERNAME&gt;\\AppData\\Local\\Microsoft\\WindowsApps ab.\nStarten Sie die Eingabeaufforderung cmd.exe, indem Sie auf den Windows Startbutton klicken und dann cmd tippen. Ihnen wird das Programm jetzt angezeigt. Tippen Sie pica --version ein. Sie erhalten jetzt die Version von pica angezeigt, jetzt wissen aber, dass das Programm korrekt installiert ist.\n\n\nLinux/Mac OS\nEntpacken Sie das Paket und legen es in einen beliebigen Ordner.\nIn der Konfigurationsdatei Ihres Terminals m√ºssen Sie dann noch den Pfad angeben, in dem Sie das Programm abgelegt haben.\nBeispiel: ZSH unter Linux oder MacOS\nDie versteckte Datei .zshrc liegt √ºblicherweise im Homeverzeichnis des aktuellen Benutzers. Dort f√ºgen Sie an beliebiger Stelle folgende Zeile hinzu:\nexport PATH=\"/&lt;PFADZUMPROGRAMM&gt;:$PATH\"\nwobei &lt;PFADZUMPROGRAMM&gt; nat√ºrlich durch Ihren tats√§chlichen Pfad ersetzt werden muss.\nNach einem Neustart des Terminals sollte jetzt der neue Befehl pica zur Verf√ºgung stehen. Mit pica -V k√∂nnen Sie testen, welche Version sie haben.",
    "crumbs": [
      "Tutorials",
      "Anf√§nger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#kommandozeile",
    "href": "tutorials/beginner.html#kommandozeile",
    "title": "Anf√§nger-Tutorial",
    "section": "Kommandozeile",
    "text": "Kommandozeile\npica ist auch deswegen sehr schnell, weil es kein grafisches Interface hat. Man sollte deshalb einige Basics der Kommandozeilen (auch Terminal oder Shell genannt) des jeweiligen Betriebssystems kennen. Alle Befehle werden hier in der Fassung f√ºr g√§ngige Linux-und Mac OS-Terminals gezeigt, abweichende Befehle der Windows Power Shell werden meistens in Klammern erw√§hnt.",
    "crumbs": [
      "Tutorials",
      "Anf√§nger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#pipes",
    "href": "tutorials/beginner.html#pipes",
    "title": "Anf√§nger-Tutorial",
    "section": "Pipes",
    "text": "Pipes\nUm das Tool optimal nutzen zu k√∂nnen, sollten Sie verstehen, was Pipes sind. Im Terminal wird die Ausgabe ausgef√ºhrter Programme oder Befehle √ºblicherweise in die sogenannte Standardausgabe (stdout) geschrieben. Normalerweise ist das die Bildschirmausgabe des Terminals selbst. Wenn sie z. B. den Inhalt des aktuellen Ordners mit ls (Windows: dir) auslesen, wird eine Liste aller Dateien und Ordner direkt im Terminal ausgegeben.\nSie k√∂nnten diese Ausgabe aber auch umleiten: z.B. in eine Datei oder auf einen angeschlossenen Drucker etc.\nPiping nennt man ein Verfahren, bei dem die Ausgabe eines Befehls direkt als Eingabe f√ºr einen weiteren Befehl verwendet wird. Wie Rohre (pipes) werden die Befehle aneinandergesteckt und die Daten flie√üen von einem Programm zum n√§chsten.\nDazu werden die Befehle mit einem senkrechten Strich verbunden: | Unter Linux und Windows ist dieser Strich normalerweise √ºber die Tastenkombination AltGr-&lt;AltGr-&lt; zu erreichen, unter MacOS √ºber Alt-7Alt-7.\nMan k√∂nnte also z. B. die Ausgabe von ls bzw. dir an einen Befehl weiterleiten, der die Anzahl der ausgegeben Zeilen z√§hlt. Dieser Befehl hei√üt wc -l (von word count -lines). Das korrekte Piping geht so:\n$ ls | wc -l\nDie Ausgabe von Word Count l√§sst sich wieder weiterleiten, z.B. in eine Datei:\n$ ls | wc -l &gt; ordnerinhalt.txt\nDer &gt;-Operator leitet den Inhalt in eine Datei weiter und ist eine Art Sonderfall des Pipings, der nur f√ºr das Schreiben in Dateien gilt.\nMan k√∂nnte die Ausgabe mit einer weiteren Pipe auch an noch einen weiteren Befehl √ºbergeben.\nMit Pipes lassen sich die einzelnen pica-rs-Tools (select, filter, frequency usw.) miteinander verkn√ºpfen. Die Ausgabe des einen Tools kann entweder zum n√§chsten Tool, in eine Datei oder einfach auf den Bildschirm geleitet werden. Alle Tools schreiben immer in die Standardausgabe. Will man die Ausgabe anders erhalten, muss man das dem Befehl mitteilen.",
    "crumbs": [
      "Tutorials",
      "Anf√§nger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#los-gehts",
    "href": "tutorials/beginner.html#los-gehts",
    "title": "Anf√§nger-Tutorial",
    "section": "Los geht‚Äôs",
    "text": "Los geht‚Äôs\nNavigieren Sie im Terminal zu dem Ordner, in dem ihre Daten liegen. Wir gehen davon aus, dass Sie im Hauptverzeichnis Ihres aktuellen Benutzers (unter Linux und Mac OS √ºber das K√ºrzel ~ zu erreichen) im Verzeichnis pica-test arbeiten. Das Testdatenpaket hei√üt testdaten.dat.\n$ cd ~/pica-test\n√úberpr√ºfen Sie, ob das Testdatenpaket vorhanden ist.\n$ ls # (unter Windows: dir)\ntotal 1872\ndrwxr-xr-x   3 testuser  staff    96B  9 Nov 14:24 .\ndrwxr-xr-x+ 76 testuser  staff   2,4K  9 Nov 14:25 ..\n-rw-r--r--@  1 testuser  staff   935K 14 Sep 18:30 testdaten.dat",
    "crumbs": [
      "Tutorials",
      "Anf√§nger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#print",
    "href": "tutorials/beginner.html#print",
    "title": "Anf√§nger-Tutorial",
    "section": "print",
    "text": "print\nWir beginnen mit mit print. Dieses Tool formatiert die unleserlichen PICA+-Daten zu gut lesbaren Datens√§tzen. Mit dem Befehl lassen sich die teilweise un√ºbersichtlichen Daten gut √ºberblicken. Wir wollen nur einen Datensatz aus den Testdaten auf dem Bildschirm ausgeben.\n$ pica print -l 1 testdaten.dat\nDie Option -l steht f√ºr Limit und begrenzt die Ausgabe auf einen Datensatz. Die folgende Ziffer gibt die Anzahl der auszugebenden Datens√§tze an.\nWir k√∂nnen die Ausgabe auch in eine Datei schreiben:\n$ pica print -l 1 testdaten.dat -o testdatensatz.txt\nWenn Sie nur einen Dateinamen angeben, wird die Datei im aktuellen Verzeichnis abgelegt. Wollen sie in ein anderes Verzeichnis schreiben, m√ºssen sie den kompletten Pfad dorthin angeben.\nIm Folgenden gehen wir davon aus, dass Sie grundlegend mit der Struktur von Pica-Daten vertraut sind, also z. B. Feldern und Unterfeldern, Satzarten, Codes etc.",
    "crumbs": [
      "Tutorials",
      "Anf√§nger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#filter",
    "href": "tutorials/beginner.html#filter",
    "title": "Anf√§nger-Tutorial",
    "section": "filter",
    "text": "filter\nMit filter k√∂nnen Teilmengen aus einem Daten-Dump nach einem bestimmten Selektionskriterium gebildet werden. filter gibt grunds√§tzlich den ganzen Datensatz aus, wenn die angegebenen Filterkriterien erf√ºllt sind.\nWir wissen, dass in unseren Testdaten jeweils 100 Datens√§tze der unterschiedlichen Satzarten enthalten sind. Wir wollen alle Oa-S√§tze herausfiltern und den ersten davon mit print ausgeben.\n$ pica filter -s \"002@.0 == 'Oa'\" testdaten.dat | pica print -l 1\nDas Ergebnis k√∂nnte man auch wieder in eine Datei schreiben:\n$ pica filter -s \"002@.0 == 'Oa'\" testdaten.dat -o oa-test.dat\n\n\n\n\n\n\nVorsicht\n\n\n\nDateien werden ohne R√ºckfrage √ºberschrieben und werden nicht im Papierkorb gesichert. Gew√∂hnen Sie sich am besten an, in ein eigenes Ausgabeverzeichnis zu schreiben oder f√ºgen Sie das aktuelle Datum an den Ausgabedateinamen an, damit sie nicht ausversehen eine √§ltere Datei √ºberschreiben.\n\n\n\nFilter-Ausdr√ºcke\nDer Filterausdruck in den doppelten Anf√ºhrungszeichen ist das m√§chtigste Werkzeug von pica-rs. Mehrere Ausdr√ºcke k√∂nnen zu komplexen Suchfiltern kombiniert werden.\nJeder Filterausdruck besteht immer aus einem Feld wie 002@, einem Unterfeldfilter wie .0, einem Operator, der angibt, wie der Inhalt des Feldes gefiltert werden soll, wie z. B. == und einem Wert, mit dem das Feld verglichen werden soll.\n\n\nFelder\nFelder k√∂nnen in der einfachsten Form direkt benannt werden: 002@ oder auch nummerierte Okkurrenzen haben wie /01. Okkurrenzen lassen sich nach ihrem Wert filtern oder alle Okkurrenzen k√∂nnen mit /* durchsucht werden. Bereiche von Okkurrenzen k√∂nnen ebenfalls eingegrenzt werden: 047A/01-03\n\n\nUnterfelder\nUnterfelder werden mit einem Punkt und ohne Dollarzeichen angeh√§ngt: 002@.9 meint Unterfeld $9 von Feld 002@.\nUm z. B. Unterfeld 9 aller Okkurrenzen von Feld 041A zu filtern, m√ºsste der Feldausdruck lauten: 041A/*.9.\n\n\nOperatoren\nWerte k√∂nnen mittels der folgenden Operatoren verglichen werden:\n\nGleichheit ==\n\nDer ==-Operator pr√ºft, ob es ein Unterfeld gibt, dass einem Wert entspricht. pica filter \"012A.a == 'abc'\" liest sich wie folgt: Es existiert ein Feld 012A mit einem Unterfeld a das gleich abc ist. Es k√∂nnten noch weitere Unterfelder a existieren, die nicht abc sind.\n\nUngleichheit !=\n\nDas Gegenst√ºck zu ==. Pr√ºft, ob ein Unterfeld existiert, das nicht einem Wert entspricht.\n\nBeginnt mit Prefix =^ (!^)\n\nDer Ausdruck wird dann wahr, wenn der Wert des Unterfelds mit dem angegebenen Prefix beginnt (=^) bzw. nicht beginnt (!^).\n\nEndet mit Suffix =$ (!$)\n\nDer Ausdruck wird dann wahr, wenn der Wert des Unterfelds mit dem angegebenen Suffix endet (=$) bzw. nicht beginnt (!$).\n\nRegul√§rem Ausdruck =~ (!~)\n\nPr√ºft ob ein Feld einem regul√§ren Ausdruck entspricht. Die Auswertung dieses Operators ben√∂tigt die meiste Rechenkapazit√§t. Er sollte deshalb nur dann verwendet werden, wenn er wirklich absolut notwendig ist. Es ist z. B. schneller, nach einer Kombination von =^ und =$ zu suchen als nach einem regul√§ren Ausdruck.\n\nEnthalten in in (not in)\n\nDer Ausdruck wird dann wahr, wenn der Wert des Unterfelds in der angegebenen enthalten ist (in) bzw. nicht enthalten ist (not in).\n\nTeilstring =?\n\nDer Ausdruck wird dann wahr, wenn der angegebenen Wert ein Teilstring des Unterfelds ist (=?).\n\nExistenz ?\n\nDer Ausdruck wird dann wahr, wenn das gesuchte Feld/Unterfeld existiert.\n\n√Ñhnlichkeit =*\n\nDer Ausdruck wird dann wahr, wenn der angegebene Wert √§hnlich dem des Unterfelds ist. Die gew√ºnschte Unsch√§rfe kann √ºber die Option --strsim-threshold parametrisiert werden.\n\n\nDie Operatoren k√∂nnen in runden Klammern gruppiert und mit den boolschen Operatoren UND &&, ODER || sowie XOR (^) verbunden werden.\n\n\nMehrere Felder adressieren\nEs kommt √∂fters vor, dass sich ein Wert vom gleichen Typ in unterschiedlichen Feldern befindet. Z. B. befindet sich im Feld 028A.9 die ‚ÄúPerson, Familie - 1. geistiger Sch√∂pfer‚Äù und im Feld 029A.9 ‚ÄúPerson, Familie - weitere geistige Sch√∂pfer‚Äù. Um Datens√§tze zu filtern, die entweder einen 1. geistigen Sch√∂pfer oder einen weiteren geistigen Sch√∂pfer haben, k√∂nnte man schreiben:\n$ pica filter \"028A.9? || 029A.9?\" testdaten.dat\nDer Ausdruck l√§sst sich vereinfachen zu:\n$ pica filter \"02[89]A.9?\" testdaten.dat\nAn jeder Position in einem Feld kann eine Liste der g√ºltigen Werte angegeben werden. Es wird dann jede m√∂gliche Kombination ausprobiert, um einen Match zu finden. Bsp. 0[12][34]A f√ºhrt zu der Liste 013A, 014A, 023A und 024A.\n\n\nMehrere Unterfelder adressieren\nSo √§hnlich k√∂nnen auch mehrere Unterfelder adressiert werden. Beispiel: Im Feld 045E befindet sich die Sachgruppe der Deutschen Nationabibliografie. Im Unterfeld $e die Hauptsachgruppe (HSG) und im Feld $f die Nebensachgruppen (NSG). Ist man an allen Datens√§tzen interessiert, die zur HSG 100 oder zur NSG 100 geh√∂ren, k√∂nnte man folgenden Filter schreiben:\n$ pica filter \"045E.e == '100' || 045E.f == '100'\" testdaten.dat\nDer Ausdruck l√§sst sich verk√ºrzen zu:\n$ pica filter \"045E.[ef] == '100'\" testdaten.dat\nBeide Verfahren sind kombinierbar: 0[12]3[AB].[xyz] ist ein g√ºltiger Ausdruck.",
    "crumbs": [
      "Tutorials",
      "Anf√§nger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#select",
    "href": "tutorials/beginner.html#select",
    "title": "Anf√§nger-Tutorial",
    "section": "Select",
    "text": "Select\nMit select k√∂nnen die Werte einzelner Unterfelder in eine CSV-Datei exportiert werden. Dabei k√∂nnen mehrere Unterfelder kombiniert werden. Man kann aus riesigen Datenbest√§nden exakt die Daten extrahieren, die man f√ºr weitere Datenanalyse ben√∂tigt.\nDer Selektionsausdruck enth√§lt eine durch Kommas getrennte Liste von Unterfeldern, die ausgelesen werden sollen, z. B.:\n$ pica select \"002@.0, 003@.0\" testdaten.dat -o test-select.csv\nDas Ergebnis ist eine CSV-Datei mit zwei Spalten, in diesem Beispiel einer Spalte f√ºr die Satzart und einer Spalte f√ºr die IDN.\nWenn Felder mehrere Unterfelder haben, k√∂nnen diese in einer Liste in geschweiften Klammer an die Feldbezeichnung angeh√§ngt werden.\n$ pica select \"002@.0, 003@.0, 021A{a,h}\" testdaten.dat -o test-select.csv\nIn die Selektionsausdr√ºcke k√∂nnen auch Filterausdr√ºcke eingebaut werden. Dazu muss die erste Position der Liste in den geschweiften Klammern mit einem Filterausdruck belegt werden.\n$ pica select \"003@.0, 028A{ (9,d,a) | 4 == 'aut' }\" testdaten.dat -o test-select.csv\nIn diesem Beispiel werden die Angaben zu den beteiligten Personen aus Feld 028A nur √ºbernommen, wenn Unterfeld 4 den Wert aut enth√§lt, die Person also Autor*in ist und nicht etwa Herausgeber*in.\nF√ºr diese Filterausdr√ºcke gelten dieselben Regeln wie f√ºr Filterausdr√ºcke im filter-Tool, die oben beschrieben wurden.\nWenn Felder wiederholbar sind (z. B. bei Schlagworten), wird pro Wiederholung eine neue Zeile in die CSV ausgegeben. Die ausgegebene CSV-Datei kann also mehr Zeilen enthalten, als Datens√§tze in den Ausgangsdaten waren. Es empfiehlt sich deshalb einen eindeutigen Identifikator mitzuselektieren, damit die wiederholten Felddaten von neuen Datens√§tzen unterschieden werden k√∂nnen.\nEs k√∂nnen auch Spaltennamen f√ºr die CSV-Ausgabe angegeben werden mit der Option -H. Wichtig: die Anzahl Spaltennamen muss der Anzahl der selektierten Unterfelder entsprechen.\n$ pica select -H \"idn, autor-idn, autor-vorname, autor-nachname\" \\\n    \"003@.0, 028A{ (9,d,a) | 4 == 'aut' }\" testdaten.dat -o test-select.csv\n\n\n\n\n\n\nHinweis\n\n\n\nDie doppelte Filterm√∂glichkeit einmal mit dem filter-Kommando und einmal im select-Kommando verwirrt auf den ersten Blick etwas. filter pr√ºft eine oder mehrere Felder oder Unterfelder auf Bedingungen und gibt den gesamten Datensatz aus, wenn die Bedingung wahr ist. select pr√ºft ebenfalls auf Bedingungen und selektiert dann die ben√∂tigten Teildaten. Man k√∂nnte auch sagen: filter arbeitet auf Datensatzebene und select auf Feldebene.",
    "crumbs": [
      "Tutorials",
      "Anf√§nger-Tutorial"
    ]
  },
  {
    "objectID": "tutorials/beginner.html#arbeit-mit-gro√üen-datenabz√ºgen",
    "href": "tutorials/beginner.html#arbeit-mit-gro√üen-datenabz√ºgen",
    "title": "Anf√§nger-Tutorial",
    "section": "Arbeit mit gro√üen Datenabz√ºgen",
    "text": "Arbeit mit gro√üen Datenabz√ºgen\npica parst immer den kompletten Datenbestand, auch wenn man nur wenige Ergebnisse erwartet. Deshalb ist es manchmal sinnvoll, die Ausgangsdatei in kleinere Dateien zu teilen, die dann viel schneller verarbeitet werden k√∂nnen.\nIn unseren Testdaten haben wir Titeldaten und Normdaten zusammen. Es k√∂nnte z.B. sinnvoll sein, die Normdaten zu extrahieren, wenn man keine Titeldaten braucht oder nur eine bestimmte Satzart zu extrahieren, wenn man nur innerhalb dieser Satzart suchen will.",
    "crumbs": [
      "Tutorials",
      "Anf√§nger-Tutorial"
    ]
  },
  {
    "objectID": "commands/completions.html",
    "href": "commands/completions.html",
    "title": "completions",
    "section": "",
    "text": "Bash\nDas completions-Kommando erzeugt Dateien, die Anweisungen enthalten, welche Argumente und Optionen des Toolkits f√ºr eine Shell zur Befehlszeilenerg√§nzung verf√ºgbar sind.\nNachfolgend werden exemplarisch die Befehle gezeigt, die f√ºr die Einbindung in die jeweilige Shell n√∂tig sind. Die Schritte sind vom System sowie der Nutzereinstellung abh√§ngig und m√ºssen ggf. angepasst werden.\nEs werden folgende Shells unterst√ºtzt:\nAlternativ kann auch immer die aktuelle Version, passend zur installierten pica-Version, eingebunden werden. Daf√ºr muss folgende Zeile in die .bashrc eingetragen werden:",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#bash",
    "href": "commands/completions.html#bash",
    "title": "completions",
    "section": "",
    "text": "$ mkdir -p ~/.local/share/bash-completion/completions\n$ pica completions bash \\\n    -o  ~/.local/share/bash-completion/completions/pica\n\n$ source &lt;(pica completions bash)\n\nBash (macOS/Homebrew)\n$ mkdir -p $(brew --prefix)/etc/bash_completion.d\n$ pica completions bash \\\n    -o $(brew --prefix)/etc/bash_completion.d/pica.bash-completion",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#elvish",
    "href": "commands/completions.html#elvish",
    "title": "completions",
    "section": "Elvish",
    "text": "Elvish\n$ mkdir -p ~/.local/share/elvish/lib/completions\n$ pica completions elvish -o ~/.local/share/elvish/lib/completions/pica.elv\n$ echo \"use completions/pica\" &gt;&gt; ~/.elvish/rc.elv",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#fish",
    "href": "commands/completions.html#fish",
    "title": "completions",
    "section": "Fish",
    "text": "Fish\n$ mkdir -p ~/.config/fish/completions\n$ pica completions fish -o ~/.config/fish/completions/pica.fish",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#powershell",
    "href": "commands/completions.html#powershell",
    "title": "completions",
    "section": "Powershell",
    "text": "Powershell\n$ pica completions powershell \\\n     &gt;&gt; ${env:USERPROFILE}\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#zsh",
    "href": "commands/completions.html#zsh",
    "title": "completions",
    "section": "ZSH",
    "text": "ZSH\nDer Pfad ~/.zfunc muss in der Variable $fpath gesetzt sein, bevor die Funktion compinit aufgerufen wird.\n$ pica completions zsh -o ~/.zfunc/_pica",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/concat.html",
    "href": "commands/concat.html",
    "title": "concat",
    "section": "",
    "text": "Optionen\nDas concat-Kommando (Alias cat) liest Datens√§tze direkt von der Standardeingabe (stdin) oder aus Dateien ein und f√ºgt diese zusammen. Die Ausgabe kann entweder in eine Datei oder in die Standardausgabe (stdout) geschrieben werden.\nDer wichtigste Anwendungsfall des Kommandos besteht in Kombination mit den Kommandos partition oder split, da mittels concat das Ergebnis dieser Kommandos (teil-)r√ºckg√§ngig gemacht werden kann. H√§ufig macht es Sinn, eine gro√üe Datei in viele kleinere Dateien anhand eines Kriteriums zu teilen (bspw. nach der Sprache), um anschlie√üend einzelne Dateien wieder zusammenzuf√ºgen.\nDas folgende Beispiel f√ºgt die Datens√§tze aus den Dateien ger.dat und eng.dat zu einer Datei ger_eng.dat zusammen:",
    "crumbs": [
      "Kommandos",
      "concat"
    ]
  },
  {
    "objectID": "commands/concat.html#optionen",
    "href": "commands/concat.html#optionen",
    "title": "concat",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-u, --unique\n\nEs werden keine Duplikate in die Ausgabe geschrieben. Die Strategie zur Erkennung von Duplikaten wird mittels der Option --unique-strategy festgelegt.\n\n--unique-strategy &lt;STRATEGY&gt;\n\nFestlegen, wie Duplikate erkannt werden sollen. Standardm√§√üig ist der Wert idn ausgew√§hlt, der Duplikate anhand der PPN aus dem Feld 003@.0 erkannt. Alternativ kann die Strategie hash gew√§hlt werden. Der Vergleich erfolgt dann √ºber die SHA-256 Pr√ºfsumme der Datens√§tze.\n\n--append\n\nWenn die Ausgabedatei bereits existiert, wird die Ausgabe an die Datei angehangen. Ist das Flag nicht gesetzt, wird eine bestehende Datei standardm√§√üig √ºberschrieben.\n\n--tee &lt;filename&gt;\n\nAbzweigen der Ausgabe in eine zus√§tzliche Datei.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datens√§tze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardm√§√üig wird die Ausgabe in die Standardausgabe stdout geschrieben. Endet der Dateiname mit dem Suffix .gz, wird die Ausgabe automatisch im Gzip-Format komprimiert.",
    "crumbs": [
      "Kommandos",
      "concat"
    ]
  },
  {
    "objectID": "commands/concat.html#beispiele",
    "href": "commands/concat.html#beispiele",
    "title": "concat",
    "section": "Beispiele",
    "text": "Beispiele\n\n√úberspringen ung√ºltiger Datens√§tze\nDer eingangs verwendete Befehl geht davon aus, dass die zwei Partitionen ausschlie√ülich g√ºltige Datens√§tze enthalten. G√ºltig in diesem Zusammenhang bedeutet, dass es sich um valide Datens√§tze im Format PICA+ handelt und nicht ob ein Datensatz einem bestimmten Regelwerk entspricht.\nDas Ausschlie√üen von ung√ºltigen Datens√§tzen wird mit der Option -s oder --skip-invalid erreicht:\n$ pica concat --skip-invalid DUMP.dat.gz -o dump_valid.dat\n$ pica concat -s DUMP.dat.gz --output dump_valid.dat.gz\nAlternativ l√§sst sich das √úberspringen ung√ºltiger Datens√§tze mittels des config-Kommandos einstellen.\n\n\nLesen von der Standardeingabe\nDas Kommando kann auch direkt von der Standardeingabe (stdin) lesen. Das ist bspw. dann hilfreich, wenn die Ausgabe aus einem vorhergehenden Pipeline-Schritt mit dem Inhalt einer oder mehrerer Dateien konkateniert werden soll.\nDas folgende Beispiel liest im ersten Pipeline-Schritt die Datei dump1.dat ein, entfernt ung√ºltige Datens√§tze und gibt die Ausgabe nach stdout aus. Der zweite Pipeline-Schritt liest diese Datens√§tze ein (-) und konkateniert diese mit den Datens√§tzen aus der Datei dump2.dat. Das Ergebnis wird in die Datei out.dat geschrieben.\n$ pica concat -s dump1.dat | pica cat - dump2.dat -o out.dat\nDer Dateiname - steht f√ºr die Standardeingabe (stdin). W√§ren die zwei Argumente vertauscht (pica cat dump2.dat -), dann w√ºrden erst die g√ºltigen Datens√§tze aus der Datei dump1.dat und anschlie√üend die Datens√§tze aus dem ersten Pipeline-Schritt geschrieben.\n\n\nHinzuf√ºgen von Datens√§tzen\nWenn eine Ausgabedatei bereits existiert, wird diese standardm√§√üig neu angelegt und √ºberschrieben. Soll das Verhalten dahingehend ge√§ndert werden, dass an die bestehenden Dateien angehangen wird, kann dies mit der --append-Option erreicht werden. Diese Option √§ndert das Verhalten von --output und --tee. Die Option hat auf das Verhalten beim Schreiben in die Standardausgabe keine Auswirkung.\nIm folgenden Beispiel erzeugt der erste Befehl eine neue Datei gnd.dat. Sollte die Datei bereits existieren, wird der Datei-Inhalt √ºberschrieben. Die darauffolgenden Kommandos h√§ngen jeweils an das Ende der Datei gnd.dat an.\n$ pica concat Tp*.dat -o gnd.dat\n$ pica concat --append Ts*.dat -o gnd.dat\n$ pica concat --append Tu*.dat -o gnd.dat\n...\n\n\nAbzweigen der Ausgabe\nGelegenlich kann es n√ºtzlich sein, die Ausgabe des concat-Kommandos in eine Datei zu schreiben und gleichzeitig die Ausgabe an einen weiteren Pipeline-Schritt weiterzureichen. Dies hat den Vorteil, dass zwei CPU-Kerne gleichzeitig genutzt werden k√∂nnen. Mit der --tee-Option l√§sst sich dieses Verhalten erzielen. Der Name der Option leitet sich von dem T-St√ºck (engl. tee connector) ab, mit dem ein Klempner eine Abzeigung in eine Leitung einbaut.\nIm folgenden Beispiel werden alle Tp*.dat zusammengef√ºgt und in eine Datei Tp.dat geschrieben. Gleichzeitig werden alle Datens√§tze mit dem filter-Kommando nach der Satzart Tp2 im Feld 002@.0 gefiltert.\n$ pica concat partitions/Tp*.dat --tee gnd_person.dat | \\\n    pica filter \"002@.0 =^ 'Tp2'\" -o Tp2.dat\n\n\nEntfernen von Duplikaten\nDuplikate k√∂nnen durch die Angabe des Flags --unique (-u) entfernt werden. Standardm√§√üig erfolgt die Erkennung von Duplikaten per PPN aus dem Feld 003@.0 . Alternativ kann durch die Angabe der Option --unique-strategy die Variante hash ausgew√§hlt werden, bei der nur solche Datens√§tze als gleich gewertet werden, bei denen alle Bytes gleich sind.\n$ pica concat --unique --unique-strategy hash ger.dat eng.dat -o out.dat\n$ pica concat --unique --unique-strategy idn ger.dat eng.dat -o out.dat",
    "crumbs": [
      "Kommandos",
      "concat"
    ]
  },
  {
    "objectID": "commands/config.html",
    "href": "commands/config.html",
    "title": "config",
    "section": "",
    "text": "Optionen\nMithilfe des config-Kommandos lassen sich bestimmte Optionen setzen und das Laufzeitverhalten von pica beeinflussen. Falls noch keine Konfigurationsdatei existiert, wird diese automatisch angelegt und je nach Betriebssystem in den daf√ºr vorgesehenen Pfaden gespeichert:",
    "crumbs": [
      "Kommandos",
      "config"
    ]
  },
  {
    "objectID": "commands/config.html#optionen",
    "href": "commands/config.html#optionen",
    "title": "config",
    "section": "",
    "text": "√úberspringen ung√ºltiger Datens√§tze\nKann eine Zeile in der Eingabe nicht als Datensatz (normalisiertes PICA+) dekodiert werden, brechen die meisten Kommandos die Verarbeitung mit einer Fehlermeldung ab. Dieses Verhalten kann mit der Option --skip-invalid ge√§ndert werden, sodass diese ung√ºltigen Datens√§tze √ºbersprungen werden. Dieses Verhalten kann auch in der Konfigurationsdatei hinterlegt werden:\n$ pica config skip-invalid true\nNachdem die Variable gesetzt wurde, kann die Angabe der --skip-invalid-Option entfallen. Die Einstellung l√§sst sich mit --unset r√ºckg√§ngig machen:\n$ pica config --unset skip-invalid\n\n\n√Ñndern der Unicode Normalform\nLiegen die PICA-Daten in einer anderen Unicode Normalform vor, lassen sich Filterausdr√ºcke mit der Option normalization an die Normalform der Daten angleichen:\n$ pica config normalization nfd\nEs werden die vier Normalformen nfd, nfc, nfkc und nfkd unterst√ºtzt. Nur wenn eine Normalform ausgew√§hlt ist, werden Filterausdr√ºcke immer entsprechend transliteriert. Die Einstellung l√§sst sich mit --unset r√ºckg√§ngig machen:\n$ pica config --unset normalization",
    "crumbs": [
      "Kommandos",
      "config"
    ]
  },
  {
    "objectID": "commands/convert.html",
    "href": "commands/convert.html",
    "title": "convert",
    "section": "",
    "text": "Optionen\nDas PICA-Format kann in verschiedene Datenformate serialisiert werden. Das convert-Kommando erm√∂glicht es, Datens√§tze von einem Format in ein anderes Format zu konvertieren. Es bietet insbesondere die M√∂glichkeit, Datens√§tze, die nicht in normalisiertem PICA+ vorliegen, nach PICA+ zu konvertieren, um sie durch andere Kommandos verarbeiten zu k√∂nnen.\nFolgende Formate werden unterst√ºtzt:\nDie Angabe der Datenformate erfolgt √ºber die Optionen --from/-f und --to/-t:",
    "crumbs": [
      "Kommandos",
      "convert"
    ]
  },
  {
    "objectID": "commands/convert.html#optionen",
    "href": "commands/convert.html#optionen",
    "title": "convert",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-f &lt;format&gt;, --from &lt;format&gt;\n\nAuswahl des Datenformats der Eingabe.\n\n-t &lt;format&gt;, --to &lt;format&gt;\n\nAuswahl des Datenformats der Ausgabe.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datens√§tze in eine Datei mittels -o bzw. --output.\n\n-o &lt;filename&gt;, --output &lt;filename&gt;\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardm√§√üig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "convert"
    ]
  },
  {
    "objectID": "commands/count.html",
    "href": "commands/count.html",
    "title": "count",
    "section": "",
    "text": "Optionen\nSoll die Anzahl der Datens√§tze und deren Felder sowie Unterfelder ermittelt werden, kann dies mit dem count-Kommando erfolgen. Ung√ºltige Datens√§tze k√∂nnen mit dem Flag --skip-invalid (bzw. -s) √ºbersprungen werden. Im folgenden Beispiel wird die Datei DUMP.dat.gz eingelesen und die Anzahl der in der Datei enthaltenen Datens√§tzes (records), Felder (_fields) und Unterfelder (subfields) ausgegeben:",
    "crumbs": [
      "Kommandos",
      "count"
    ]
  },
  {
    "objectID": "commands/count.html#optionen",
    "href": "commands/count.html#optionen",
    "title": "count",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGro√ü- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim √Ñhnlichkeitsvergleich von Zeichenketten mittels =*.\n\n--append\n\nWenn die Ausgabedatei bereits existiert, wird die Ausgabe an die Datei angehangen. Ist das Flag nicht gesetzt, wird eine bestehende Datei standardm√§√üig √ºberschrieben.\n\n--records\n\nGibt nur die Anzahl der vorhandenen Datens√§tze aus. Dieses Flag ist nicht mit den Optionen --fields, --subfields, --csv, --tsv und --no-header kombinierbar.\n\n--fields\n\nGibt nur die Anzahl der vorhandenen Felder aus. Dieses Flag ist nicht mit den Optionen --records, --subfields, --csv, --tsv und --no-header kombinierbar.\n\n--subfields\n\nGibt nur die Anzahl der vorhandenen Unterfelder aus. Dieses Flag ist nicht mit den Optionen --records, --fields, --csv, --tsv und --no-header kombinierbar.\n\n--csv\n\nDie Ausgabe erfolgt im CSV-Format.\n\n--tsv\n\nDie Ausgabe erfolgt im TSV-Format.\n\n--no-header\n\nEs wird keine Kopfzeile in die Ausgabe geschrieben.\n\n--where &lt;expr&gt;\n\nAngabe eines Filters, um Datens√§tze aus der Eingabe auszuw√§hlen.\n\n--and &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters mittels der booleschen &&-Verkn√ºpfung. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters mittels der booleschen ||-Verkn√ºpfung. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datens√§tze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardm√§√üig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "count"
    ]
  },
  {
    "objectID": "commands/count.html#beispiele",
    "href": "commands/count.html#beispiele",
    "title": "count",
    "section": "Beispiele",
    "text": "Beispiele\n\nAusgabe im CSV/TSV-Format\nDie Ausgabe des Kommandos kann auch im Format CSV bzw. TSV erfolgen, was die Weiterverarbeitung in anderen Programmen erleichtert. Die Ausgabe der Kopfzeile l√§sst sich mit dem Flag --no-header ausschalten.\n$ pica count -s --csv DUMP.dat.gz\nrecords,fields,subfields\n12,1035,3973\n\n$ pica count -s --tsv DUMP.dat.gz\nrecords fields  subfields\n12  1035    3973\n\n$ pica count -s --csv --no-header DUMP.dat.gz\n12,1035,3973\n\n\nAusgabe in eine Datei\nDie Ausgabe des Kommandos wird standardm√§√üig auf der Konsole ausgegeben. Diese kann mit der Option --output (bzw. -o) in eine Datei umgeleitet werden. Soll diese Datei eine neue Zeile erhalten und nicht bei jedem Aufruf √ºberschrieben werden, kann dies mit dem Flag --append erzielt werden.\n$ pica count -s --csv -o count.csv DUMP.dat.gz\n$ cat count.csv\nrecords,fields,subfields\n12,1035,3973\n\n$ pica count -s --csv --append -o count.csv DUMP.dat.gz\n$ cat count.csv\nrecords,fields,subfields\n12,1035,3973\n12,1035,3973\n\n\nAusgabe von Einzelwerten\nSoll entweder die Anzahl von Datens√§tzen, Feldern oder Unterfeldern ausgegeben werden, kann dies mit den Flags --records, --fields bzw. --subfields erfolgen. Diese Flags schlie√üen sich gegenseitig aus und k√∂nnen nicht mit den Flags --csv, --tsv und --no-header kombiniert werden.\n$ pica count -s --records DUMP.dat.gz\n12\n\n$ pica count -s --fields DUMP.dat.gz\n1035\n\n$ pica count -s --subfields DUMP.dat.gz\n3973\n\n\nAnwendungsbeispiel\nSoll die Ver√§nderung (Anzahl Datens√§tze, Felder, Unterfelder) eines PICA-Abzugs √ºber die Zeit verfolgt werden, k√∂nnte dies wie folgt erreicht werden:\n$ echo \"date,records,fields,subfields\" &gt; count.csv # Kopfzeile\n$ pica count -s dump_20220222.dat.gz --append -o count.csv # Initialer Aufruf\n$ pica count -s dump_20220223.dat.gz --append -o count.csv # Aufruf nach x Tagen\n\n$ cat count.csv\n$ records,fields,subfields\n7,247,549\n9,347,1022\nSoll auch das aktuelle Datum vor die Zeile geschrieben werden, k√∂nnten bspw. folgende Unix-Kommandos genutzt werden:\n# Schreiben der Kopfzeile\n$ echo \"date,records,fields,subfields\" &gt; count.csv\n\n# Aufruf am 22.02.2022\n$ pica count -s --no-header --csv dump_20220222.dat.gz | \\\n    xargs -d\"\\n\" -I {} date +\"%Y-%m-%d,{}\" &gt;&gt; count.csv\n\n# Aufruf am 23.02.2022\n$ pica count -s --no-header --csv dump_20220223.dat.gz | \\\n    xargs -d\"\\n\" -I {} date +\"%Y-%m-%d,{}\" &gt;&gt; count.csv\n\n$ cat count.csv\n$ date,records,fields,subfields\n2022-02-22,7,247,549\n2022-02-23,9,347,1022",
    "crumbs": [
      "Kommandos",
      "count"
    ]
  },
  {
    "objectID": "commands/explode.html",
    "href": "commands/explode.html",
    "title": "explode",
    "section": "",
    "text": "Optionen\nDie Verarbeitung und Analyse von Datens√§tzen auf Lokal- bzw. Exemplarebene ist mitunter nur unzureichend m√∂glich, da Filterausdr√ºcke die Grenzen von untergeordneten Ebenen nicht respektiert. Abhilfe kann das explode-Kommando schaffen, das einen Datensatz in einzelne Lokal- bzw. Exemplardatens√§tze aufteilen kann. Dabei werden alle Felder der √ºbergeordneten Ebenen mit in die Datens√§tze √ºbernommen.\nDas Aufteilen der Datens√§tze erfolgt durch die Angabe der Ebene (level) an der der Datensatz geteilt werden soll. Es k√∂nnen folgende Werte ausgew√§hlt werden:\nSoll ein Datensatz in alle Lokaldatens√§tze aufgeteilt werden, muss die Ebene local ausgew√§hlt werden. Die neu erstellten Datens√§tze enthalten alle Titeldaten (Felder der Ebene main), den Identifikator des Lokaldatensatzes (Feld 101@.a) sowie alle Exemplare, die diesem Lokaldatensatz zugeordnet werden.\nWird dar√ºber hinaus f√ºr jedes Exemplar ein eigenst√§ndiger Datensatz ben√∂tigt, muss die Ebene copy angegeben werden. Jeder erzeugte Datensatz enth√§lt die Titeldaten (Felder der Ebene main), den Identifikator des Lokaldatensatzes (Feld 101@.a) und nur die Felder, die zum jeweiligen Exemplar geh√∂ren.\nSchlie√ülich kann ein Datensatz auch auf Ebene der Titeldaten (main) aufgeteilt werden. Diese Auswahl ver√§ndert nichts am Datensatz und gibt den vollst√§ndigen Datensatz mit allen Feldern aus.\nAls Beispiel soll folgender (reduzierter) Datensatz dienen:\nDieser Datensatz l√§sst sich in zwei Datens√§tze auf Ebene der Lokaldaten aufteilen:\nSoll jedes Exemplar ein eigenst√§ndiger Datensatz werden, wird dies durch Angabe von copy erzielt:\n-k &lt;expr&gt;, --keep &lt;expr&gt; Es werden nur die Felder eines Datensatzes beibehalten, die in der Liste aufgef√ºhrt werden.",
    "crumbs": [
      "Kommandos",
      "explode"
    ]
  },
  {
    "objectID": "commands/explode.html#optionen",
    "href": "commands/explode.html#optionen",
    "title": "explode",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGro√ü- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim √Ñhnlichkeitsvergleich von Zeichenketten mittels =*.\n\n\n\n\n-d, --discard\n\nEs werden die Felder eines Datensatzes verworfen, die in der Liste aufgef√ºhrt werden.\n\n-l &lt;n&gt;, --limit &lt;n&gt;\n\nEingrenzung der Ausgabe auf die ersten &lt;n&gt; (g√ºltigen) Datens√§tze.\n\n--where &lt;filter&gt;\n\nAngabe eines Filters, der auf die erzeugten Datens√§tze angewandt wird.\n\n--and &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters mittels der booleschen &&-Verkn√ºpfung. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters mittels der booleschen ||-Verkn√ºpfung. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datens√§tze in eine Datei mittels -o bzw. --output.\n\n-o &lt;filename&gt;, --output &lt;filename&gt;\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardm√§√üig wird die Ausgabe in die Standardausgabe stdout geschrieben. Endet der Dateiname mit dem Suffix .gz, wird die Ausgabe automatisch im Gzip-Format komprimiert.",
    "crumbs": [
      "Kommandos",
      "explode"
    ]
  },
  {
    "objectID": "commands/explode.html#beispiele",
    "href": "commands/explode.html#beispiele",
    "title": "explode",
    "section": "Beispiele",
    "text": "Beispiele\n\nEingrenzen der Datens√§tze\nIst nur eine Teilmenge der erzeugten Datens√§tze von Interesse, l√§sst sich die Ergebnismenge durch Hinzuf√ºgen eines Filterausdrucks eingrenzen.\nWerden bspw. nur die Exemplare mit dem Identifikator 101@.a == \"1\" ben√∂tigt, kann die Eingrenzung durch Angabe der --where-Option eingegrenzt werden:\n$ pica explode -s copy --where '101@.a == \"1\"' COPY.dat.gz -o copy.dat\n$ pica print copy.dat\n003@ $0 123456789\n002@ $0 Abvz\n101@ $a 1\n203@/01 $0 0123456789\n\n003@ $0 123456789\n002@ $0 Abvz\n101@ $a 1\n203@/02 $0 1234567890",
    "crumbs": [
      "Kommandos",
      "explode"
    ]
  },
  {
    "objectID": "commands/filter.html",
    "href": "commands/filter.html",
    "title": "filter",
    "section": "",
    "text": "Optionen\nDas filter-Kommando bildet das Herzst√ºck des pica-Tools. Es erm√∂glicht es, aus einer (mitunter sehr gro√üen) Datenmenge, bspw. dem Gesamtabzug des Katalogsystems, eine kleinere Menge effizient zu extrahieren, um sie anschlie√üend weiterzuverarbeiten. Dies erfolgt √ºber die Angabe eines Filterausdrucks, der dar√ºber entscheidet, ob ein Datensatz in die Zielmenge eingeht oder nicht.\nIm folgenden Beispiel werden alle Datens√§tze aus der Datei DUMP.dat.gz extrahiert, die ein Feld 003@ enthalten, das ein Unterfeld 0 besitzt, welches mit dem Wert 118540238 belegt ist.",
    "crumbs": [
      "Kommandos",
      "filter"
    ]
  },
  {
    "objectID": "commands/filter.html#optionen",
    "href": "commands/filter.html#optionen",
    "title": "filter",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-v, --invert-match\n\nGibt alle Datens√§tze aus, die nicht dem Filterausdruck entsprechen.\n\n-i, --ignore-case\n\nGro√ü- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim √Ñhnlichkeitsvergleich von Zeichenketten mittels =*.\n\n-k, --keep\n\nEs werden nur die Felder eines Datensatzes beibehalten, die in der Liste aufgef√ºhrt werden.\n\n-d, --discard\n\nEs werden die Felder eines Datensatzes verworfen, die in der Liste aufgef√ºhrt werden.\n\n-F &lt;filename&gt;, --file &lt;filename&gt;\n\nEs wird der Filterausdruck aus der Datei &lt;filename&gt; eingelesen. Es darf keine weitere Angabe eines Filterausdrucks als Kommandozeilenargument erfolgen!\n\n-A &lt;filename&gt;, --allow-list &lt;filename&gt;\n\nEs werden alle Datens√§tze ignoriert, die nicht explizit in der Positivliste1 auftauchen. Werden mehrere Positivlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-D &lt;filename&gt;, --deny-list &lt;filename&gt;\n\nEs werden alle Datens√§tze ignoriert, die in der Negativliste auftauchen. Werden mehrere Negativlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-l &lt;n&gt;, --limit &lt;n&gt;\n\nEingrenzung der Ausgabe auf die ersten &lt;n&gt; (g√ºltigen) Datens√§tze.\n\n--and &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters mittels der booleschen &&-Verkn√ºpfung. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters mittels der booleschen ||-Verkn√ºpfung. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n--append\n\nWenn die Ausgabedatei bereits existiert, wird die Ausgabe an die Datei angehangen. Ist das Flag nicht gesetzt, wird eine bestehende Datei √ºberschrieben.\n\n--tee &lt;filename&gt;\n\nAbzweigen der Ausgabe in eine zus√§tzliche Datei.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datens√§tze in eine Datei mittels -o bzw. --output.\n\n-o &lt;filename&gt;, --output &lt;filename&gt;\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardm√§√üig wird die Ausgabe in die Standardausgabe stdout geschrieben. Endet der Dateiname mit dem Suffix .gz, wird die Ausgabe automatisch im gzip-Format komprimiert.",
    "crumbs": [
      "Kommandos",
      "filter"
    ]
  },
  {
    "objectID": "commands/filter.html#beispiele",
    "href": "commands/filter.html#beispiele",
    "title": "filter",
    "section": "Beispiele",
    "text": "Beispiele\n\nInvertierte Treffermenge (invert match)\nMitunter ist es einfacher, einen Ausdruck zu formulieren, der alle Datens√§tze umfasst, die nicht in der Treffermenge gew√ºnscht sind. Durch die Option -v/--invert-match werden dann nur die Datens√§tze ausgegeben, die nicht dem Filterkriterum entsprechen.\nBeispielweise enth√§lt der Abzug DUMP.dat.gz verschiedene Normdatens√§tze. Werden nur die Datens√§tze ben√∂tigt, die nicht vom Satztyp Werk sind, ist es einfacher, zuerst nach den Werken zu suchen und dann durch das Flag -v alle Datens√§tze zu erhalten, die nicht dem Filterkriterium entsprechen.\n$ pica filter -s -v '002@.0 =^ \"Tu\"' DUMP.dat.gz -o not-Tu.dat.gz\n$ pica frequency '002@.0' not-Tu.dat.gz\nTsz,2\nTg1,1\nTp1,1\nTpz,1\nTs1,1\n\n\nGro√ü- und Kleinschreibung ingorieren\nStandardm√§√üig wird bei Vergleichen von Zeichenketten die Gro√ü- und Kleinschreibung beachtet. Dies l√§sst sich mit dem Flag -i/--ignore-case deaktivieren:\n$ pica filter -s -i '028A.a == \"GOETHE\"' DUMP.dat.gz -o goethe.dat\n$ pica print goethe.dat\n...\n028A $d Johann Wolfgang $c von $a Goethe\n...\n\n\nFelder eines Datensatzes reduzieren\nTeilweise ist die Anzahl der Felder pro Datensatz sehr gro√ü, was zu erheblichen Laufzeiteinbu√üen nachfolgender Datenanalysen f√ºhren kann. Mittels der Optionen -k/--keep bzw. -d/--discard lassen sich Datens√§tze auf eine Teilmenge der Felder reduzieren.\nWerden f√ºr eine Datenanalyse nur die IDN/PPN (003@), die Satzart (002@) und die Entit√§tenkodierung (004B) ben√∂tigt, k√∂nnen die Datens√§tze wie folgt auf die Felder reduziert werden:\n$ pica filter -s --keep '00[23]@,004B' \"003@?\" DUMP.dat.gz -o out.dat\n$ pica print out.dat\n002@ $0 Tpz\n003@ $0 118540238\n004B $a piz\n\n002@ $0 Tp1\n003@ $0 118607626\n004B $a piz\n...\nSollen bestimmte Felder entfernt werden, kann dies mit der Option -d/--discard erfolgen. Der folgende Aufruf entfernt das Feld 004B, sofern vorhanden, aus allen Datens√§tzen:\n$ pica filter -s --keep '00[23]@,004B' \"003@?\" DUMP.dat.gz -o out.dat\n$ pica filter --discard '004B' \"003@?\" out.dat -o out2.dat\n$ pica print out2.dat\n002@ $0 Tpz\n003@ $0 118540238\n\n002@ $0 Tp1\n003@ $0 118607626\n...",
    "crumbs": [
      "Kommandos",
      "filter"
    ]
  },
  {
    "objectID": "commands/filter.html#footnotes",
    "href": "commands/filter.html#footnotes",
    "title": "filter",
    "section": "",
    "text": "Eine Positiv- oder Negativliste muss entweder als CSV-Datei vorliegen oder als eine Arrow-Datei, die eine ppn- oder idn-Spalte enth√§lt. Alle Dateien werden automatisch als CSV-Datei interpretiert, es sei denn, die Datei endet mit .ipc oder .arrow, dann erfolgt die Interpretation im Arrow-Format. CSV- bzw. TSV-Dateien mit der Ending .csv.gz bzw. .tsv.gz werden automatisch entpackt.‚Ü©Ô∏é",
    "crumbs": [
      "Kommandos",
      "filter"
    ]
  },
  {
    "objectID": "commands/frequency.html",
    "href": "commands/frequency.html",
    "title": "frequency",
    "section": "",
    "text": "Optionen\nDas Kommando frequency wird dazu genutzt, um die H√§ufigkeiten der Werte ein oder mehrerer Unterfelder zu bestimmen. Ist das zu untersuchende Feld bzw. Unterfeld wiederholbar, gehen alle Wertauspr√§gungen eines Datensatzes in die H√§ufigkeitsverteilung ein. Die Ausgabe erfolgt standardm√§√üig im CSV-Format. Im folgenden Beispiel wird die H√§ufigkeitsverteilung des Unterfelds 002@.0 (Satzart) ermittelt:\n-H &lt;header&gt;, --header &lt;header&gt; Kopfzeile, die den Ergebnissen vorangestellt wird; Spaltennamen werde mit einem Komma separiert.\n-t, --tsv Ausgabe erfolgt im TSV-Format.",
    "crumbs": [
      "Kommandos",
      "frequency"
    ]
  },
  {
    "objectID": "commands/frequency.html#optionen",
    "href": "commands/frequency.html#optionen",
    "title": "frequency",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGro√ü- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim √Ñhnlichkeitsvergleich von Zeichenketten mittels =*.\n\n--unique, -u\n\nDoppelte Werte eines Datensatzes werden ignorieren.\n\n--reverse\n\nErgebnisse werden in aufsteigender Reihenfolge ausgegeben.\n\n-A &lt;file&gt;, --allow-list &lt;file&gt;\n\nEs werden alle Datens√§tze ignoriert, die nicht explizit in der Positivliste1 auftauchen. Werden mehrere Positivlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-D &lt;file&gt;, --deny-list &lt;file&gt;\n\nEs werden alle Datens√§tze ignoriert, die in der Negativliste auftauchen. Werden mehrere Negativlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-l &lt;n&gt;, --limit &lt;n&gt;\n\nEingrenzung der Ausgabe auf die h√§ufigsten &lt;n&gt; Unterfeldwerte.\n\n-t &lt;n&gt;, --threshold &lt;n&gt;\n\nZeilen mit einer H√§ufigkeit \\(&lt;\\) &lt;n&gt; ignorieren.\n\n--where &lt;filter&gt;\n\nAngabe eines Filters, der auf die erzeugten Datens√§tze angewandt wird.\n\n--and &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters mittels der booleschen &&-Verkn√ºpfung. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters mittels der booleschen ||-Verkn√ºpfung. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n\n\n\n\n--translit &lt;normalization&gt;\n\nAusgabe wird in die angegebene Normalform transliteriert. M√∂gliche Werte: nfd, nfkd, nfc und nfkc.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datens√§tze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardm√§√üig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "frequency"
    ]
  },
  {
    "objectID": "commands/frequency.html#beispiele",
    "href": "commands/frequency.html#beispiele",
    "title": "frequency",
    "section": "Beispiele",
    "text": "Beispiele\n\nHinzuf√ºgen einer Kopfzeile\nF√ºr die Dokumentation sowie die Verwendung in anderen Programmiersprachen ist es h√§ufig sinnvoll, eine Kopfzeile hinzuzuf√ºgen. Dies erfolgt mit der Option --header bzw. -H. Die Namen der Spalten werden kommasepariert angegeben. Eine Angabe von mehr als zwei Spalten ist nicht erlaubt.\n$ pica frequency -s --header \"satzart,anzahl\" \"002@.0\" DUMP.dat.gz\nsatzart,anzahl\nTu1,6\nTsz,2\nTg1,1\nTp1,1\nTpz,1\nTs1,1\n\n\nAuswertung mehrerer Felder bzw. Unterfelder\nDurch die Angabe von mehreren Pfadausdr√ºcken l√§sst sich eine H√§ufigkeitsverteilung √ºber mehrere Untefelder ermitteln. Das folgende Beispiel berechnet die H√§ufigkeit der Kombination aus Satzart (002@.0) und dem Entit√§tencode(s) (004B.a):\n$ pica frequency -s -H \"bbg,ent,count\" \"002@.0, 004B.a\" DUMP.dat.gz\nbbg,ent,count\nTu1,wit,6\nTsz,saz,2\nTg1,gik,1\nTp1,piz,1\nTpz,piz,1\nTs1,saz,1\nEbenfalls k√∂nnen auch mehrere Unterfelder ausgewertet werden. Eine Auswertung der H√§ufigkeiten von verkn√ºpften Sachbegriffen (Level 1) und dem GND-Code f√ºr Beziehungen im Feld 041R (Sachbegriff - Beziehung) wird wie folgt ermittelt:\n$ pica frequency -s '041R{(7,4) | 7 == \"Ts1\"}' DUMP.dat.gz\nTs1,beru,12\nTs1,obal,5\nTs1,vbal,3\nTs1,obge,1\nTs1,stud,1\n\n\nEingrenzung auf bestimmte Felder\nOftmals sollen nicht alle Felder in die Berechnung der H√§ufigkeiten miteinbezogen werden. Dies ist bspw. dann der Fall, wenn sich Felder anhand eines Unterfelds unterschieden lassen, wie etwa durch die Angabe der Metadatenherkunft. Durch Verwenden eines Pfad-Ausdrucks in {}-Notation k√∂nnen nur die Felder ausgew√§hlt werden, die einem bestimmten Kriterium entsprechen.\nDas folgende Beispiel bezieht nur die Felder 041R in die Auswertung mit ein, bei denen ein Unterfeld 4 existiert, das entweder berc oder beru ist; Felder die diesem Kriterium nicht entsprechen, werden ignoriert.\n$ pica frequency -s \"041R{ 9 | 4 in ['berc', 'beru'] }\" DUMP.dat.gz\n040533093,2\n040250989,1\n040252434,1\n040290506,1\n...\n\n\nEingrenzen der Treffermenge\nSoll die Ergebnismenge auf die h√§ufigsten n Unterfeldwerte eingeschr√§nkt werden, wird dies mit der Option --limit bzw. -l erreicht. Das nachfolgende Beispeil ermittelt die 3 h√§ufigsten Werte im Feld 041R.4.\n$ pica frequency -s --limit 3 \"041R.4\" DUMP.dat.gz\nberu,12\nobal,5\nvbal,4\n\n\nEingrenzen der Treffermenge (Schwellenwert)\nDie Treffermenge kann auch mittels der Angabe eines Schwellenwerts eingesch√§nkt werden. Sollen nur die Werte angezeigt werden, die ab einem Schwellenwert vorkommen, dann kann dies mit der Option --threshold/-t erzielt werden:\n$ pica frequency -s --threshold 4 \"041R.4\" DUMP.dat.gz\nberu,12\nobal,5\nvbal,4\n\n\n√Ñnderung der Sortierreihenfolge (Limit)\nStandardm√§√üig wird die H√§ufigkeitsverteilung absteigend ausgegeben, d.h., der h√§ufigste Wert steht in der Ausgabe oben. Soll das Verhalten so ge√§ndert werden, dass die Ausgabe aufsteigend sortiert wird, kann dies mit der Option --reverse bzw. -r erfolgen. Das folgende Kommando sucht nach den vier Satzarten, die am wenigsten vorkommen:\n$ pica frequency -s -l 4 --reverse \"002@.0\" DUMP.dat.gz\nTg1,1\nTp1,1\nTpz,1\nTs1,1\n\n\nAusgabe im TSV-Format\nDie Ausgabe l√§sst sich mittels der Option --tsv (bzw. -t) in das TSV-Format √§ndern.\n$ pica frequency -s -l3 --tsv tests/data/dump.dat.gz\nTu1 6\nTsz 2\n...",
    "crumbs": [
      "Kommandos",
      "frequency"
    ]
  },
  {
    "objectID": "commands/frequency.html#footnotes",
    "href": "commands/frequency.html#footnotes",
    "title": "frequency",
    "section": "",
    "text": "Alle Werte mit gleicher H√§ufigkeit werden immer in lexikographisch aufsteigender Reihenfolge sortiert. Dies erfolgt unabh√§ngig vom Parameter --reverse.‚Ü©Ô∏é",
    "crumbs": [
      "Kommandos",
      "frequency"
    ]
  },
  {
    "objectID": "commands/hash.html",
    "href": "commands/hash.html",
    "title": "hash",
    "section": "",
    "text": "Optionen\nMithilfe des Kommandos hash l√§sst sich eine Tabelle erzeugen, die in der ersten Spalte die PPN (Feld 003@.0) und in der zweiten Spalte den SHA-256-Hashwert des Datensatzes enth√§lt.\nMitunter kommt es vor, dass eine regelm√§√üige und aufw√§ndige Berechnung f√ºr Datens√§tze ausgef√ºhrt werden muss und es nicht praktikabel ist, die Berechnung √ºber alle Datens√§tze durchzuf√ºhren. Mittels des hash-Kommandos k√∂nnen auf unterschiedlichen Abz√ºgen die Hashwerte der Datens√§tze bestimmt werden. Durch Vergleich dieser Tabelle ist es m√∂glich, die Datens√§tze zu identifizieren, die sich ge√§ndert haben, gel√∂scht oder neu hinzugef√ºgt wurden. Das folgende Beispiel demonstriert die Erzeugung dieser Hash-Tabellen:",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/hash.html#optionen",
    "href": "commands/hash.html#optionen",
    "title": "hash",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-H &lt;header&gt;, --header &lt;header&gt;\n\nKopfzeile, die den Ergebnissen vorangestellt wird.\n\n-t, --tsv\n\nAusgabe erfolgt im TSV-Format.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datens√§tze in eine Datei mittels -o bzw. --output.\n\n-o &lt;filename&gt;, --output &lt;filename&gt;\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll.",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/hash.html#beispiele",
    "href": "commands/hash.html#beispiele",
    "title": "hash",
    "section": "Beispiele",
    "text": "Beispiele\n\nAusgabe im TSV-Format\nDie Ausgabe l√§sst sich mittels der Option --tsv (bzw. -t) in das TSV-Format √§ndern.\n$ pica hash -s --tsv DUMP.dat.gz\nppn hash\n118540238   762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3\n118607626   8d75e2cdfec20aa025d36018a40b150363d79571788cd92e7ff568595ba4f9ee\n040993396   0361c33e1f7a80e21eecde747b2721b7884e003ac4deb8c271684ec0dc4d059a\n...\n\n\nHinzuf√ºgen einer Kopfzeile\nEine individuelle Kopfzeile l√§sst sich mit der Option --header (bzw. -H) ausgeben:\n$ pica hash -s --header 'idn,sha256' DUMP.dat.gz\nidn,sha256\n118540238,762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3\n118607626,8d75e2cdfec20aa025d36018a40b150363d79571788cd92e7ff568595ba4f9ee\n040993396,0361c33e1f7a80e21eecde747b2721b7884e003ac4deb8c271684ec0dc4d059a\n...",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/hash.html#anmerkung",
    "href": "commands/hash.html#anmerkung",
    "title": "hash",
    "section": "Anmerkung",
    "text": "Anmerkung\nZur Berechnung des SHA-256-Hashwerts wird der abschlie√üende Zeilenumbruch mit einbezogen, um einen gleichen Hashwert zu erzeugen, der entsteht, wenn der Hashwert √ºber die gesamte Zeile aus der Eingabe ermittelt wird. Im folgenden Beispiel wird zuerst der erste g√ºltige Datensatz in die Datei 1.dat geschrieben. Anschlie√üend wird der Hashwert einmal mit dem hash-Kommando und einmal mit dem Unix-Programm sha256sum gebildet. Beide Ergebnisse sind gleich.\n$ pica filter -s -l1 \"003@?\" DUMP.dat.gz -o 1.dat\n$ pica hash 1.dat\nppn,hash\n118540238,762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3\n\n$ sha256sum 1.dat\n762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/invalid.html",
    "href": "commands/invalid.html",
    "title": "invalid",
    "section": "",
    "text": "Beispiel\nBei der Verarbeitung von PICA-Daten kann es vorkommen, dass Zeilen in der Eingabe nicht als normalisiertes PICA+ dekodiert werden k√∂nnen. Diese ung√ºltigen Zeilen lassen sich bei vielen Kommandos mit der Option --skip-invalid / -s √ºberspringen, wobei die Anzahl der √ºbersprungenen Zeilen nicht angezeigt wird. Es ist zu empfehlen, die Anzahl invalider Datens√§tze zu kontrollieren und einer Pr√ºfung zu unterziehen, um diese ggf. zu korrigieren. Das invalid-Kommando findet diese Zeilen in der Eingabe und gibt diese wieder auf der Standardausgabe (stdout) aus. Durch Angabe der Option --output / -o kann die Ausgabe in eine Datei geschrieben werden.\nDer folgende Befehl findet alle ung√ºltigen Datens√§tze aus der Datei DUMP.dat.gz und schreibt diese Zeile in die Datei invalid.dat:",
    "crumbs": [
      "Kommandos",
      "invalid"
    ]
  },
  {
    "objectID": "commands/invalid.html#beispiel",
    "href": "commands/invalid.html#beispiel",
    "title": "invalid",
    "section": "",
    "text": "$ pica invalid DUMP.dat.gz -o invalid.dat",
    "crumbs": [
      "Kommandos",
      "invalid"
    ]
  },
  {
    "objectID": "commands/partition.html",
    "href": "commands/partition.html",
    "title": "partition",
    "section": "",
    "text": "Optionen\nMittels des partition-Kommandos lassen sich Datens√§tze anhand eines Unterfelds in Partitionen einteilen.\nLassen sich Datens√§tze anhand von den Wertauspr√§gungen in einem Unterfeld gruppieren, ist es mitunter hilfreich die Gesamtmenge der Datens√§tze in Partitionen aufzuteilen. Ist das Unterfeld, nach dem partitioniert werden soll, wiederholbar, sind die erzeugten Partitionen i.d.R. nicht disjunkt. Ein Datensatz der das Unterfeld nicht besitzt, geht in keine Partition ein.\nIm folgenden Beispiel wird pro Entit√§tencode im Feld 004B.a eine Partition erstellt, die alle GND-Entit√§ten enth√§lt, die diesem Entit√§tencode zugeordnet sind.",
    "crumbs": [
      "Kommandos",
      "partition"
    ]
  },
  {
    "objectID": "commands/partition.html#optionen",
    "href": "commands/partition.html#optionen",
    "title": "partition",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGro√ü- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim √Ñhnlichkeitsvergleich von Zeichenketten mittels =*.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n-t &lt;string&gt;, --template &lt;string&gt;\n\nTemplate f√ºr die Dateinamen. Der Platzhalter {} wird durch den Namen der Partition ersetzt.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt.\n\n-o &lt;path&gt;, --outdir &lt;path&gt;\n\nAngabe, in welches Verzeichnis die Partitionen geschrieben werden sollen. Standardm√§√üig wird das aktuelle Verzeichnis verwendet.",
    "crumbs": [
      "Kommandos",
      "partition"
    ]
  },
  {
    "objectID": "commands/partition.html#beispiele",
    "href": "commands/partition.html#beispiele",
    "title": "partition",
    "section": "Beispiele",
    "text": "Beispiele\n\nEingrenzen der Partitionen\nSollen nicht alle Partitionen erstellt werden, kann die Anzahl der m√∂glichen Partition durch die Angabe eines Filterausdrucks eingegrenzt werden:\n$ pica partition -s \"004B{a | a in ['piz', 'saz']}\" DUMP.dat.gz -o out\n$ tree out/\nout\n‚îú‚îÄ‚îÄ piz.dat\n‚îî‚îÄ‚îÄ saz.dat\n\n\nBenutzerdefinierte Dateinamen\nStandardm√§√üig werden die erstellten Partitionen nach den Werten im Unterfeld benannt. Der Dateiname kann individuell mit der -t/--template-Option angepasst werden. Jedes Vorkommen der Zeichenfolge {} im Template wird durch den Wert des Unterfelds ersetzt. Endet die Datei auf der Dateiendung .gz, wird die Ausgabe automatisch im Gzip-Format komprimiert.\n$ pica partition -s \"004B.a\" --template \"code_{}.dat.gz\" DUMP.dat.gz -o out\n$ tree out/\nout\n‚îú‚îÄ‚îÄ code_gik.dat.gz\n‚îú‚îÄ‚îÄ code_piz.dat.gz\n‚îú‚îÄ‚îÄ code_saz.dat.gz\n‚îî‚îÄ‚îÄ code_wit.dat.gz\n\n\nKomprimierte Ausgabe\nMittels der Option --gzip bzw. -g erfolgt eine Komprimierung der Ausgabe:\n$ pica partition -s \"004B.a\" --gzip DUMP.dat.gz -o out\n$ tree out/\nout\n‚îú‚îÄ‚îÄ gik.dat.gz\n‚îú‚îÄ‚îÄ piz.dat.gz\n‚îú‚îÄ‚îÄ saz.dat.gz\n‚îî‚îÄ‚îÄ wit.dat.gz",
    "crumbs": [
      "Kommandos",
      "partition"
    ]
  },
  {
    "objectID": "commands/print.html",
    "href": "commands/print.html",
    "title": "print",
    "section": "",
    "text": "Optionen\nMithilfe des print-Kommandos k√∂nnen Datens√§tze in einer menschenlesbaren Form auf dem Terminal ausgegeben oder in eine Datei geschrieben werden. Das Format ist an die Darstellung in der WinIBW angelehnt: Felder werden zeilenweise ausgegeben; zuerst wird das Feld (bspw. 003@), dann - sofern vorhanden - die Okkurrenz (bspw. /01), und schlie√ülich die Liste von Unterfeldern ausgegeben. Dem Unterfeld-Code wird ein Dollarzeichen vorangestellt. Die Unterfeldwerte werden genau so ausgegeben, wie sie im Datensatz vorhanden sind; es findet kein [Escaping] von Sonderzeichen statt. Einzelne Datens√§tze werden durch eine Leerzeile voneinander getrennt.\nDer folgende Befehl gibt den ersten Datensatz aus:",
    "crumbs": [
      "Kommandos",
      "print"
    ]
  },
  {
    "objectID": "commands/print.html#optionen",
    "href": "commands/print.html#optionen",
    "title": "print",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-l &lt;number&gt;, --limit &lt;number&gt;\n\nEingrenzung der Ausgabe auf die ersten n Datens√§tze.\n\n--translit &lt;nf&gt;\n\nAusgabe wird in die angegebene Normalform transliteriert. M√∂gliche Werte: nfd, nfkd, nfc und nfkc.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datens√§tze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardm√§√üig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "print"
    ]
  },
  {
    "objectID": "commands/print.html#beispiele",
    "href": "commands/print.html#beispiele",
    "title": "print",
    "section": "Beispiele",
    "text": "Beispiele\n\nTransliteration der Ausgabe\nStandardm√§√üig werden die Unterfeldwerte so ausgegeben, wie sie im Datensatz vorkommen. Mit der Option --translit werden die Werte in die angegebene Unicode-Normalform transliteriert.\n$ pica print -s -l1 --translit nfc DUMP.dat.gz\n...\n028@ $d Yohan Wolfgang $a Gete\n028@ $d Y√¥h√¢n W√¥lfgang $c f√¥n $a Gete\n028@ $d Y√¥han Wolfgang $a G√™te\n028@ $d Yohann Volfqanq $a Gete\n028@ $d Yogann Vol πfgang $a Gete\n...\n028@ $T 01 $U Cyrl $L uzb $d –ô–æ“≥–∞–Ω–Ω –í–æ–ª—Ñ–≥–∞–Ω–≥ $a –ì—ë—Ç–µ\n028@ $T 01 $U Hans $P Ê≠åÂæ∑ $5 DE-576\n028@ $T 01 $U Hant $P Á¥ÑÁø∞„ÉªÊ≤ÉÁàæÂ§´Â≤°„ÉªÈ¶Æ„ÉªÊ≠åÂæ∑ $5 DE-576\n028@ $T 01 $U Hans $P Á∫¶Áø∞„ÉªÊ≤ÉÂ∞îÂ§´ÂÜà„ÉªÂÜØ„ÉªÊ≠åÂæ∑ $5 DE-576\n028@ $T 01 $U Jpan $d „É®„Éè„É≥„Éª„É¥„Ç©„É´„Éï„Ç¨„É≥„Ç∞„Éª„Éï„Ç©„É≥ $a „Ç≤„Éº„ÉÜ $5 DE-576\n028@ $T 01 $U Hebr $d ◊ô◊ï◊î◊ü ◊ï◊ï◊ú◊§◊í◊†◊í ◊§◊ï◊ü $a ◊í◊™◊î\n028@ $T 01 $U Hans $P Ê≠åÂæ∑\n028@ $T 01 $U Jpan $d „É®„Éè„É≥„Éª„É¥„Ç©„É´„Éï„Ç¨„É≥„Ç∞„Éª„Éï„Ç©„É≥ $a „Ç≤„Éº„ÉÜ\n028@ $T 01 $U Geor $d ·Éò·Éù·É∞·Éê·Éú ·Éï·Éù·Éö·É§·Éí·Éê·Éú·Éí ·É§·Éù·Éú $a ·Éí·Éù·Éî·Éó·Éî\n028A $d Johann Wolfgang $c von $a Goethe\n...",
    "crumbs": [
      "Kommandos",
      "print"
    ]
  },
  {
    "objectID": "commands/sample.html",
    "href": "commands/sample.html",
    "title": "sample",
    "section": "",
    "text": "Optionen\nDas sample-Kommando zieht nach dem Zufallsprinzip gleichm√§√üig Datens√§tze aus der Eingabe.\nIm folgenden Beispiel werden zuf√§llig 200 Datens√§tze aus der Eingabe ausgew√§hlt und in die Datei samples.dat geschrieben:",
    "crumbs": [
      "Kommandos",
      "sample"
    ]
  },
  {
    "objectID": "commands/sample.html#optionen",
    "href": "commands/sample.html#optionen",
    "title": "sample",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGro√ü- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim √Ñhnlichkeitsvergleich von Zeichenketten mittels =*.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im [Gzip]-Format.\n\n--seed &lt;number&gt;\n\nInitialisiert den Zufallszahlengenerator mit einem seed-Wert, um eine deterministische Auswahl zu erhalten.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt.\n\n-o &lt;path&gt;, --outdir &lt;path&gt;\n\nAngabe, in welches Verzeichnis die Partitionen geschrieben werden sollen. Standardm√§√üig wird das aktuelle Verzeichnis verwendet.",
    "crumbs": [
      "Kommandos",
      "sample"
    ]
  },
  {
    "objectID": "commands/sample.html#beispiele",
    "href": "commands/sample.html#beispiele",
    "title": "sample",
    "section": "Beispiele",
    "text": "Beispiele\n\nZuf√§llige PPN-Liste\nIn Kombination mit dem select-Kommando kann eine zuf√§llige PPN-Liste erzeugt werden:\n$ pica sample 3 DUMP.dat.gz | pica select -H 'ppn' '003@.0' -o samples.csv",
    "crumbs": [
      "Kommandos",
      "sample"
    ]
  },
  {
    "objectID": "commands/select.html",
    "href": "commands/select.html",
    "title": "select",
    "section": "",
    "text": "Optionen\nMit dem select-Kommando werden Werte eines unter mehrerer Unterfelder tabelliert. Dies erm√∂glicht weiterf√ºhrende Datenanalysen in Excel, Python oder R.\nIm folgenden Beispiel wird die PPN eines Datensatzes (Feld 003@.0) und die dazugeh√∂rige Satzart Feld (002@.0) in einer Tabelle im CSV-Format erzeugt:\nIst ein Feld oder Unterfeld mehrfach vorhanden, werden pro Datensatz alle Zeilen mit diesen Werten kombiniert, indem das Kartesische Produkt gebildet wird. Dadurch ist es m√∂glich f√ºr jede wiederholte Wertauspr√§gung eine Zeile zu erzeugen. In Kombination mit einem nicht-wiederholten Feld (bspw. der PPN im Feld 003@.0 lassen sich Tabellen, im Sinne des Entity-Relationship-Modell, erzeugen. Im Folgenden wird eine Tabelle erstellt, die in der ersten Spalte die PPN und in der zweiten Spalte, die dazugeh√∂rigen Teilbestandskennzeichen aus dem Feld 008A.a enth√§lt. Jede Kombination von einer PPN mit einem Teilbestandskennzeichen erzeugt eine neue Zeile:\n-t, --tsv Ausgabe erfolgt im TSV-Format.",
    "crumbs": [
      "Kommandos",
      "select"
    ]
  },
  {
    "objectID": "commands/select.html#optionen",
    "href": "commands/select.html#optionen",
    "title": "select",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n--squash\n\nWenn das Flag gesetzt ist, werden wiederholte Unterfelder als eine Zeichenkette zusammengefasst. Die einzelnen Werte werden durch einen Separator (siehe --separator) getrennt.\n\n--merge\n\nIn jeder Spalte werden alle Zeilen zu einer Zeile zusammengefasst. Die einzelnen Werte werden durch einen Separator (siehe --separator) getrennt. Ist die Option gesetzt wird f√ºr jeden Datensatz maximal eine Zeile erzeugt.\n\n--separator &lt;value&gt;\n\nFestlegen des Separators, der f√ºr --squash und --merge genutzt wird. Standardm√§√üig wird der Separator | verwendet.\n\n--no-empty-columns\n\nIst die Option gesetzt, werden nur Zeilen geschrieben, in denen jede Spalte einen nicht-leeren Wert enth√§lt.\n\n--unique, -u\n\nMehrfach vorkommende Zeilen werden ignorieren.\n\n-i, --ignore-case\n\nGro√ü- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim √Ñhnlichkeitsvergleich von Zeichenketten mittels =*.\n\n-A &lt;file&gt;, --allow-list &lt;file&gt;\n\nEs werden alle Datens√§tze ignoriert, die nicht explizit in der Positivliste[^1] auftauchen. Werden mehrere Positivlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n-D &lt;file&gt;, --deny-list &lt;file&gt;\n\nEs werden alle Datens√§tze ignoriert, die in der Negativliste auftauchen. Werden mehrere Negativlisten angegeben, wird die Mengenvereinigung aus allen Listen gebildet.\n\n--where &lt;filter&gt;\n\nAngabe eines Filters, der auf die erzeugten Datens√§tze angewandt wird.\n\n--and &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters mittels der booleschen &&-Verkn√ºpfung. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters mittels der booleschen ||-Verkn√ºpfung. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzuf√ºgen eines zus√§tzlichen Filters. Der urspr√ºngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n-H &lt;header&gt;, --header &lt;header&gt;\n\nKopfzeile, die den Ergebnissen vorangestellt wird; Spaltennamen werde mit einem Komma separiert.\n\n\n\n\n--translit &lt;normalization&gt;\n\nAusgabe wird in die angegebene Normalform transliteriert. M√∂gliche Werte: nfd, nfkd, nfc und nfkc.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datens√§tze in eine Datei mittels -o bzw. --output.\n\n--limit &lt;n&gt;\n\nLimitiert die Aufbereitung auf die ersten &lt;n&gt; Datens√§tze.\n\n--append\n\nIst die Option gesetzt, wird die Ausgabe an das Ende der Ausgabedatei geschrieben, anstatt diese zu √ºberschreiben.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardm√§√üig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "select"
    ]
  },
  {
    "objectID": "commands/slice.html",
    "href": "commands/slice.html",
    "title": "slice",
    "section": "",
    "text": "Optionen\nMittels des slice-Kommandos kann ein zusammenh√§ngender Teilbereich aus der Eingabe ausgeschnitten werden. Dabei wird der Teilbereich als halb-offenes Intervall angegeben, wobei die Positionen von 0 an gez√§hlt werden. Beim Auftreten eines ung√ºltigen Datensatzes wird die Position weitergez√§hlt. Enth√§lt bspw. die Eingabe 1.000 Zeilen, mit 990 Datens√§tzen und 10 ung√ºltigen Zeilen, dann sind die Positionen von 0 bis 999 durchnummeriert.\nDas folgende Beispiel extrahiert alle (g√ºltigen) Datens√§tze aus den Positionen 2 bis 4:",
    "crumbs": [
      "Kommandos",
      "slice"
    ]
  },
  {
    "objectID": "commands/slice.html#optionen",
    "href": "commands/slice.html#optionen",
    "title": "slice",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n--start &lt;number&gt;\n\nStartposition des Teilbereichs (Voreinstellung: 0).\n\n--end &lt;number&gt;\n\nEndposition des Teilbereichs; diese Position ist nicht mehr Teil der Ausgabe. Ist keine Endposition angegeben, wird der Teilbereich bis zum Ende der Eingabe fortgef√ºhrt. Diese Option ist nicht kombinierbar mit --length.\n\n--length &lt;number&gt;\n\nFestlegen der maximalen Anzahl an Datens√§tzen, die in der Ausgabe enthalten sind. Diese Option kann nicht mit --end kombiniert werden.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n--append\n\nWenn die Ausgabedatei bereits existiert, wird die Ausgabe an die Datei angehangen. Ist das Flag nicht gesetzt, wird eine bestehende Datei standardm√§√üig √ºberschrieben.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datens√§tze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardm√§√üig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "slice"
    ]
  },
  {
    "objectID": "commands/slice.html#beispiele",
    "href": "commands/slice.html#beispiele",
    "title": "slice",
    "section": "Beispiele",
    "text": "Beispiele\n\nAusschneiden eines Teilbereichs fester Gr√∂√üe\nWenn die Eingabe ausreichend Datens√§tze enth√§lt, kann beginnend bei einer festen Position (--start) ein Teilbereich mit einer festen L√§nge (--length) ausgeschnitten werden.\nIm folgenden Beispiel wird beginnend beim dritten Datensatz (Position 2) ein Teilbereich mit einer L√§nge von zwei ausgeschnitten:\n$ pica slice -s --start 2 --length 2 DUMP.dat.gz -o slice.dat\n$ pica count --records slice.dat\n2",
    "crumbs": [
      "Kommandos",
      "slice"
    ]
  },
  {
    "objectID": "commands/split.html",
    "href": "commands/split.html",
    "title": "split",
    "section": "",
    "text": "Optionen\nMithilfe des split-Kommandos k√∂nnen alle Datens√§tze aus der Eingabe in mehrere Dateien aufgeteilt werden, wobei jede Datei eine maximale Anzahl an Datens√§tzen nicht √ºberschreitet. Ein Anwendungsfall f√ºr das Splitting k√∂nnte eine automatisierte Stapelverarbeitung oder die parallele Verarbeitung der entstandenen Dateien sein.\nDer folgende Aufruf des split-Kommandos teilt die zw√∂lf Datens√§tze der Eingabe (DUMP.dat.gz) in drei Dateien mit maximal f√ºnf Datens√§tzen pro Datei. Die ersten beiden Dateien (0.dat und 1.dat) enthalten die maximale Anzahl an Datens√§tzen, die letzte Datei (2.dat) die restlichen zwei Datens√§tze.",
    "crumbs": [
      "Kommandos",
      "split"
    ]
  },
  {
    "objectID": "commands/split.html#optionen",
    "href": "commands/split.html#optionen",
    "title": "split",
    "section": "",
    "text": "-s, --skip-invalid\n\n√úberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n--template\n\nTemplate f√ºr die Dateinamen. Der Platzhalter {} wird durch eine fortlaufende Nummer ersetzt.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen g√ºltigen sowie invaliden Datens√§tze anzeigt.\n\n-o &lt;path&gt;, --outdir &lt;path&gt;\n\nAngabe, in welches Verzeichnis die Ausgabe geschrieben werden soll. Standardm√§√üig wird das aktuelle Verzeichnis verwendet.",
    "crumbs": [
      "Kommandos",
      "split"
    ]
  },
  {
    "objectID": "commands/split.html#beispiele",
    "href": "commands/split.html#beispiele",
    "title": "split",
    "section": "Beispiele",
    "text": "Beispiele\n\nBenutzerdefinierte Dateinamen\nStandardm√§√üig werden die erstellten Dateien beginnend bei 0 durchnummeriert. Der Dateiname kann individuell mit der --template-Option angepasst werden. Jedes Vorkommen der Zeichenfolge {} im Template wird durch die aktuelle Nummer ersetzt. Endet die Datei auf der Dateiendung .gz, wird die Ausgabe automatisch im Gzip-Format komprimiert.\n$ pica split -s --template \"CHUNK_{}.dat.gz\" 10 DUMP.dat.gz\n$ tree\n.\n‚îú‚îÄ‚îÄ CHUNK_0.dat.gz\n‚îú‚îÄ‚îÄ CHUNK_1.dat.gz\n‚îî‚îÄ‚îÄ DUMP.dat.gz\n\n$ pica count --records CHUNK_0.dat.gz\n10\n\n$ pica count --records CHUNK_1.dat.gz\n2\n\n\nKomprimierte Ausgabe\nMittels der Option --gzip bzw. -g erfolgt eine Komprimierung der Ausgabe:\n$ pica split -s --gzip 10 DUMP.dat.gz\n$ tree\n.\n‚îú‚îÄ‚îÄ 0.dat.gz\n‚îú‚îÄ‚îÄ 1.dat.gz\n‚îî‚îÄ‚îÄ DUMP.dat.gz\n\n$ pica count --records 0.dat.gz\n10\n\n$ pica count --records 1.dat.gz\n2",
    "crumbs": [
      "Kommandos",
      "split"
    ]
  }
]