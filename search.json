[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pica-rs",
    "section": "",
    "text": "Start\n\nDas Projekt pica-rs ermöglicht eine effiziente Verarbeitung von bibliografischen Metadaten, die in PICA+, dem internen Format des OCLC-Katalogsystems, kodiert sind. Das Programm pica stellt unterschiedliche Kommandos zur Verfügung, um Daten auszuwählen, statistisch aufzubereiten oder für die Weiterverarbeitung in Data Science-Frameworks wie Polars (Python) oder der Sprache R nutzbar zu machen. Die Anwendung ist in der Programmiersprache Rust geschrieben und lässt sich unter den Betriebsystemen Linux, macOS und Windows verwenden. Die Kommandos lassen sich über die Standard-Datenströme (Kombination von verschiedenen Programmen mittels Unix-Pipelines) miteinander verketten, wodurch sich leicht Metadaten-Workflows erstellen und automatisieren lassen.\nDie Entwicklung von pica-rs wurde vom Referat Automatische Erschließungsverfahren; Netzpublikationen (AEN) der Deutsche Nationalbibliothek (DNB) initiert und wird dort für die Erstellung von Datenanalysen sowie für die Automatisierung von Workflows (Datenmanagement) im Rahmen der automatischen Inhaltserschließung genutzt. Weiterhin wird es zur Unterstützung der Forschungsarbeiten im KI-Projekt sowie für diverse andere Datenanalysen innerhalb der DNB eingesetzt.",
    "crumbs": [
      "Start"
    ]
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "Installation",
    "section": "",
    "text": "Installation unter Linux\nDas Kommandozeilen-Tool pica lässt sich unter den Betriebssystemen Linux, macOS und Windows nutzen. Folgend wird die Installation sowie Einrichtung und Konfiguration des Tools beschrieben. Die Zeichenkette X.Y.Z ist ein Platzhalter für eine konkrete pica-rs Version und muss in dem Befehl entsprechend ersetzt werden.\nAbhängig von der genutzten Linux-Distribution, gibt es unterschiedliche Möglichkeiten der Installation. Vorgefertigte Releases stehen auf der Plattform GitHub zum Download bereit.",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#installation-unter-linux",
    "href": "install.html#installation-unter-linux",
    "title": "Installation",
    "section": "",
    "text": "Debian und Ubuntu\nUnter Debian GNU/Linux und Ubuntu Linux können fertige DEB-Pakete genutzt werden. Diese können mit dem dpkg-Programm installiert werden:\n$ dpkg -i pica_X.Y.Z-glibc2.35-1_amd64.deb\n\n\nRed Hat, SUSE und CentOS\nFür die Distributionen Red Hat Linux, SUSE Linux und CentOS Linux stehen fertige RPM-Pakete zum Download bereit, die sich mit dem rpm-Programm installieren lassen:\n$ rpm -i pica-X.Y.Z-glibc2.35-1.x86_64.rpm\n\n\nBinary Releases\nSoll pica nicht über einen Paketmanager installiert werden, stehen für die Zielarchitektur x86_64-unknown-linux-gnu mit den glibc-Versionen 2.28, 2.31 und 2.35 fertige Binary Releases zur Verfügung. Die glibc-Version des Systems lässt sich mit dem Aufruf ldd --version ermitteln.\nDas tar-Archiv enthält neben dem Tool pica auch weitere Dateien wie bspw. Shell-Skripte zur Befehlszeilenergänzung:\n$ tar -tf\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35.tar.gz\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/pica\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/README.md\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/pica.zsh\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/LICENSE\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/pica.fish\npica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/pica.bash\nEine systemweite Installation von pica in das Verzeichnis /usr/local/bin kann mit dem install erfolgen. Hierfür sind ggf. root-Rechte nötig:\n$ tar xfz pica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35.tar.gz\n$ sudo install -m755 pica-X.Y.Z-x86_64-unknown-linux-gnu-glibc2.35/pica \\\n      /usr/local/bin/pica",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#installation-unter-macos",
    "href": "install.html#installation-unter-macos",
    "title": "Installation",
    "section": "Installation unter macOS",
    "text": "Installation unter macOS\nUnter macOS wird nur die Zielarchitektur x86_64-apple-darwin (macOS 10.7+, Lion+) unterstützt. Diese lässt sich analog wie unter Linux installieren:\n$ tar xfz pica-X.Y.Z-x86_64-apple-darwin.tar.gz\n$ install -m755  pica-X.Y.Z-x86_64-apple-darwin/pica /usr/local/bin/pica",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#installation-unter-windows",
    "href": "install.html#installation-unter-windows",
    "title": "Installation",
    "section": "Installation unter Windows",
    "text": "Installation unter Windows\nUnter Windows (x86_64-pc-windows-gnu oder x86_64-pc-windows-msvc) kann das Programm direkt dem zip-Archiv entnommen werden. Nach einem Wechsel in das Verzeichnis, in dem sich die pica.exe befindet, kann das Programm direkt genutzt werden. Soll pica aus jedem beliebigen Verzeichnis heraus aufrufbar sein, dann muss der Installationspfad in der PATH-Umgebungsvariable mit aufgelistet werden.",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "install.html#aus-dem-quellcode-installieren",
    "href": "install.html#aus-dem-quellcode-installieren",
    "title": "Installation",
    "section": "Aus dem Quellcode installieren",
    "text": "Aus dem Quellcode installieren\nDas Projekt lässt sich auch direkt aus den Quellen kompilieren. Hierfür wird eine aktuelle Rust-Version (&gt;= 1.74.1) mit dem Paketmanager cargo benötigt.\nDer aktuelle Entwicklungsstand lässt sich wie folgt installieren:\n$ git clone https://github.com/deutsche-nationalbibliothek/pica-rs.git\n$ cd pica-rs\n$ cargo build --release\nDas fertige pica-Programm liegt im Verzeichnis target/release/ und kann bspw. in das Verzeichnis /usr/local/bin installiert werden:\n$ install -m755 target/release/pica /usr/local/bin/pica\nWenn der Quellcode nicht benötigt wird, kann das Projekt auch direkt über den Paketmanager cargo installiert werden:\n$ # Installation der aktuellen Entwicklungsversion\n$ cargo install --git https://github.com/deutsche-nationalbibliothek/pica-rs \\\n     --branch main pica-cli\n\n$ # Installation der Version X.Y.Z\n$ cargo install --git https://github.com/deutsche-nationalbibliothek/pica-rs \\\n      --tag vX.Y.Z pica-cli\nDas fertige Programm befindet sich dann unter ~/.cargo/bin/pica.\n\nFeatures\nWird das Programm anhand der Quellen gebaut, können optionale Features aktiviert werden.Die folgenden Funktionen können mit der cargo-Option --features aktiviert werden:\n\nunstable, um die neuesten Funktionalitäten, die noch in der Entwicklung sind und für eine der nächsten Versionen vorgesehen sind, zu aktivieren\nund compat, um eine höhere Kompatibilität mit der Abfragesprache PICA Path zu erhalten.",
    "crumbs": [
      "Erste Schritte",
      "Installation"
    ]
  },
  {
    "objectID": "commands/completions.html",
    "href": "commands/completions.html",
    "title": "completions",
    "section": "",
    "text": "Bash\nDas completions-Kommando erzeugt Dateien, die Anweisungen enthalten, welche Argumente und Optionen des Toolkits für eine Shell zur Befehlszeilenergänzung verfügbar sind.\nNachfolgend werden exemplarisch die Befehle gezeigt, die für die Einbindung in die jeweilige Shell nötig sind. Die Schritte sind vom System sowie der Nutzereinstellung abhängig und müssen ggf. angepasst werden.\nEs werden folgende Shells unterstützt:\nAlternativ kann auch immer die aktuelle Version, passend zur installierten pica-Version, eingebunden werden. Dafür muss folgende Zeile in die .bashrc eingetragen werden:",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#bash",
    "href": "commands/completions.html#bash",
    "title": "completions",
    "section": "",
    "text": "$ mkdir -p ~/.local/share/bash-completion/completions\n$ pica completions bash \\\n    -o  ~/.local/share/bash-completion/completions/pica\n\n$ source &lt;(pica completions bash)\n\nBash (macOS/Homebrew)\n$ mkdir -p $(brew --prefix)/etc/bash_completion.d\n$ pica completions bash \\\n    -o $(brew --prefix)/etc/bash_completion.d/pica.bash-completion",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#elvish",
    "href": "commands/completions.html#elvish",
    "title": "completions",
    "section": "Elvish",
    "text": "Elvish\n$ mkdir -p ~/.local/share/elvish/lib/completions\n$ pica completions elvish -o ~/.local/share/elvish/lib/completions/pica.elv\n$ echo \"use completions/pica\" &gt;&gt; ~/.elvish/rc.elv",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#fish",
    "href": "commands/completions.html#fish",
    "title": "completions",
    "section": "Fish",
    "text": "Fish\n$ mkdir -p ~/.config/fish/completions\n$ pica completions fish -o ~/.config/fish/completions/pica.fish",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#powershell",
    "href": "commands/completions.html#powershell",
    "title": "completions",
    "section": "Powershell",
    "text": "Powershell\n$ pica completions powershell \\\n     &gt;&gt; ${env:USERPROFILE}\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/completions.html#zsh",
    "href": "commands/completions.html#zsh",
    "title": "completions",
    "section": "ZSH",
    "text": "ZSH\nDer Pfad ~/.zfunc muss in der Variable $fpath gesetzt sein, bevor die Funktion compinit aufgerufen wird.\n$ pica completions zsh -o ~/.zfunc/_pica",
    "crumbs": [
      "Kommandos",
      "completions"
    ]
  },
  {
    "objectID": "commands/concat.html",
    "href": "commands/concat.html",
    "title": "concat",
    "section": "",
    "text": "Optionen\nDas concat-Kommando (Alias cat) liest Datensätze direkt von der Standardeingabe (stdin) oder aus Dateien ein und fügt diese zusammen. Die Ausgabe kann entweder in eine Datei oder in die Standardausgabe (stdout) geschrieben werden.\nDer wichtigste Anwendungsfall des Kommandos besteht in Kombination mit den Kommandos partition oder split, da mittels concat das Ergebnis dieser Kommandos (teil-)rückgängig gemacht werden kann. Häufig macht es Sinn, eine große Datei in viele kleinere Dateien anhand eines Kriteriums zu teilen (bspw. nach der Sprache), um anschließend einzelne Dateien wieder zusammenzufügen.\nDas folgende Beispiel fügt die Datensätze aus den Dateien ger.dat und eng.dat zu einer Datei ger_eng.dat zusammen:",
    "crumbs": [
      "Kommandos",
      "concat"
    ]
  },
  {
    "objectID": "commands/concat.html#optionen",
    "href": "commands/concat.html#optionen",
    "title": "concat",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-u, --unique\n\nEs werden keine Duplikate in die Ausgabe geschrieben. Die Strategie zur Erkennung von Duplikaten wird mittels der Option --unique-strategy festgelegt.\n\n--unique-strategy &lt;STRATEGY&gt;\n\nFestlegen, wie Duplikate erkannt werden sollen. Standardmäßig ist der Wert idn ausgewählt, der Duplikate anhand der PPN aus dem Feld 003@.0 erkannt. Alternativ kann die Strategie hash gewählt werden. Der Vergleich erfolgt dann über die SHA-256 Prüfsumme der Datensätze.\n\n--append\n\nWenn die Ausgabedatei bereits existiert, wird die Ausgabe an die Datei angehangen. Ist das Flag nicht gesetzt, wird eine bestehende Datei standardmäßig überschrieben.\n\n--tee &lt;filename&gt;\n\nAbzweigen der Ausgabe in eine zusätzliche Datei.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben. Endet der Dateiname mit dem Suffix .gz, wird die Ausgabe automatisch im Gzip-Format komprimiert.",
    "crumbs": [
      "Kommandos",
      "concat"
    ]
  },
  {
    "objectID": "commands/concat.html#beispiele",
    "href": "commands/concat.html#beispiele",
    "title": "concat",
    "section": "Beispiele",
    "text": "Beispiele\n\nÜberspringen ungültiger Datensätze\nDer eingangs verwendete Befehl geht davon aus, dass die zwei Partitionen ausschließlich gültige Datensätze enthalten. Gültig in diesem Zusammenhang bedeutet, dass es sich um valide Datensätze im Format PICA+ handelt und nicht ob ein Datensatz einem bestimmten Regelwerk entspricht.\nDas Ausschließen von ungültigen Datensätzen wird mit der Option -s oder --skip-invalid erreicht:\n$ pica concat --skip-invalid DUMP.dat.gz -o dump_valid.dat\n$ pica concat -s DUMP.dat.gz --output dump_valid.dat.gz\nAlternativ lässt sich das Überspringen ungültiger Datensätze mittels des config-Kommandos einstellen.\n\n\nLesen von der Standardeingabe\nDas Kommando kann auch direkt von der Standardeingabe (stdin) lesen. Das ist bspw. dann hilfreich, wenn die Ausgabe aus einem vorhergehenden Pipeline-Schritt mit dem Inhalt einer oder mehrerer Dateien konkateniert werden soll.\nDas folgende Beispiel liest im ersten Pipeline-Schritt die Datei dump1.dat ein, entfernt ungültige Datensätze und gibt die Ausgabe nach stdout aus. Der zweite Pipeline-Schritt liest diese Datensätze ein (-) und konkateniert diese mit den Datensätzen aus der Datei dump2.dat. Das Ergebnis wird in die Datei out.dat geschrieben.\n$ pica concat -s dump1.dat | pica cat - dump2.dat -o out.dat\nDer Dateiname - steht für die Standardeingabe (stdin). Wären die zwei Argumente vertauscht (pica cat dump2.dat -), dann würden erst die gültigen Datensätze aus der Datei dump1.dat und anschließend die Datensätze aus dem ersten Pipeline-Schritt geschrieben.\n\n\nHinzufügen von Datensätzen\nWenn eine Ausgabedatei bereits existiert, wird diese standardmäßig neu angelegt und überschrieben. Soll das Verhalten dahingehend geändert werden, dass an die bestehenden Dateien angehangen wird, kann dies mit der --append-Option erreicht werden. Diese Option ändert das Verhalten von --output und --tee. Die Option hat auf das Verhalten beim Schreiben in die Standardausgabe keine Auswirkung.\nIm folgenden Beispiel erzeugt der erste Befehl eine neue Datei gnd.dat. Sollte die Datei bereits existieren, wird der Datei-Inhalt überschrieben. Die darauffolgenden Kommandos hängen jeweils an das Ende der Datei gnd.dat an.\n$ pica concat Tp*.dat -o gnd.dat\n$ pica concat --append Ts*.dat -o gnd.dat\n$ pica concat --append Tu*.dat -o gnd.dat\n...\n\n\nAbzweigen der Ausgabe\nGelegenlich kann es nützlich sein, die Ausgabe des concat-Kommandos in eine Datei zu schreiben und gleichzeitig die Ausgabe an einen weiteren Pipeline-Schritt weiterzureichen. Dies hat den Vorteil, dass zwei CPU-Kerne gleichzeitig genutzt werden können. Mit der --tee-Option lässt sich dieses Verhalten erzielen. Der Name der Option leitet sich von dem T-Stück (engl. tee connector) ab, mit dem ein Klempner eine Abzeigung in eine Leitung einbaut.\nIm folgenden Beispiel werden alle Tp*.dat zusammengefügt und in eine Datei Tp.dat geschrieben. Gleichzeitig werden alle Datensätze mit dem filter-Kommando nach der Satzart Tp2 im Feld 002@.0 gefiltert.\n$ pica concat partitions/Tp*.dat --tee gnd_person.dat | \\\n    pica filter \"002@.0 =^ 'Tp2'\" -o Tp2.dat\n\n\nEntfernen von Duplikaten\nDuplikate können durch die Angabe des Flags --unique (-u) entfernt werden. Standardmäßig erfolgt die Erkennung von Duplikaten per PPN aus dem Feld 003@.0 . Alternativ kann durch die Angabe der Option --unique-strategy die Variante hash ausgewählt werden, bei der nur solche Datensätze als gleich gewertet werden, bei denen alle Bytes gleich sind.\n$ pica concat --unique --unique-strategy hash ger.dat eng.dat -o out.dat\n$ pica concat --unique --unique-strategy idn ger.dat eng.dat -o out.dat",
    "crumbs": [
      "Kommandos",
      "concat"
    ]
  },
  {
    "objectID": "commands/config.html",
    "href": "commands/config.html",
    "title": "config",
    "section": "",
    "text": "Optionen\nMithilfe des config-Kommandos lassen sich bestimmte Optionen setzen und das Laufzeitverhalten von pica beeinflussen. Falls noch keine Konfigurationsdatei existiert, wird diese automatisch angelegt und je nach Betriebssystem in den dafür vorgesehenen Pfaden gespeichert:",
    "crumbs": [
      "Kommandos",
      "config"
    ]
  },
  {
    "objectID": "commands/config.html#optionen",
    "href": "commands/config.html#optionen",
    "title": "config",
    "section": "",
    "text": "Überspringen ungültiger Datensätze\nKann eine Zeile in der Eingabe nicht als Datensatz (normalisiertes PICA+) dekodiert werden, brechen die meisten Kommandos die Verarbeitung mit einer Fehlermeldung ab. Dieses Verhalten kann mit der Option --skip-invalid geändert werden, sodass diese ungültigen Datensätze übersprungen werden. Dieses Verhalten kann auch in der Konfigurationsdatei hinterlegt werden:\n$ pica config skip-invalid true\nNachdem die Variable gesetzt wurde, kann die Angabe der --skip-invalid-Option entfallen. Die Einstellung lässt sich mit --unset rückgängig machen:\n$ pica config --unset skip-invalid\n\n\nÄndern der Unicode Normalform\nLiegen die PICA-Daten in einer anderen Unicode Normalform vor, lassen sich Filterausdrücke mit der Option normalization an die Normalform der Daten angleichen:\n$ pica config normalization nfd\nEs werden die vier Normalformen nfd, nfc, nfkc und nfkd unterstützt. Nur wenn eine Normalform ausgewählt ist, werden Filterausdrücke immer entsprechend transliteriert. Die Einstellung lässt sich mit --unset rückgängig machen:\n$ pica config --unset normalization",
    "crumbs": [
      "Kommandos",
      "config"
    ]
  },
  {
    "objectID": "commands/count.html",
    "href": "commands/count.html",
    "title": "count",
    "section": "",
    "text": "Optionen\nSoll die Anzahl der Datensätze und deren Felder sowie Unterfelder ermittelt werden, kann dies mit dem count-Kommando erfolgen. Ungültige Datensätze können mit dem Flag --skip-invalid (bzw. -s) übersprungen werden. Im folgenden Beispiel wird die Datei DUMP.dat.gz eingelesen und die Anzahl der in der Datei enthaltenen Datensätzes (records), Felder (_fields) und Unterfelder (subfields) ausgegeben:",
    "crumbs": [
      "Kommandos",
      "count"
    ]
  },
  {
    "objectID": "commands/count.html#optionen",
    "href": "commands/count.html#optionen",
    "title": "count",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGroß- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim Ähnlichkeitsvergleich von Zeichenketten mittels =*.\n\n--append\n\nWenn die Ausgabedatei bereits existiert, wird die Ausgabe an die Datei angehangen. Ist das Flag nicht gesetzt, wird eine bestehende Datei standardmäßig überschrieben.\n\n--records\n\nGibt nur die Anzahl der vorhandenen Datensätze aus. Dieses Flag ist nicht mit den Optionen --fields, --subfields, --csv, --tsv und --no-header kombinierbar.\n\n--fields\n\nGibt nur die Anzahl der vorhandenen Felder aus. Dieses Flag ist nicht mit den Optionen --records, --subfields, --csv, --tsv und --no-header kombinierbar.\n\n--subfields\n\nGibt nur die Anzahl der vorhandenen Unterfelder aus. Dieses Flag ist nicht mit den Optionen --records, --fields, --csv, --tsv und --no-header kombinierbar.\n\n--csv\n\nDie Ausgabe erfolgt im CSV-Format.\n\n--tsv\n\nDie Ausgabe erfolgt im TSV-Format.\n\n--no-header\n\nEs wird keine Kopfzeile in die Ausgabe geschrieben.\n\n--where &lt;expr&gt;\n\nAngabe eines Filters, um Datensätze aus der Eingabe auszuwählen.\n\n--and &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen &&-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && &lt;expr&gt;.\n\n--or &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters mittels der booleschen ||-Verknüpfung. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; || &lt;expr&gt;.\n\n--not &lt;expr&gt;\n\nHinzufügen eines zusätzlichen Filters. Der ursprüngliche Filterausdruck &lt;filter&gt; wird zum Ausdruck &lt;filter&gt; && !(&lt;expr&gt;).\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "count"
    ]
  },
  {
    "objectID": "commands/count.html#beispiele",
    "href": "commands/count.html#beispiele",
    "title": "count",
    "section": "Beispiele",
    "text": "Beispiele\n\nAusgabe im CSV/TSV-Format\nDie Ausgabe des Kommandos kann auch im Format CSV bzw. TSV erfolgen, was die Weiterverarbeitung in anderen Programmen erleichtert. Die Ausgabe der Kopfzeile lässt sich mit dem Flag --no-header ausschalten.\n$ pica count -s --csv DUMP.dat.gz\nrecords,fields,subfields\n12,1035,3973\n\n$ pica count -s --tsv DUMP.dat.gz\nrecords fields  subfields\n12  1035    3973\n\n$ pica count -s --csv --no-header DUMP.dat.gz\n12,1035,3973\n\n\nAusgabe in eine Datei\nDie Ausgabe des Kommandos wird standardmäßig auf der Konsole ausgegeben. Diese kann mit der Option --output (bzw. -o) in eine Datei umgeleitet werden. Soll diese Datei eine neue Zeile erhalten und nicht bei jedem Aufruf überschrieben werden, kann dies mit dem Flag --append erzielt werden.\n$ pica count -s --csv -o count.csv DUMP.dat.gz\n$ cat count.csv\nrecords,fields,subfields\n12,1035,3973\n\n$ pica count -s --csv --append -o count.csv DUMP.dat.gz\n$ cat count.csv\nrecords,fields,subfields\n12,1035,3973\n12,1035,3973\n\n\nAusgabe von Einzelwerten\nSoll entweder die Anzahl von Datensätzen, Feldern oder Unterfeldern ausgegeben werden, kann dies mit den Flags --records, --fields bzw. --subfields erfolgen. Diese Flags schließen sich gegenseitig aus und können nicht mit den Flags --csv, --tsv und --no-header kombiniert werden.\n$ pica count -s --records DUMP.dat.gz\n12\n\n$ pica count -s --fields DUMP.dat.gz\n1035\n\n$ pica count -s --subfields DUMP.dat.gz\n3973\n\n\nAnwendungsbeispiel\nSoll die Veränderung (Anzahl Datensätze, Felder, Unterfelder) eines PICA-Abzugs über die Zeit verfolgt werden, könnte dies wie folgt erreicht werden:\n$ echo \"date,records,fields,subfields\" &gt; count.csv # Kopfzeile\n$ pica count -s dump_20220222.dat.gz --append -o count.csv # Initialer Aufruf\n$ pica count -s dump_20220223.dat.gz --append -o count.csv # Aufruf nach x Tagen\n\n$ cat count.csv\n$ records,fields,subfields\n7,247,549\n9,347,1022\nSoll auch das aktuelle Datum vor die Zeile geschrieben werden, könnten bspw. folgende Unix-Kommandos genutzt werden:\n# Schreiben der Kopfzeile\n$ echo \"date,records,fields,subfields\" &gt; count.csv\n\n# Aufruf am 22.02.2022\n$ pica count -s --no-header --csv dump_20220222.dat.gz | \\\n    xargs -d\"\\n\" -I {} date +\"%Y-%m-%d,{}\" &gt;&gt; count.csv\n\n# Aufruf am 23.02.2022\n$ pica count -s --no-header --csv dump_20220223.dat.gz | \\\n    xargs -d\"\\n\" -I {} date +\"%Y-%m-%d,{}\" &gt;&gt; count.csv\n\n$ cat count.csv\n$ date,records,fields,subfields\n2022-02-22,7,247,549\n2022-02-23,9,347,1022",
    "crumbs": [
      "Kommandos",
      "count"
    ]
  },
  {
    "objectID": "commands/hash.html",
    "href": "commands/hash.html",
    "title": "hash",
    "section": "",
    "text": "Optionen\nMithilfe des Kommandos hash lässt sich eine Tabelle erzeugen, die in der ersten Spalte die PPN (Feld 003@.0) und in der zweiten Spalte den SHA-256-Hashwert des Datensatzes enthält.\nMitunter kommt es vor, dass eine regelmäßige und aufwändige Berechnung für Datensätze ausgeführt werden muss und es nicht praktikabel ist, die Berechnung über alle Datensätze durchzuführen. Mittels des hash-Kommandos können auf unterschiedlichen Abzügen die Hashwerte der Datensätze bestimmt werden. Durch Vergleich dieser Tabelle ist es möglich, die Datensätze zu identifizieren, die sich geändert haben, gelöscht oder neu hinzugefügt wurden. Das folgende Beispiel demonstriert die Erzeugung dieser Hash-Tabellen:",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/hash.html#optionen",
    "href": "commands/hash.html#optionen",
    "title": "hash",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-H &lt;header&gt;, --header &lt;header&gt;\n\nKopfzeile, die den Ergebnissen vorangestellt wird.\n\n-t, --tsv\n\nAusgabe erfolgt im TSV-Format.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o &lt;filename&gt;, --output &lt;filename&gt;\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll.",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/hash.html#beispiele",
    "href": "commands/hash.html#beispiele",
    "title": "hash",
    "section": "Beispiele",
    "text": "Beispiele\n\nAusgabe im TSV-Format\nDie Ausgabe lässt sich mittels der Option --tsv (bzw. -t) in das TSV-Format ändern.\n$ pica hash -s --tsv DUMP.dat.gz\nppn hash\n118540238   762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3\n118607626   8d75e2cdfec20aa025d36018a40b150363d79571788cd92e7ff568595ba4f9ee\n040993396   0361c33e1f7a80e21eecde747b2721b7884e003ac4deb8c271684ec0dc4d059a\n...\n\n\nHinzufügen einer Kopfzeile\nEine individuelle Kopfzeile lässt sich mit der Option --header (bzw. -H) ausgeben:\n$ pica hash -s --header 'idn,sha256' DUMP.dat.gz\nidn,sha256\n118540238,762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3\n118607626,8d75e2cdfec20aa025d36018a40b150363d79571788cd92e7ff568595ba4f9ee\n040993396,0361c33e1f7a80e21eecde747b2721b7884e003ac4deb8c271684ec0dc4d059a\n...",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/hash.html#anmerkung",
    "href": "commands/hash.html#anmerkung",
    "title": "hash",
    "section": "Anmerkung",
    "text": "Anmerkung\nZur Berechnung des SHA-256-Hashwerts wird der abschließende Zeilenumbruch mit einbezogen, um einen gleichen Hashwert zu erzeugen, der entsteht, wenn der Hashwert über die gesamte Zeile aus der Eingabe ermittelt wird. Im folgenden Beispiel wird zuerst der erste gültige Datensatz in die Datei 1.dat geschrieben. Anschließend wird der Hashwert einmal mit dem hash-Kommando und einmal mit dem Unix-Programm sha256sum gebildet. Beide Ergebnisse sind gleich.\n$ pica filter -s -l1 \"003@?\" DUMP.dat.gz -o 1.dat\n$ pica hash 1.dat\nppn,hash\n118540238,762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3\n\n$ sha256sum 1.dat\n762cf3a1b18a0cad2d0401cd2b573a89ff9c81b43c4ddab76e136d7a10a851f3",
    "crumbs": [
      "Kommandos",
      "hash"
    ]
  },
  {
    "objectID": "commands/invalid.html",
    "href": "commands/invalid.html",
    "title": "invalid",
    "section": "",
    "text": "Beispiel\nBei der Verarbeitung von PICA-Daten kann es vorkommen, dass Zeilen in der Eingabe nicht als normalisiertes PICA+ dekodiert werden können. Diese ungültigen Zeilen lassen sich bei vielen Kommandos mit der Option --skip-invalid / -s überspringen, wobei die Anzahl der übersprungenen Zeilen nicht angezeigt wird. Es ist zu empfehlen, die Anzahl invalider Datensätze zu kontrollieren und einer Prüfung zu unterziehen, um diese ggf. zu korrigieren. Das invalid-Kommando findet diese Zeilen in der Eingabe und gibt diese wieder auf der Standardausgabe (stdout) aus. Durch Angabe der Option --output / -o kann die Ausgabe in eine Datei geschrieben werden.\nDer folgende Befehl findet alle ungültigen Datensätze aus der Datei DUMP.dat.gz und schreibt diese Zeile in die Datei invalid.dat:",
    "crumbs": [
      "Kommandos",
      "invalid"
    ]
  },
  {
    "objectID": "commands/invalid.html#beispiel",
    "href": "commands/invalid.html#beispiel",
    "title": "invalid",
    "section": "",
    "text": "$ pica invalid DUMP.dat.gz -o invalid.dat",
    "crumbs": [
      "Kommandos",
      "invalid"
    ]
  },
  {
    "objectID": "commands/partition.html",
    "href": "commands/partition.html",
    "title": "partition",
    "section": "",
    "text": "Optionen\nMittels des partition-Kommandos lassen sich Datensätze anhand eines Unterfelds in Partitionen einteilen.\nLassen sich Datensätze anhand von den Wertausprägungen in einem Unterfeld gruppieren, ist es mitunter hilfreich die Gesamtmenge der Datensätze in Partitionen aufzuteilen. Ist das Unterfeld, nach dem partitioniert werden soll, wiederholbar, sind die erzeugten Partitionen i.d.R. nicht disjunkt. Ein Datensatz der das Unterfeld nicht besitzt, geht in keine Partition ein.\nIm folgenden Beispiel wird pro Entitätencode im Feld 004B.a eine Partition erstellt, die alle GND-Entitäten enthält, die diesem Entitätencode zugeordnet sind.",
    "crumbs": [
      "Kommandos",
      "partition"
    ]
  },
  {
    "objectID": "commands/partition.html#optionen",
    "href": "commands/partition.html#optionen",
    "title": "partition",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGroß- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim Ähnlichkeitsvergleich von Zeichenketten mittels =*.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n-t &lt;string&gt;, --template &lt;string&gt;\n\nTemplate für die Dateinamen. Der Platzhalter {} wird durch den Namen der Partition ersetzt.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt.\n\n-o &lt;path&gt;, --outdir &lt;path&gt;\n\nAngabe, in welches Verzeichnis die Partitionen geschrieben werden sollen. Standardmäßig wird das aktuelle Verzeichnis verwendet.",
    "crumbs": [
      "Kommandos",
      "partition"
    ]
  },
  {
    "objectID": "commands/partition.html#beispiele",
    "href": "commands/partition.html#beispiele",
    "title": "partition",
    "section": "Beispiele",
    "text": "Beispiele\n\nEingrenzen der Partitionen\nSollen nicht alle Partitionen erstellt werden, kann die Anzahl der möglichen Partition durch die Angabe eines Filterausdrucks eingegrenzt werden:\n$ pica partition -s \"004B{a | a in ['piz', 'saz']}\" DUMP.dat.gz -o out\n$ tree out/\nout\n├── piz.dat\n└── saz.dat\n\n\nBenutzerdefinierte Dateinamen\nStandardmäßig werden die erstellten Partitionen nach den Werten im Unterfeld benannt. Der Dateiname kann individuell mit der -t/--template-Option angepasst werden. Jedes Vorkommen der Zeichenfolge {} im Template wird durch den Wert des Unterfelds ersetzt. Endet die Datei auf der Dateiendung .gz, wird die Ausgabe automatisch im Gzip-Format komprimiert.\n$ pica partition -s \"004B.a\" --template \"code_{}.dat.gz\" DUMP.dat.gz -o out\n$ tree out/\nout\n├── code_gik.dat.gz\n├── code_piz.dat.gz\n├── code_saz.dat.gz\n└── code_wit.dat.gz\n\n\nKomprimierte Ausgabe\nMittels der Option --gzip bzw. -g erfolgt eine Komprimierung der Ausgabe:\n$ pica partition -s \"004B.a\" --gzip DUMP.dat.gz -o out\n$ tree out/\nout\n├── gik.dat.gz\n├── piz.dat.gz\n├── saz.dat.gz\n└── wit.dat.gz",
    "crumbs": [
      "Kommandos",
      "partition"
    ]
  },
  {
    "objectID": "commands/sample.html",
    "href": "commands/sample.html",
    "title": "sample",
    "section": "",
    "text": "Optionen\nDas sample-Kommando zieht nach dem Zufallsprinzip gleichmäßig Datensätze aus der Eingabe.\nIm folgenden Beispiel werden zufällig 200 Datensätze aus der Eingabe ausgewählt und in die Datei samples.dat geschrieben:",
    "crumbs": [
      "Kommandos",
      "sample"
    ]
  },
  {
    "objectID": "commands/sample.html#optionen",
    "href": "commands/sample.html#optionen",
    "title": "sample",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-i, --ignore-case\n\nGroß- und Kleinschreibung wird bei Vergleichen ignoriert.\n\n--strsim-threshold &lt;value&gt;\n\nFestlegen des Schwellenwerts beim Ähnlichkeitsvergleich von Zeichenketten mittels =*.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im [Gzip]-Format.\n\n--seed &lt;number&gt;\n\nInitialisiert den Zufallszahlengenerator mit einem seed-Wert, um eine deterministische Auswahl zu erhalten.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt.\n\n-o &lt;path&gt;, --outdir &lt;path&gt;\n\nAngabe, in welches Verzeichnis die Partitionen geschrieben werden sollen. Standardmäßig wird das aktuelle Verzeichnis verwendet.",
    "crumbs": [
      "Kommandos",
      "sample"
    ]
  },
  {
    "objectID": "commands/sample.html#beispiele",
    "href": "commands/sample.html#beispiele",
    "title": "sample",
    "section": "Beispiele",
    "text": "Beispiele\n\nZufällige PPN-Liste\nIn Kombination mit dem select-Kommando kann eine zufällige PPN-Liste erzeugt werden:\n$ pica sample 3 DUMP.dat.gz | pica select -H 'ppn' '003@.0' -o samples.csv",
    "crumbs": [
      "Kommandos",
      "sample"
    ]
  },
  {
    "objectID": "commands/slice.html",
    "href": "commands/slice.html",
    "title": "slice",
    "section": "",
    "text": "Optionen\nMittels des slice-Kommandos kann ein zusammenhängender Teilbereich aus der Eingabe ausgeschnitten werden. Dabei wird der Teilbereich als halb-offenes Intervall angegeben, wobei die Positionen von 0 an gezählt werden. Beim Auftreten eines ungültigen Datensatzes wird die Position weitergezählt. Enthält bspw. die Eingabe 1.000 Zeilen, mit 990 Datensätzen und 10 ungültigen Zeilen, dann sind die Positionen von 0 bis 999 durchnummeriert.\nDas folgende Beispiel extrahiert alle (gültigen) Datensätze aus den Positionen 2 bis 4:",
    "crumbs": [
      "Kommandos",
      "slice"
    ]
  },
  {
    "objectID": "commands/slice.html#optionen",
    "href": "commands/slice.html#optionen",
    "title": "slice",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n--start &lt;number&gt;\n\nStartposition des Teilbereichs (Voreinstellung: 0).\n\n--end &lt;number&gt;\n\nEndposition des Teilbereichs; diese Position ist nicht mehr Teil der Ausgabe. Ist keine Endposition angegeben, wird der Teilbereich bis zum Ende der Eingabe fortgeführt. Diese Option ist nicht kombinierbar mit --length.\n\n--length &lt;number&gt;\n\nFestlegen der maximalen Anzahl an Datensätzen, die in der Ausgabe enthalten sind. Diese Option kann nicht mit --end kombiniert werden.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n--append\n\nWenn die Ausgabedatei bereits existiert, wird die Ausgabe an die Datei angehangen. Ist das Flag nicht gesetzt, wird eine bestehende Datei standardmäßig überschrieben.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt. Das Aktivieren der Option erfordert das Schreiben der Datensätze in eine Datei mittels -o bzw. --output.\n\n-o, --output\n\nAngabe, in welche Datei die Ausgabe geschrieben werden soll. Standardmäßig wird die Ausgabe in die Standardausgabe stdout geschrieben.",
    "crumbs": [
      "Kommandos",
      "slice"
    ]
  },
  {
    "objectID": "commands/slice.html#beispiele",
    "href": "commands/slice.html#beispiele",
    "title": "slice",
    "section": "Beispiele",
    "text": "Beispiele\n\nAusschneiden eines Teilbereichs fester Größe\nWenn die Eingabe ausreichend Datensätze enthält, kann beginnend bei einer festen Position (--start) ein Teilbereich mit einer festen Länge (--length) ausgeschnitten werden.\nIm folgenden Beispiel wird beginnend beim dritten Datensatz (Position 2) ein Teilbereich mit einer Länge von zwei ausgeschnitten:\n$ pica slice -s --start 2 --length 2 DUMP.dat.gz -o slice.dat\n$ pica count --records slice.dat\n2",
    "crumbs": [
      "Kommandos",
      "slice"
    ]
  },
  {
    "objectID": "commands/split.html",
    "href": "commands/split.html",
    "title": "split",
    "section": "",
    "text": "Optionen\nMithilfe des split-Kommandos können alle Datensätze aus der Eingabe in mehrere Dateien aufgeteilt werden, wobei jede Datei eine maximale Anzahl an Datensätzen nicht überschreitet. Ein Anwendungsfall für das Splitting könnte eine automatisierte Stapelverarbeitung oder die parallele Verarbeitung der entstandenen Dateien sein.\nDer folgende Aufruf des split-Kommandos teilt die zwölf Datensätze der Eingabe (DUMP.dat.gz) in drei Dateien mit maximal fünf Datensätzen pro Datei. Die ersten beiden Dateien (0.dat und 1.dat) enthalten die maximale Anzahl an Datensätzen, die letzte Datei (2.dat) die restlichen zwei Datensätze.",
    "crumbs": [
      "Kommandos",
      "split"
    ]
  },
  {
    "objectID": "commands/split.html#optionen",
    "href": "commands/split.html#optionen",
    "title": "split",
    "section": "",
    "text": "-s, --skip-invalid\n\nÜberspringt jene Zeilen aus der Eingabe, die nicht dekodiert werden konnten.\n\n-g, --gzip\n\nKomprimieren der Ausgabe im Gzip-Format.\n\n--template\n\nTemplate für die Dateinamen. Der Platzhalter {} wird durch eine fortlaufende Nummer ersetzt.\n\n-p, --progress\n\nAnzeige des Fortschritts, der die Anzahl der eingelesenen gültigen sowie invaliden Datensätze anzeigt.\n\n-o &lt;path&gt;, --outdir &lt;path&gt;\n\nAngabe, in welches Verzeichnis die Ausgabe geschrieben werden soll. Standardmäßig wird das aktuelle Verzeichnis verwendet.",
    "crumbs": [
      "Kommandos",
      "split"
    ]
  },
  {
    "objectID": "commands/split.html#beispiele",
    "href": "commands/split.html#beispiele",
    "title": "split",
    "section": "Beispiele",
    "text": "Beispiele\n\nBenutzerdefinierte Dateinamen\nStandardmäßig werden die erstellten Dateien beginnend bei 0 durchnummeriert. Der Dateiname kann individuell mit der --template-Option angepasst werden. Jedes Vorkommen der Zeichenfolge {} im Template wird durch die aktuelle Nummer ersetzt. Endet die Datei auf der Dateiendung .gz, wird die Ausgabe automatisch im Gzip-Format komprimiert.\n$ pica split -s --template \"CHUNK_{}.dat.gz\" 10 DUMP.dat.gz\n$ tree\n.\n├── CHUNK_0.dat.gz\n├── CHUNK_1.dat.gz\n└── DUMP.dat.gz\n\n$ pica count --records CHUNK_0.dat.gz\n10\n\n$ pica count --records CHUNK_1.dat.gz\n2\n\n\nKomprimierte Ausgabe\nMittels der Option --gzip bzw. -g erfolgt eine Komprimierung der Ausgabe:\n$ pica split -s --gzip 10 DUMP.dat.gz\n$ tree\n.\n├── 0.dat.gz\n├── 1.dat.gz\n└── DUMP.dat.gz\n\n$ pica count --records 0.dat.gz\n10\n\n$ pica count --records 1.dat.gz\n2",
    "crumbs": [
      "Kommandos",
      "split"
    ]
  }
]